{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881edca4-197a-45bf-b1bd-af504f90fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f64a4a-7ab4-4a3e-be96-b22c1a6d4e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 33067 entries, 2019-02-18 00:00:00 to 2022-12-31 00:00:00\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   SL NO        33067 non-null  int64  \n",
      " 1   PM2.5        31251 non-null  float64\n",
      " 2   PM10         31790 non-null  float64\n",
      " 3   NO           31396 non-null  float64\n",
      " 4   NO2          32393 non-null  float64\n",
      " 5   Nox          32616 non-null  float64\n",
      " 6   NH3          31705 non-null  float64\n",
      " 7   SO2          32592 non-null  float64\n",
      " 8   CO           32616 non-null  float64\n",
      " 9   Ozone        32473 non-null  float64\n",
      " 10  Benzene      30961 non-null  float64\n",
      " 11  Eth-Benzene  16450 non-null  float64\n",
      " 12  MP-Xylene    22659 non-null  float64\n",
      " 13  WS           31483 non-null  float64\n",
      " 14  WD           31528 non-null  float64\n",
      " 15  SR           20184 non-null  float64\n",
      " 16  BP           21876 non-null  float64\n",
      " 17  AT           31462 non-null  float64\n",
      " 18  RF           32623 non-null  float64\n",
      "dtypes: float64(18), int64(1)\n",
      "memory usage: 5.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_12256\\4218735721.py:1: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.read_csv(r\"C:\\Users\\himan\\Desktop\\7th GAN\\updataDataSets.csv\",\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\himan\\Desktop\\7th GAN\\updataDataSets.csv\", \n",
    "                 index_col='From Date', \n",
    "                 parse_dates=['From Date'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "effc6358-a964-4260-a172-8d3c2d1b8ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "                     SL NO  PM2.5   PM10    NO   NO2    Nox    NH3    SO2  \\\n",
      "From Date                                                                   \n",
      "2019-02-18 00:00:00      1  58.04  81.06  1.81  6.40   9.66  27.18  13.77   \n",
      "2019-02-18 01:00:00      2  60.00  82.39  1.91  6.08   9.70  26.46  13.64   \n",
      "2019-02-18 02:00:00      3  55.50  73.42  2.22  6.71  10.86  27.06  13.64   \n",
      "2019-02-18 03:00:00      4  66.56  96.04  2.79  7.00  12.10  26.34  13.72   \n",
      "2019-02-18 04:00:00      5  67.08  97.21  3.29  9.45  15.53  25.16  13.80   \n",
      "\n",
      "                       CO  Ozone  Benzene  Eth-Benzene  MP-Xylene    WS  \\\n",
      "From Date                                                                 \n",
      "2019-02-18 00:00:00  0.49  15.73     0.92          NaN       0.33  1.86   \n",
      "2019-02-18 01:00:00  0.44  18.36     0.84          NaN       0.29  1.54   \n",
      "2019-02-18 02:00:00  0.44  34.54     0.77          NaN       0.23  0.66   \n",
      "2019-02-18 03:00:00  0.45  31.85     0.82          NaN       0.21  0.61   \n",
      "2019-02-18 04:00:00  0.47  24.07     0.74          NaN       0.22  0.54   \n",
      "\n",
      "                         WD    SR       BP     AT    RF  \n",
      "From Date                                                \n",
      "2019-02-18 00:00:00  229.44  0.22  1002.19  20.87  0.00  \n",
      "2019-02-18 01:00:00  217.20  1.46   956.00  18.92  0.24  \n",
      "2019-02-18 02:00:00  197.21   NaN  1001.60  18.33  0.00  \n",
      "2019-02-18 03:00:00  206.44   NaN  1001.41  17.63  0.00  \n",
      "2019-02-18 04:00:00  230.53   NaN   980.67  16.61  0.00  \n"
     ]
    }
   ],
   "source": [
    "print(\"Original Dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85f0412-9a0d-4fba-8b12-afe2009cf18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f314980-0f9c-45ec-b2cf-e85c19b84774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "def build_generator(latent_dim, n_features):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=latent_dim, activation='relu'))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(n_features, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef78fb3-f624-46fc-b076-6bb109258810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "def build_discriminator(n_features):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(512, input_dim=n_features, activation='relu'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed4cb6d-37d4-4c10-a900-5881f690073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Conditional Sig-Wasserstein GAN\n",
    "def build_cswgan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3e59b2-07b6-46d0-adaa-33110d17fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4bdd9f3-e29d-4017-b2a3-0719d1a469b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the models\n",
    "latent_dim = 100  # Adjust as needed\n",
    "generator = build_generator(latent_dim, df.shape[1])\n",
    "discriminator = build_discriminator(df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dfd75b5-ed5d-4102-9b55-ae12a1d88592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "latent_dim = 150\n",
    "generator = build_generator(latent_dim, df.shape[1])\n",
    "discriminator = build_discriminator(df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a1c1e1b-c88a-4c8a-b533-36768ccadce7",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Function to generate random noise for the generator\n",
    "def generate_latent_samples(batch_size, latent_dim):\n",
    "    return np.random.randn(batch_size, latent_dim)\n",
    "\n",
    "# Function to get a random batch from the real data\n",
    "def get_real_samples(data, batch_size):\n",
    "    idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "    return data[idx]\n",
    "\n",
    "# Function to train the Conditional Sig-Wasserstein GAN\n",
    "def train_cswgan(generator, discriminator, cswgan, real_data, latent_dim, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(len(real_data) // batch_size):\n",
    "            # Train discriminator\n",
    "            real_samples = get_real_samples(real_data, batch_size)\n",
    "            fake_samples = generator.predict(generate_latent_samples(batch_size, latent_dim))\n",
    "            \n",
    "            discriminator_loss_real = discriminator.train_on_batch(real_samples, -np.ones((batch_size, 1)))\n",
    "            discriminator_loss_fake = discriminator.train_on_batch(fake_samples, np.ones((batch_size, 1)))\n",
    "            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "            # Train generator (via the combined model)\n",
    "            latent_samples = generate_latent_samples(batch_size, latent_dim)\n",
    "            valid_labels = -np.ones((batch_size, 1))\n",
    "            generator_loss = cswgan.train_on_batch(latent_samples, valid_labels)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch + 1}, Batch {_ + 1}/{len(real_data) // batch_size}, D Loss: {discriminator_loss}, G Loss: {generator_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02fc1bf1-fa61-4e96-aceb-d546c162397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 150\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b29cfa9-84b5-47c6-b8ff-8bf44e802b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = df.drop(['SL NO'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b35c82-2ad4-4278-9ae7-5c0d829d4b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "WARNING:tensorflow:From C:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Epoch 1, Batch 1/516, D Loss: -0.13476157188415527, G Loss: -0.5057960748672485\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 2/516, D Loss: -0.06753608584403992, G Loss: -0.5416631698608398\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 3/516, D Loss: -0.102784663438797, G Loss: -0.5893239974975586\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 4/516, D Loss: -0.04916226863861084, G Loss: -0.6341465711593628\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 5/516, D Loss: -0.062362343072891235, G Loss: -0.6884182095527649\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 6/516, D Loss: -0.06283771991729736, G Loss: -0.7392426133155823\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 7/516, D Loss: -0.01695913076400757, G Loss: -0.7823032140731812\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 8/516, D Loss: 0.023337960243225098, G Loss: -0.8285609483718872\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 9/516, D Loss: 0.03229370713233948, G Loss: -0.8675326108932495\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 10/516, D Loss: 0.02955135703086853, G Loss: -0.8981050252914429\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 11/516, D Loss: 0.04660898447036743, G Loss: -0.9310764074325562\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 12/516, D Loss: 0.06424283981323242, G Loss: -0.9489526152610779\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 13/516, D Loss: 0.05735832452774048, G Loss: -0.9656715393066406\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 14/516, D Loss: 0.04270964860916138, G Loss: -0.9744871854782104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 15/516, D Loss: 0.10438042879104614, G Loss: -0.9856672286987305\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 16/516, D Loss: 0.07090210914611816, G Loss: -0.9889044761657715\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 17/516, D Loss: 0.08951017260551453, G Loss: -0.9923882484436035\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 18/516, D Loss: 0.07300925254821777, G Loss: -0.9951163530349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 19/516, D Loss: 0.08819258213043213, G Loss: -0.9964678287506104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 20/516, D Loss: 0.11827591061592102, G Loss: -0.9976301193237305\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 21/516, D Loss: 0.11693570017814636, G Loss: -0.9980740547180176\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 22/516, D Loss: 0.11628097295761108, G Loss: -0.998528003692627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 23/516, D Loss: 0.11091572046279907, G Loss: -0.9990620613098145\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 24/516, D Loss: 0.0854673981666565, G Loss: -0.9992653131484985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 25/516, D Loss: 0.07714784145355225, G Loss: -0.9996240735054016\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 26/516, D Loss: 0.1733090877532959, G Loss: -0.999566376209259\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 27/516, D Loss: 0.07440721988677979, G Loss: -0.9996844530105591\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 28/516, D Loss: 0.09780958294868469, G Loss: -0.9997313022613525\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 29/516, D Loss: 0.11492633819580078, G Loss: -0.9998340010643005\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 30/516, D Loss: 0.08471530675888062, G Loss: -0.9998079538345337\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 31/516, D Loss: 0.11049768328666687, G Loss: -0.9998897314071655\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 32/516, D Loss: 0.12270256876945496, G Loss: -0.9998787641525269\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 33/516, D Loss: 0.12177181243896484, G Loss: -0.9999051690101624\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 34/516, D Loss: 0.08071106672286987, G Loss: -0.9999148845672607\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 35/516, D Loss: 0.10547342896461487, G Loss: -0.9999312162399292\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 36/516, D Loss: 0.11407256126403809, G Loss: -0.9999353885650635\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 37/516, D Loss: 0.07027870416641235, G Loss: -0.9999470710754395\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 38/516, D Loss: 0.1086982786655426, G Loss: -0.9999469518661499\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 39/516, D Loss: 0.14030897617340088, G Loss: -0.9999533891677856\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 40/516, D Loss: 0.09309828281402588, G Loss: -0.9999526739120483\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 41/516, D Loss: 0.07409733533859253, G Loss: -0.9999440312385559\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 42/516, D Loss: 0.10424727201461792, G Loss: -0.9999557733535767\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 43/516, D Loss: 0.09741026163101196, G Loss: -0.9999679327011108\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 44/516, D Loss: 0.10172680020332336, G Loss: -0.9999651908874512\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 45/516, D Loss: 0.10358789563179016, G Loss: -0.9999596476554871\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 46/516, D Loss: 0.10268285870552063, G Loss: -0.9999701380729675\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 47/516, D Loss: 0.08346790075302124, G Loss: -0.9999555349349976\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 48/516, D Loss: 0.06489977240562439, G Loss: -0.9999679327011108\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 49/516, D Loss: 0.11344331502914429, G Loss: -0.9999722242355347\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 50/516, D Loss: 0.0945776104927063, G Loss: -0.9999651908874512\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 51/516, D Loss: 0.10129255056381226, G Loss: -0.9999736547470093\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 52/516, D Loss: 0.0791972279548645, G Loss: -0.9999747276306152\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 53/516, D Loss: 0.07025989890098572, G Loss: -0.9999722838401794\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 54/516, D Loss: 0.07906746864318848, G Loss: -0.9999759197235107\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 55/516, D Loss: 0.11044663190841675, G Loss: -0.9999768733978271\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 56/516, D Loss: 0.10144731402397156, G Loss: -0.9999752044677734\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 57/516, D Loss: 0.11600828170776367, G Loss: -0.9999774098396301\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 58/516, D Loss: 0.08980557322502136, G Loss: -0.9999775290489197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 59/516, D Loss: 0.0833960771560669, G Loss: -0.9999697208404541\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 60/516, D Loss: 0.10985106229782104, G Loss: -0.9999833106994629\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 61/516, D Loss: 0.03631323575973511, G Loss: -0.9999769330024719\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 62/516, D Loss: 0.09280923008918762, G Loss: -0.9999796748161316\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 63/516, D Loss: 0.09778070449829102, G Loss: -0.9999786615371704\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 64/516, D Loss: 0.09274125099182129, G Loss: -0.999980092048645\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 65/516, D Loss: 0.08694884181022644, G Loss: -0.9999796152114868\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 66/516, D Loss: 0.07139697670936584, G Loss: -0.9999790191650391\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 67/516, D Loss: 0.07605192065238953, G Loss: -0.999980628490448\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 68/516, D Loss: 0.06223726272583008, G Loss: -0.9999804496765137\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 69/516, D Loss: 0.08558481931686401, G Loss: -0.9999779462814331\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 70/516, D Loss: 0.05515483021736145, G Loss: -0.9999686479568481\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 71/516, D Loss: 0.05591264367103577, G Loss: -0.9999809265136719\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 72/516, D Loss: 0.06757599115371704, G Loss: -0.9999733567237854\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 73/516, D Loss: 0.11123102903366089, G Loss: -0.9999804496765137\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 74/516, D Loss: 0.090311199426651, G Loss: -0.9999814629554749\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 75/516, D Loss: 0.08160987496376038, G Loss: -0.9999760985374451\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 76/516, D Loss: 0.07922732830047607, G Loss: -0.9999858140945435\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 77/516, D Loss: 0.0654195249080658, G Loss: -0.9999818205833435\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 78/516, D Loss: 0.0546136200428009, G Loss: -0.999962329864502\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 79/516, D Loss: 0.12225341796875, G Loss: -0.9999831914901733\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 80/516, D Loss: 0.08420386910438538, G Loss: -0.9999692440032959\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 81/516, D Loss: 0.04819822311401367, G Loss: -0.9999783039093018\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 82/516, D Loss: 0.10320213437080383, G Loss: -0.9999814629554749\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 83/516, D Loss: 0.1092769205570221, G Loss: -0.9999833106994629\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 84/516, D Loss: 0.09695592522621155, G Loss: -0.9999830722808838\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 85/516, D Loss: 0.10440000891685486, G Loss: -0.9999828338623047\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 86/516, D Loss: 0.07058835029602051, G Loss: -0.9999778270721436\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 87/516, D Loss: 0.05284962058067322, G Loss: -0.9999843835830688\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 88/516, D Loss: 0.09982603788375854, G Loss: -0.9999757409095764\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 89/516, D Loss: 0.07433149218559265, G Loss: -0.9999731183052063\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 90/516, D Loss: 0.09376105666160583, G Loss: -0.9999839067459106\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 91/516, D Loss: 0.07827264070510864, G Loss: -0.9999761581420898\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 92/516, D Loss: 0.11957031488418579, G Loss: -0.9999892711639404\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 93/516, D Loss: 0.07843264937400818, G Loss: -0.9999832510948181\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 94/516, D Loss: 0.10440200567245483, G Loss: -0.9999780058860779\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 95/516, D Loss: 0.10397666692733765, G Loss: -0.9999861121177673\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 96/516, D Loss: 0.10841336846351624, G Loss: -0.999974250793457\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 97/516, D Loss: 0.10045990347862244, G Loss: -0.9999839067459106\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 98/516, D Loss: 0.057888031005859375, G Loss: -0.9999839067459106\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 99/516, D Loss: 0.09029477834701538, G Loss: -0.9999799728393555\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 100/516, D Loss: 0.08564543724060059, G Loss: -0.9999818801879883\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 101/516, D Loss: 0.09935027360916138, G Loss: -0.9999767541885376\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 102/516, D Loss: 0.08673220872879028, G Loss: -0.9999779462814331\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 103/516, D Loss: 0.09563389420509338, G Loss: -0.9999823570251465\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 104/516, D Loss: 0.09145569801330566, G Loss: -0.9999806880950928\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 105/516, D Loss: 0.10081970691680908, G Loss: -0.9999812841415405\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 106/516, D Loss: 0.11232277750968933, G Loss: -0.999983549118042\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 107/516, D Loss: 0.06993556022644043, G Loss: -0.9999792575836182\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 108/516, D Loss: 0.04467073082923889, G Loss: -0.9999851584434509\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 109/516, D Loss: 0.10087338089942932, G Loss: -0.9999822378158569\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 110/516, D Loss: 0.1304256021976471, G Loss: -0.9999799132347107\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 111/516, D Loss: 0.11117318272590637, G Loss: -0.9999783039093018\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 112/516, D Loss: 0.12003576755523682, G Loss: -0.9999842643737793\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 113/516, D Loss: 0.14921557903289795, G Loss: -0.9999837875366211\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 114/516, D Loss: 0.06719040870666504, G Loss: -0.9999817609786987\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 115/516, D Loss: 0.11467891931533813, G Loss: -0.9999810457229614\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 116/516, D Loss: 0.049611419439315796, G Loss: -0.999980092048645\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 117/516, D Loss: 0.10604333877563477, G Loss: -0.9999868273735046\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 118/516, D Loss: 0.11436891555786133, G Loss: -0.9999867677688599\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 119/516, D Loss: 0.1385839879512787, G Loss: -0.9999866485595703\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 120/516, D Loss: 0.12092763185501099, G Loss: -0.9999815225601196\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 121/516, D Loss: 0.11717420816421509, G Loss: -0.9999788403511047\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 122/516, D Loss: 0.11728757619857788, G Loss: -0.9999813437461853\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 123/516, D Loss: 0.07199960947036743, G Loss: -0.9999822378158569\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 124/516, D Loss: 0.11924237012863159, G Loss: -0.9999828934669495\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 125/516, D Loss: 0.12584179639816284, G Loss: -0.9999862313270569\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 126/516, D Loss: 0.1373206377029419, G Loss: -0.9999822378158569\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 127/516, D Loss: 0.07803654670715332, G Loss: -0.9999740719795227\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 128/516, D Loss: 0.0693073570728302, G Loss: -0.9999808073043823\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 129/516, D Loss: 0.07008153200149536, G Loss: -0.9999791979789734\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 130/516, D Loss: 0.16685673594474792, G Loss: -0.9999796152114868\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 131/516, D Loss: 0.06582164764404297, G Loss: -0.9999819993972778\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 132/516, D Loss: 0.07200470566749573, G Loss: -0.9999772310256958\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 133/516, D Loss: 0.07856953144073486, G Loss: -0.9999815225601196\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 134/516, D Loss: 0.07495042681694031, G Loss: -0.9999883770942688\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 135/516, D Loss: 0.07508257031440735, G Loss: -0.9999855756759644\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 136/516, D Loss: 0.10812216997146606, G Loss: -0.9999860525131226\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 137/516, D Loss: 0.10553067922592163, G Loss: -0.9999808073043823\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 138/516, D Loss: 0.10584700107574463, G Loss: -0.9999854564666748\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 139/516, D Loss: 0.11566871404647827, G Loss: -0.9999860525131226\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 140/516, D Loss: 0.10777053236961365, G Loss: -0.9999842643737793\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 141/516, D Loss: 0.11005142331123352, G Loss: -0.9999818801879883\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 142/516, D Loss: 0.10878479480743408, G Loss: -0.999981164932251\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 143/516, D Loss: 0.14695420861244202, G Loss: -0.9999774694442749\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 144/516, D Loss: 0.08110928535461426, G Loss: -0.9999791979789734\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 145/516, D Loss: 0.06958454847335815, G Loss: -0.9999828338623047\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 146/516, D Loss: 0.10378587245941162, G Loss: -0.9999890923500061\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 147/516, D Loss: 0.12754204869270325, G Loss: -0.9999836683273315\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 148/516, D Loss: 0.1057167649269104, G Loss: -0.9999863505363464\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 149/516, D Loss: 0.09390375018119812, G Loss: -0.9999831914901733\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 150/516, D Loss: 0.0776943564414978, G Loss: -0.9999868869781494\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 151/516, D Loss: 0.07316184043884277, G Loss: -0.9999827742576599\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 152/516, D Loss: 0.06309229135513306, G Loss: -0.9999833106994629\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 153/516, D Loss: 0.13255977630615234, G Loss: -0.9999866485595703\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 154/516, D Loss: 0.1293482780456543, G Loss: -0.9999873638153076\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 155/516, D Loss: 0.08154460787773132, G Loss: -0.9999825954437256\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 156/516, D Loss: 0.10996690392494202, G Loss: -0.9999843239784241\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 157/516, D Loss: 0.10016685724258423, G Loss: -0.999987006187439\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 158/516, D Loss: 0.12167450785636902, G Loss: -0.999985933303833\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 159/516, D Loss: 0.08282017707824707, G Loss: -0.9999836683273315\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 160/516, D Loss: 0.11446407437324524, G Loss: -0.999984622001648\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 161/516, D Loss: 0.07604581117630005, G Loss: -0.9999827146530151\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 162/516, D Loss: 0.0846305787563324, G Loss: -0.9999849200248718\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 163/516, D Loss: 0.07312312722206116, G Loss: -0.9999816417694092\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 164/516, D Loss: 0.0670049786567688, G Loss: -0.9999871253967285\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 165/516, D Loss: 0.14026868343353271, G Loss: -0.9999858140945435\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 166/516, D Loss: 0.12205749750137329, G Loss: -0.9999812841415405\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 167/516, D Loss: 0.1028052568435669, G Loss: -0.9999848008155823\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 168/516, D Loss: 0.11904007196426392, G Loss: -0.9999858140945435\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 169/516, D Loss: 0.06720298528671265, G Loss: -0.9999818205833435\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 170/516, D Loss: 0.12332665920257568, G Loss: -0.999987006187439\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 171/516, D Loss: 0.07206079363822937, G Loss: -0.9999817609786987\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 172/516, D Loss: 0.12685704231262207, G Loss: -0.9999768137931824\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 173/516, D Loss: 0.09001156687736511, G Loss: -0.9999812245368958\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 174/516, D Loss: 0.11207616329193115, G Loss: -0.9999887347221375\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 175/516, D Loss: 0.10232701897621155, G Loss: -0.9999847412109375\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 176/516, D Loss: 0.07262909412384033, G Loss: -0.9999854564666748\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 177/516, D Loss: 0.09240514039993286, G Loss: -0.9999883770942688\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 178/516, D Loss: 0.12698322534561157, G Loss: -0.9999868273735046\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 179/516, D Loss: 0.07297393679618835, G Loss: -0.9999867677688599\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 180/516, D Loss: 0.1488909125328064, G Loss: -0.9999862909317017\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 181/516, D Loss: 0.15758761763572693, G Loss: -0.9999834895133972\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 182/516, D Loss: 0.07331544160842896, G Loss: -0.9999873042106628\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 183/516, D Loss: 0.06848019361495972, G Loss: -0.9999874234199524\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 184/516, D Loss: 0.12093496322631836, G Loss: -0.9999798536300659\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 185/516, D Loss: 0.09377390146255493, G Loss: -0.9999826550483704\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 186/516, D Loss: 0.09562402963638306, G Loss: -0.9999839067459106\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 187/516, D Loss: 0.07391566038131714, G Loss: -0.999982476234436\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 188/516, D Loss: 0.08367657661437988, G Loss: -0.9999833106994629\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 189/516, D Loss: 0.09937170147895813, G Loss: -0.9999858140945435\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 190/516, D Loss: 0.11909842491149902, G Loss: -0.9999848008155823\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 191/516, D Loss: 0.06670236587524414, G Loss: -0.9999866485595703\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 192/516, D Loss: 0.08961015939712524, G Loss: -0.9999817609786987\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 193/516, D Loss: 0.12357068061828613, G Loss: -0.9999877214431763\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 194/516, D Loss: 0.10950389504432678, G Loss: -0.9999837279319763\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 195/516, D Loss: 0.09001466631889343, G Loss: -0.9999898672103882\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 196/516, D Loss: 0.08830052614212036, G Loss: -0.9999845623970032\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 197/516, D Loss: 0.0853920578956604, G Loss: -0.9999872446060181\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 198/516, D Loss: 0.11039867997169495, G Loss: -0.99998539686203\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 199/516, D Loss: 0.11551082134246826, G Loss: -0.9999837875366211\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 200/516, D Loss: 0.06713458895683289, G Loss: -0.9999864101409912\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 201/516, D Loss: 0.0728704035282135, G Loss: -0.9999836683273315\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 202/516, D Loss: 0.13097703456878662, G Loss: -0.9999878406524658\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 203/516, D Loss: 0.07464513182640076, G Loss: -0.9999890327453613\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 204/516, D Loss: 0.08727496862411499, G Loss: -0.9999858140945435\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 205/516, D Loss: 0.12435460090637207, G Loss: -0.9999892711639404\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 206/516, D Loss: 0.10904809832572937, G Loss: -0.9999884366989136\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 207/516, D Loss: 0.11955863237380981, G Loss: -0.9999874830245972\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 208/516, D Loss: 0.08419549465179443, G Loss: -0.9999856948852539\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 209/516, D Loss: 0.1251208782196045, G Loss: -0.9999842643737793\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 210/516, D Loss: 0.06968358159065247, G Loss: -0.9999885559082031\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 211/516, D Loss: 0.10378643870353699, G Loss: -0.9999880194664001\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 212/516, D Loss: 0.08942222595214844, G Loss: -0.9999869465827942\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 213/516, D Loss: 0.11552238464355469, G Loss: -0.999982476234436\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 214/516, D Loss: 0.13610237836837769, G Loss: -0.9999873042106628\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 215/516, D Loss: 0.1298266053199768, G Loss: -0.9999873638153076\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 216/516, D Loss: 0.08682715892791748, G Loss: -0.999984622001648\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 217/516, D Loss: 0.11639788746833801, G Loss: -0.999987006187439\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 218/516, D Loss: 0.1068699061870575, G Loss: -0.9999876022338867\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 219/516, D Loss: 0.08397975564002991, G Loss: -0.9999868869781494\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 220/516, D Loss: 0.06351625919342041, G Loss: -0.9999856948852539\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 221/516, D Loss: 0.09759193658828735, G Loss: -0.9999911785125732\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 222/516, D Loss: 0.11059388518333435, G Loss: -0.999988317489624\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 223/516, D Loss: 0.0599094033241272, G Loss: -0.9999849796295166\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 224/516, D Loss: 0.0763406753540039, G Loss: -0.9999830722808838\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 225/516, D Loss: 0.07794255018234253, G Loss: -0.9999830722808838\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 226/516, D Loss: 0.040257275104522705, G Loss: -0.9999821186065674\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 227/516, D Loss: 0.09776440262794495, G Loss: -0.9999856948852539\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 228/516, D Loss: 0.08049139380455017, G Loss: -0.9999854564666748\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 229/516, D Loss: 0.0671667754650116, G Loss: -0.9999881982803345\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 230/516, D Loss: 0.10793894529342651, G Loss: -0.9999886751174927\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 231/516, D Loss: 0.07110428810119629, G Loss: -0.9999868869781494\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 232/516, D Loss: 0.061088353395462036, G Loss: -0.9999862909317017\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 233/516, D Loss: 0.09599214792251587, G Loss: -0.9999829530715942\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 234/516, D Loss: 0.04360601305961609, G Loss: -0.9999850988388062\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 235/516, D Loss: 0.11412930488586426, G Loss: -0.9999897480010986\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 236/516, D Loss: 0.09002387523651123, G Loss: -0.99998939037323\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 237/516, D Loss: 0.12270024418830872, G Loss: -0.9999896883964539\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 238/516, D Loss: 0.08631476759910583, G Loss: -0.9999860525131226\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 239/516, D Loss: 0.12323281168937683, G Loss: -0.9999874830245972\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 240/516, D Loss: 0.06483861804008484, G Loss: -0.9999871253967285\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 241/516, D Loss: 0.10072275996208191, G Loss: -0.9999905824661255\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 242/516, D Loss: 0.06753787398338318, G Loss: -0.999988853931427\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 243/516, D Loss: 0.08061563968658447, G Loss: -0.9999876022338867\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 244/516, D Loss: 0.09389117360115051, G Loss: -0.99998939037323\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 245/516, D Loss: 0.11162805557250977, G Loss: -0.9999877214431763\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 246/516, D Loss: 0.08910617232322693, G Loss: -0.99998939037323\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 247/516, D Loss: 0.08538955450057983, G Loss: -0.9999848008155823\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 248/516, D Loss: 0.13006892800331116, G Loss: -0.9999844431877136\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 249/516, D Loss: 0.0807967483997345, G Loss: -0.9999852776527405\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 250/516, D Loss: 0.11653760075569153, G Loss: -0.9999856948852539\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 251/516, D Loss: 0.06923437118530273, G Loss: -0.9999842643737793\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 252/516, D Loss: 0.0709303617477417, G Loss: -0.999986469745636\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 253/516, D Loss: 0.11405795812606812, G Loss: -0.999984860420227\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 254/516, D Loss: 0.0838804543018341, G Loss: -0.9999865293502808\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 255/516, D Loss: 0.047399938106536865, G Loss: -0.9999889135360718\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 256/516, D Loss: 0.1032295823097229, G Loss: -0.999989926815033\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 257/516, D Loss: 0.12077051401138306, G Loss: -0.9999862909317017\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 258/516, D Loss: 0.08982574939727783, G Loss: -0.9999871253967285\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 259/516, D Loss: 0.04179653525352478, G Loss: -0.9999854564666748\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 260/516, D Loss: 0.09626683592796326, G Loss: -0.9999879598617554\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 261/516, D Loss: 0.11147060990333557, G Loss: -0.99998939037323\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 262/516, D Loss: 0.11028283834457397, G Loss: -0.9999887347221375\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 263/516, D Loss: 0.1327548623085022, G Loss: -0.9999885559082031\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 264/516, D Loss: 0.07247138023376465, G Loss: -0.9999889135360718\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 265/516, D Loss: 0.0855642557144165, G Loss: -0.9999904036521912\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 266/516, D Loss: 0.0741199254989624, G Loss: -0.9999887943267822\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 267/516, D Loss: 0.07744336128234863, G Loss: -0.9999852180480957\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 268/516, D Loss: 0.07573184370994568, G Loss: -0.9999879598617554\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 269/516, D Loss: 0.051695317029953, G Loss: -0.9999822378158569\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 270/516, D Loss: 0.06828749179840088, G Loss: -0.9999914765357971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 271/516, D Loss: 0.08009693026542664, G Loss: -0.9999880194664001\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 272/516, D Loss: 0.13728243112564087, G Loss: -0.9999875426292419\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 273/516, D Loss: 0.08506953716278076, G Loss: -0.9999836087226868\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 274/516, D Loss: 0.1263967752456665, G Loss: -0.9999886155128479\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 275/516, D Loss: 0.09126293659210205, G Loss: -0.9999869465827942\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 276/516, D Loss: 0.06380051374435425, G Loss: -0.9999858140945435\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 277/516, D Loss: 0.06194669008255005, G Loss: -0.9999880790710449\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 278/516, D Loss: 0.07042402029037476, G Loss: -0.9999897480010986\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 279/516, D Loss: 0.10857957601547241, G Loss: -0.9999860525131226\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 280/516, D Loss: 0.05022400617599487, G Loss: -0.9999830722808838\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 281/516, D Loss: 0.106850266456604, G Loss: -0.9999871253967285\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 282/516, D Loss: 0.10012805461883545, G Loss: -0.999988853931427\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 283/516, D Loss: 0.07274070382118225, G Loss: -0.99998939037323\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 284/516, D Loss: 0.05376818776130676, G Loss: -0.9999901056289673\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 285/516, D Loss: 0.08009850978851318, G Loss: -0.9999889731407166\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 286/516, D Loss: 0.08635410666465759, G Loss: -0.9999913573265076\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 287/516, D Loss: 0.08681750297546387, G Loss: -0.9999881386756897\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 288/516, D Loss: 0.08829593658447266, G Loss: -0.999987781047821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 289/516, D Loss: 0.078046053647995, G Loss: -0.999990701675415\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 290/516, D Loss: 0.12087744474411011, G Loss: -0.9999885559082031\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 291/516, D Loss: 0.05626398324966431, G Loss: -0.9999892711639404\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 292/516, D Loss: 0.09121006727218628, G Loss: -0.9999896287918091\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 293/516, D Loss: 0.09303656220436096, G Loss: -0.9999879598617554\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 294/516, D Loss: 0.04066595435142517, G Loss: -0.9999915361404419\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 295/516, D Loss: 0.10948392748832703, G Loss: -0.9999877214431763\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 296/516, D Loss: 0.09918266534805298, G Loss: -0.9999861121177673\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 297/516, D Loss: 0.09067553281784058, G Loss: -0.9999892711639404\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 298/516, D Loss: 0.10612332820892334, G Loss: -0.9999895095825195\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 299/516, D Loss: 0.06722909212112427, G Loss: -0.9999912977218628\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 300/516, D Loss: 0.07530057430267334, G Loss: -0.999985933303833\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 301/516, D Loss: 0.06213700771331787, G Loss: -0.9999908804893494\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 302/516, D Loss: 0.0812670886516571, G Loss: -0.9999898672103882\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 303/516, D Loss: 0.12771788239479065, G Loss: -0.9999915957450867\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 304/516, D Loss: 0.09890663623809814, G Loss: -0.9999939799308777\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 305/516, D Loss: 0.059778839349746704, G Loss: -0.9999899864196777\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 306/516, D Loss: 0.06952250003814697, G Loss: -0.9999910593032837\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 307/516, D Loss: 0.09904584288597107, G Loss: -0.9999879598617554\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 308/516, D Loss: 0.07707768678665161, G Loss: -0.9999927878379822\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 309/516, D Loss: 0.12642204761505127, G Loss: -0.999984622001648\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 310/516, D Loss: 0.10703793168067932, G Loss: -0.999992311000824\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 311/516, D Loss: 0.12778374552726746, G Loss: -0.9999897480010986\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 312/516, D Loss: 0.15064087510108948, G Loss: -0.9999905824661255\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 313/516, D Loss: 0.09312820434570312, G Loss: -0.9999841451644897\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 314/516, D Loss: 0.09026145935058594, G Loss: -0.9999908804893494\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 315/516, D Loss: 0.10112962126731873, G Loss: -0.9999919533729553\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 316/516, D Loss: 0.07718449831008911, G Loss: -0.9999910593032837\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 317/516, D Loss: 0.10017499327659607, G Loss: -0.9999912977218628\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 318/516, D Loss: 0.07226607203483582, G Loss: -0.9999877214431763\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 319/516, D Loss: 0.11801880598068237, G Loss: -0.99998539686203\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 320/516, D Loss: 0.11771488189697266, G Loss: -0.9999908804893494\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 321/516, D Loss: 0.13940250873565674, G Loss: -0.9999908208847046\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 322/516, D Loss: 0.09637978672981262, G Loss: -0.9999897480010986\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 323/516, D Loss: 0.06734371185302734, G Loss: -0.9999917149543762\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 324/516, D Loss: 0.08111324906349182, G Loss: -0.9999901056289673\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 325/516, D Loss: 0.12718465924263, G Loss: -0.999991238117218\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 326/516, D Loss: 0.11549079418182373, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 327/516, D Loss: 0.097901850938797, G Loss: -0.9999873042106628\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 328/516, D Loss: 0.11840304732322693, G Loss: -0.9999897480010986\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 329/516, D Loss: 0.1575867235660553, G Loss: -0.9999889135360718\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 1, Batch 330/516, D Loss: 0.05480220913887024, G Loss: -0.9999914765357971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 331/516, D Loss: 0.10951471328735352, G Loss: -0.9999873638153076\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 332/516, D Loss: 0.08543926477432251, G Loss: -0.9999880790710449\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 333/516, D Loss: 0.11546623706817627, G Loss: -0.9999834895133972\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 334/516, D Loss: 0.08630752563476562, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 335/516, D Loss: 0.09652438759803772, G Loss: -0.9999889731407166\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 336/516, D Loss: 0.10082346200942993, G Loss: -0.9999895095825195\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 337/516, D Loss: 0.11462864279747009, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 338/516, D Loss: 0.09928476810455322, G Loss: -0.9999890327453613\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 339/516, D Loss: 0.10365617275238037, G Loss: -0.9999900460243225\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 340/516, D Loss: 0.13057532906532288, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 341/516, D Loss: 0.05978751182556152, G Loss: -0.9999931454658508\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 342/516, D Loss: 0.12917232513427734, G Loss: -0.9999891519546509\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 343/516, D Loss: 0.06516936421394348, G Loss: -0.9999923706054688\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 344/516, D Loss: 0.05988779664039612, G Loss: -0.999990701675415\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 345/516, D Loss: 0.07264071702957153, G Loss: -0.9999884366989136\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 346/516, D Loss: 0.09788388013839722, G Loss: -0.9999902248382568\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 347/516, D Loss: 0.10668179392814636, G Loss: -0.9999904036521912\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 348/516, D Loss: 0.09433779120445251, G Loss: -0.99998939037323\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 349/516, D Loss: 0.07685372233390808, G Loss: -0.9999861717224121\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 350/516, D Loss: 0.08202540874481201, G Loss: -0.9999916553497314\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 351/516, D Loss: 0.05589336156845093, G Loss: -0.999990701675415\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 352/516, D Loss: 0.09963557124137878, G Loss: -0.9999914169311523\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 353/516, D Loss: 0.11565059423446655, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 354/516, D Loss: 0.11794987320899963, G Loss: -0.9999886155128479\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 355/516, D Loss: 0.08931070566177368, G Loss: -0.9999921321868896\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 356/516, D Loss: 0.07104673981666565, G Loss: -0.9999904036521912\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 357/516, D Loss: 0.08577331900596619, G Loss: -0.999993085861206\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 358/516, D Loss: 0.08220541477203369, G Loss: -0.9999890327453613\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 359/516, D Loss: 0.03746378421783447, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 360/516, D Loss: 0.054504185914993286, G Loss: -0.9999901056289673\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 361/516, D Loss: 0.08383259177207947, G Loss: -0.9999933838844299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 362/516, D Loss: 0.11164304614067078, G Loss: -0.9999900460243225\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 363/516, D Loss: 0.11675947904586792, G Loss: -0.9999908804893494\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 364/516, D Loss: 0.049652278423309326, G Loss: -0.9999926686286926\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 365/516, D Loss: 0.09848380088806152, G Loss: -0.9999932646751404\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 366/516, D Loss: 0.10224512219429016, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 1, Batch 367/516, D Loss: 0.0689879059791565, G Loss: -0.999992311000824\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 368/516, D Loss: 0.09757229685783386, G Loss: -0.9999901652336121\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 369/516, D Loss: 0.10215246677398682, G Loss: -0.9999892115592957\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 370/516, D Loss: 0.09548527002334595, G Loss: -0.9999902248382568\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 371/516, D Loss: 0.09814232587814331, G Loss: -0.9999891519546509\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 372/516, D Loss: 0.0649409294128418, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 373/516, D Loss: 0.14672976732254028, G Loss: -0.9999905824661255\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 374/516, D Loss: 0.09096527099609375, G Loss: -0.9999930262565613\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 375/516, D Loss: 0.0894746482372284, G Loss: -0.9999914765357971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 376/516, D Loss: 0.07756307721138, G Loss: -0.9999896287918091\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 377/516, D Loss: 0.06449249386787415, G Loss: -0.9999896883964539\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 378/516, D Loss: 0.1429775357246399, G Loss: -0.9999907612800598\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 379/516, D Loss: 0.10949981212615967, G Loss: -0.9999920129776001\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 380/516, D Loss: 0.05596914887428284, G Loss: -0.9999905824661255\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 381/516, D Loss: 0.0917794406414032, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 382/516, D Loss: 0.07599291205406189, G Loss: -0.9999861717224121\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 383/516, D Loss: 0.06416067481040955, G Loss: -0.9999914169311523\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 384/516, D Loss: 0.11203095316886902, G Loss: -0.9999914169311523\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 385/516, D Loss: 0.0913158655166626, G Loss: -0.9999908208847046\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 386/516, D Loss: 0.11785957217216492, G Loss: -0.9999881982803345\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 387/516, D Loss: 0.04905217885971069, G Loss: -0.9999886155128479\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 388/516, D Loss: 0.08858740329742432, G Loss: -0.9999901056289673\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 389/516, D Loss: 0.08314254879951477, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 390/516, D Loss: 0.14318609237670898, G Loss: -0.9999892711639404\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 391/516, D Loss: 0.12988251447677612, G Loss: -0.9999901652336121\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 392/516, D Loss: 0.11104732751846313, G Loss: -0.9999929666519165\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 393/516, D Loss: 0.07274085283279419, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 394/516, D Loss: 0.07701346278190613, G Loss: -0.9999919533729553\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 395/516, D Loss: 0.06660661101341248, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 396/516, D Loss: 0.06431177258491516, G Loss: -0.9999905228614807\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 397/516, D Loss: 0.07634156942367554, G Loss: -0.9999895095825195\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 398/516, D Loss: 0.11278069019317627, G Loss: -0.9999933838844299\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 399/516, D Loss: 0.1121523380279541, G Loss: -0.9999916553497314\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 400/516, D Loss: 0.10879272222518921, G Loss: -0.9999911785125732\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 401/516, D Loss: 0.10616147518157959, G Loss: -0.999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 402/516, D Loss: 0.10052692890167236, G Loss: -0.9999915957450867\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 403/516, D Loss: 0.10014411807060242, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 404/516, D Loss: 0.0841837227344513, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 405/516, D Loss: 0.1277172565460205, G Loss: -0.9999929666519165\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 406/516, D Loss: 0.0844896137714386, G Loss: -0.9999920129776001\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 407/516, D Loss: 0.08831238746643066, G Loss: -0.9999911785125732\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 408/516, D Loss: 0.09965169429779053, G Loss: -0.9999912977218628\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 409/516, D Loss: 0.06528842449188232, G Loss: -0.9999932646751404\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 410/516, D Loss: 0.13442113995552063, G Loss: -0.9999898672103882\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 411/516, D Loss: 0.07418152689933777, G Loss: -0.9999916553497314\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 412/516, D Loss: 0.11242258548736572, G Loss: -0.9999908208847046\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 413/516, D Loss: 0.13258835673332214, G Loss: -0.9999921321868896\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 414/516, D Loss: 0.09246814250946045, G Loss: -0.9999905824661255\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 415/516, D Loss: 0.07167920470237732, G Loss: -0.9999913573265076\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 416/516, D Loss: 0.10071823000907898, G Loss: -0.9999923706054688\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 417/516, D Loss: 0.11448866128921509, G Loss: -0.9999907612800598\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 418/516, D Loss: 0.09901502728462219, G Loss: -0.9999932050704956\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 419/516, D Loss: 0.09596312046051025, G Loss: -0.9999920129776001\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 420/516, D Loss: 0.10018706321716309, G Loss: -0.9999905824661255\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 421/516, D Loss: 0.12199515104293823, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 422/516, D Loss: 0.10628354549407959, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 423/516, D Loss: 0.09667974710464478, G Loss: -0.9999915361404419\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 424/516, D Loss: 0.0820435881614685, G Loss: -0.9999934434890747\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 425/516, D Loss: 0.09764689207077026, G Loss: -0.9999912977218628\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 426/516, D Loss: 0.05893757939338684, G Loss: -0.999990701675415\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 427/516, D Loss: 0.10246717929840088, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 428/516, D Loss: 0.08110383152961731, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 429/516, D Loss: 0.09049391746520996, G Loss: -0.9999910593032837\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 430/516, D Loss: 0.13302543759346008, G Loss: -0.99998939037323\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 431/516, D Loss: 0.0761922299861908, G Loss: -0.9999891519546509\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 432/516, D Loss: 0.06532484292984009, G Loss: -0.9999891519546509\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 433/516, D Loss: 0.09662783145904541, G Loss: -0.9999920725822449\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 434/516, D Loss: 0.1133280098438263, G Loss: -0.9999894499778748\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 435/516, D Loss: 0.1332072913646698, G Loss: -0.9999905824661255\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 436/516, D Loss: 0.10996514558792114, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 437/516, D Loss: 0.08941298723220825, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 438/516, D Loss: 0.11420115828514099, G Loss: -0.9999921321868896\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 439/516, D Loss: 0.07826387882232666, G Loss: -0.999991238117218\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 440/516, D Loss: 0.09646829962730408, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 441/516, D Loss: 0.13141858577728271, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 442/516, D Loss: 0.11826619505882263, G Loss: -0.9999943971633911\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 443/516, D Loss: 0.034811437129974365, G Loss: -0.9999938607215881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 444/516, D Loss: 0.08003842830657959, G Loss: -0.999992847442627\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 445/516, D Loss: 0.11971992254257202, G Loss: -0.9999904632568359\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 446/516, D Loss: 0.10700222849845886, G Loss: -0.9999918937683105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 447/516, D Loss: 0.12503740191459656, G Loss: -0.9999895691871643\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 448/516, D Loss: 0.0851677656173706, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 449/516, D Loss: 0.08146622776985168, G Loss: -0.9999932050704956\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 450/516, D Loss: 0.10243573784828186, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 451/516, D Loss: 0.09930911660194397, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 452/516, D Loss: 0.07704788446426392, G Loss: -0.9999902248382568\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 453/516, D Loss: 0.1202089786529541, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 454/516, D Loss: 0.10657384991645813, G Loss: -0.9999940395355225\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 455/516, D Loss: 0.09747228026390076, G Loss: -0.9999909400939941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 456/516, D Loss: 0.11000680923461914, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 457/516, D Loss: 0.08062079548835754, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 458/516, D Loss: 0.07736676931381226, G Loss: -0.9999937415122986\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 459/516, D Loss: 0.08582517504692078, G Loss: -0.9999930262565613\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 460/516, D Loss: 0.06765848398208618, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 461/516, D Loss: 0.07066428661346436, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 462/516, D Loss: 0.10857772827148438, G Loss: -0.9999926686286926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 463/516, D Loss: 0.11103150248527527, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 464/516, D Loss: 0.07242822647094727, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 465/516, D Loss: 0.07025820016860962, G Loss: -0.9999918937683105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 466/516, D Loss: 0.09455913305282593, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 467/516, D Loss: 0.06641218066215515, G Loss: -0.9999919533729553\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 468/516, D Loss: 0.10374820232391357, G Loss: -0.9999927878379822\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 469/516, D Loss: 0.09873020648956299, G Loss: -0.9999940395355225\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 470/516, D Loss: 0.1340581476688385, G Loss: -0.9999907612800598\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 471/516, D Loss: 0.13572421669960022, G Loss: -0.9999924302101135\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 472/516, D Loss: 0.10348305106163025, G Loss: -0.9999918937683105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 473/516, D Loss: 0.10893690586090088, G Loss: -0.9999937415122986\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 474/516, D Loss: 0.08635368943214417, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 475/516, D Loss: 0.1007014811038971, G Loss: -0.9999938607215881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 476/516, D Loss: 0.09290182590484619, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 477/516, D Loss: 0.11076971888542175, G Loss: -0.9999921917915344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 478/516, D Loss: 0.10518544912338257, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 479/516, D Loss: 0.09433594346046448, G Loss: -0.9999951124191284\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 480/516, D Loss: 0.09801304340362549, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 481/516, D Loss: 0.1162605881690979, G Loss: -0.9999937415122986\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 482/516, D Loss: 0.1046183705329895, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 483/516, D Loss: 0.11131128668785095, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 484/516, D Loss: 0.08963534235954285, G Loss: -0.9999931454658508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 485/516, D Loss: 0.10177868604660034, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 486/516, D Loss: 0.10207116603851318, G Loss: -0.9999914765357971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 487/516, D Loss: 0.1213894784450531, G Loss: -0.9999934434890747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 488/516, D Loss: 0.08139893412590027, G Loss: -0.999993085861206\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 489/516, D Loss: 0.07949581742286682, G Loss: -0.999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 490/516, D Loss: 0.06677788496017456, G Loss: -0.9999929666519165\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 491/516, D Loss: 0.12731188535690308, G Loss: -0.9999931454658508\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 492/516, D Loss: 0.09782123565673828, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 493/516, D Loss: 0.10240569710731506, G Loss: -0.9999929666519165\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 494/516, D Loss: 0.062494754791259766, G Loss: -0.9999909400939941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 495/516, D Loss: 0.06361311674118042, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 496/516, D Loss: 0.10319846868515015, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 497/516, D Loss: 0.09447124600410461, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 498/516, D Loss: 0.1245015561580658, G Loss: -0.9999933838844299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 499/516, D Loss: 0.10960960388183594, G Loss: -0.9999916553497314\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 500/516, D Loss: 0.09701818227767944, G Loss: -0.9999893307685852\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 501/516, D Loss: 0.10465207695960999, G Loss: -0.9999929070472717\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 502/516, D Loss: 0.14829808473587036, G Loss: -0.9999931454658508\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 503/516, D Loss: 0.08603334426879883, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 504/516, D Loss: 0.08542978763580322, G Loss: -0.999993085861206\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 505/516, D Loss: 0.1100434958934784, G Loss: -0.9999927878379822\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 506/516, D Loss: 0.09820419549942017, G Loss: -0.9999895095825195\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 507/516, D Loss: 0.09527274966239929, G Loss: -0.9999940395355225\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 508/516, D Loss: 0.12876388430595398, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 509/516, D Loss: 0.09532544016838074, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 510/516, D Loss: 0.06827589869499207, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 511/516, D Loss: 0.10633549094200134, G Loss: -0.999993085861206\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 512/516, D Loss: 0.08584544062614441, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 513/516, D Loss: 0.08281746506690979, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 514/516, D Loss: 0.09632512927055359, G Loss: -0.9999940395355225\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 515/516, D Loss: 0.0893213152885437, G Loss: -0.999993085861206\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 516/516, D Loss: 0.10663765668869019, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 1/516, D Loss: 0.08031642436981201, G Loss: -0.9999940991401672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 2/516, D Loss: 0.07006201148033142, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 3/516, D Loss: 0.08978450298309326, G Loss: -0.9999937415122986\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 4/516, D Loss: 0.09157001972198486, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 5/516, D Loss: 0.09799835085868835, G Loss: -0.9999943971633911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 6/516, D Loss: 0.08509930968284607, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 7/516, D Loss: 0.05045008659362793, G Loss: -0.9999951124191284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 8/516, D Loss: 0.0542776882648468, G Loss: -0.9999889135360718\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 9/516, D Loss: 0.08708128333091736, G Loss: -0.9999915957450867\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 10/516, D Loss: 0.1059626042842865, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 11/516, D Loss: 0.10265940427780151, G Loss: -0.9999938607215881\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 12/516, D Loss: 0.11217141151428223, G Loss: -0.9999905228614807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 13/516, D Loss: 0.10819509625434875, G Loss: -0.9999947547912598\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 14/516, D Loss: 0.1111571192741394, G Loss: -0.9999949336051941\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 15/516, D Loss: 0.16745254397392273, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 16/516, D Loss: 0.08164164423942566, G Loss: -0.999993622303009\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 17/516, D Loss: 0.09945434331893921, G Loss: -0.9999940395355225\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 18/516, D Loss: 0.0436398983001709, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 19/516, D Loss: 0.0917879045009613, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 20/516, D Loss: 0.13010552525520325, G Loss: -0.9999933838844299\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 21/516, D Loss: 0.10975039005279541, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 22/516, D Loss: 0.07247179746627808, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 23/516, D Loss: 0.10356855392456055, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 24/516, D Loss: 0.11141973733901978, G Loss: -0.9999949932098389\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 25/516, D Loss: 0.1247851550579071, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 26/516, D Loss: 0.08478215336799622, G Loss: -0.9999943375587463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 27/516, D Loss: 0.07472455501556396, G Loss: -0.9999951124191284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 28/516, D Loss: 0.07407990097999573, G Loss: -0.9999938607215881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 29/516, D Loss: 0.10387659072875977, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 30/516, D Loss: 0.07078975439071655, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 31/516, D Loss: 0.12348291277885437, G Loss: -0.9999942183494568\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 32/516, D Loss: 0.09488511085510254, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 33/516, D Loss: 0.122616708278656, G Loss: -0.9999945759773254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 34/516, D Loss: 0.07159048318862915, G Loss: -0.9999945759773254\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 35/516, D Loss: 0.11031007766723633, G Loss: -0.999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 36/516, D Loss: 0.10770520567893982, G Loss: -0.9999909400939941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 37/516, D Loss: 0.12464368343353271, G Loss: -0.9999951124191284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 38/516, D Loss: 0.08367657661437988, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 39/516, D Loss: 0.07788673043251038, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 40/516, D Loss: 0.11347141861915588, G Loss: -0.9999927878379822\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 41/516, D Loss: 0.09350544214248657, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 42/516, D Loss: 0.08750176429748535, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 43/516, D Loss: 0.09916806221008301, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 44/516, D Loss: 0.09035733342170715, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 45/516, D Loss: 0.08878454566001892, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 46/516, D Loss: 0.08956101536750793, G Loss: -0.9999932050704956\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 47/516, D Loss: 0.07969728112220764, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 48/516, D Loss: 0.12371483445167542, G Loss: -0.9999943375587463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 49/516, D Loss: 0.07543450593948364, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 50/516, D Loss: 0.04015633463859558, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 51/516, D Loss: 0.0756167471408844, G Loss: -0.9999932646751404\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 52/516, D Loss: 0.10665297508239746, G Loss: -0.9999950528144836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 53/516, D Loss: 0.1148560643196106, G Loss: -0.9999944567680359\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 54/516, D Loss: 0.1231490969657898, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 55/516, D Loss: 0.059750527143478394, G Loss: -0.9999938607215881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 56/516, D Loss: 0.12224447727203369, G Loss: -0.9999916553497314\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 57/516, D Loss: 0.09154510498046875, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 58/516, D Loss: 0.1144266426563263, G Loss: -0.999993622303009\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 59/516, D Loss: 0.07596039772033691, G Loss: -0.9999951720237732\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 60/516, D Loss: 0.1208241879940033, G Loss: -0.9999929666519165\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 61/516, D Loss: 0.11418119072914124, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 62/516, D Loss: 0.11376872658729553, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 63/516, D Loss: 0.07917296886444092, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 64/516, D Loss: 0.1160886287689209, G Loss: -0.9999943375587463\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 65/516, D Loss: 0.10702598094940186, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 66/516, D Loss: 0.08004921674728394, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 67/516, D Loss: 0.07908311486244202, G Loss: -0.9999947547912598\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 68/516, D Loss: 0.07724353671073914, G Loss: -0.9999921321868896\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 69/516, D Loss: 0.12441956996917725, G Loss: -0.9999951124191284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 70/516, D Loss: 0.10836002230644226, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 71/516, D Loss: 0.08588701486587524, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 72/516, D Loss: 0.08088818192481995, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 73/516, D Loss: 0.10289400815963745, G Loss: -0.9999947547912598\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 74/516, D Loss: 0.05591025948524475, G Loss: -0.9999949336051941\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 75/516, D Loss: 0.11355236172676086, G Loss: -0.9999940395355225\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 76/516, D Loss: 0.0640135407447815, G Loss: -0.9999949336051941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 77/516, D Loss: 0.03630074858665466, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 78/516, D Loss: 0.0711182951927185, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 79/516, D Loss: 0.05561399459838867, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 80/516, D Loss: 0.03805956244468689, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 81/516, D Loss: 0.07271760702133179, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 82/516, D Loss: 0.07566618919372559, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 83/516, D Loss: 0.10302850604057312, G Loss: -0.9999932050704956\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 84/516, D Loss: 0.077776700258255, G Loss: -0.9999950528144836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 85/516, D Loss: 0.12249910831451416, G Loss: -0.9999932646751404\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 86/516, D Loss: 0.07954096794128418, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 87/516, D Loss: 0.08519670367240906, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "Epoch 2, Batch 88/516, D Loss: 0.11229115724563599, G Loss: -0.9999940395355225\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 89/516, D Loss: 0.07906991243362427, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 90/516, D Loss: 0.09478634595870972, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 91/516, D Loss: 0.09417217969894409, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 92/516, D Loss: 0.05912688374519348, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 93/516, D Loss: 0.07285809516906738, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 94/516, D Loss: 0.1098690927028656, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 95/516, D Loss: 0.10109648108482361, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 96/516, D Loss: 0.06601497530937195, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 97/516, D Loss: 0.13561445474624634, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 98/516, D Loss: 0.07340556383132935, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 99/516, D Loss: 0.10749933123588562, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 100/516, D Loss: 0.11518698930740356, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 101/516, D Loss: 0.06848505139350891, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 102/516, D Loss: 0.12728354334831238, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 103/516, D Loss: 0.11325517296791077, G Loss: -0.999995768070221\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 104/516, D Loss: 0.06305688619613647, G Loss: -0.9999945759773254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 105/516, D Loss: 0.09734225273132324, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 106/516, D Loss: 0.09253808856010437, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 107/516, D Loss: 0.07085323333740234, G Loss: -0.9999938607215881\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 108/516, D Loss: 0.0880824625492096, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 109/516, D Loss: 0.08243125677108765, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 110/516, D Loss: 0.07857736945152283, G Loss: -0.9999967217445374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 111/516, D Loss: 0.07998102903366089, G Loss: -0.9999943971633911\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 112/516, D Loss: 0.07923415303230286, G Loss: -0.9999907612800598\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 113/516, D Loss: 0.09922394156455994, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 114/516, D Loss: 0.09692990779876709, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 115/516, D Loss: 0.09361952543258667, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 116/516, D Loss: 0.0685853660106659, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 117/516, D Loss: 0.0710189938545227, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 118/516, D Loss: 0.05314686894416809, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 119/516, D Loss: 0.09954115748405457, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 120/516, D Loss: 0.07512286305427551, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 121/516, D Loss: 0.09868624806404114, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 122/516, D Loss: 0.11952763795852661, G Loss: -0.9999921917915344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 123/516, D Loss: 0.06114193797111511, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 124/516, D Loss: 0.053059399127960205, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 125/516, D Loss: 0.08685189485549927, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 126/516, D Loss: 0.09604281187057495, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 127/516, D Loss: 0.11504185199737549, G Loss: -0.9999951124191284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 128/516, D Loss: 0.11741390824317932, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 129/516, D Loss: 0.10038959980010986, G Loss: -0.9999944567680359\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 130/516, D Loss: 0.1114138662815094, G Loss: -0.999994695186615\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 131/516, D Loss: 0.11258000135421753, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 132/516, D Loss: 0.07959955930709839, G Loss: -0.9999963641166687\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 133/516, D Loss: 0.05145719647407532, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 134/516, D Loss: 0.10767209529876709, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 135/516, D Loss: 0.09066930413246155, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 136/516, D Loss: 0.12752079963684082, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 137/516, D Loss: 0.07065355777740479, G Loss: -0.9999968409538269\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 138/516, D Loss: 0.08465924859046936, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 139/516, D Loss: 0.08775520324707031, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 140/516, D Loss: 0.10585197806358337, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 141/516, D Loss: 0.1095600426197052, G Loss: -0.9999947547912598\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 142/516, D Loss: 0.11613759398460388, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 143/516, D Loss: 0.08681601285934448, G Loss: -0.9999943971633911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 144/516, D Loss: 0.10211813449859619, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 145/516, D Loss: 0.06418544054031372, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 146/516, D Loss: 0.07132714986801147, G Loss: -0.9999949336051941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 147/516, D Loss: 0.0639997124671936, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 148/516, D Loss: 0.0989001989364624, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 149/516, D Loss: 0.06590026617050171, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 150/516, D Loss: 0.10043302178382874, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 151/516, D Loss: 0.09568053483963013, G Loss: -0.9999956488609314\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 152/516, D Loss: 0.06502175331115723, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 153/516, D Loss: 0.06522059440612793, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 154/516, D Loss: 0.07192188501358032, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 155/516, D Loss: 0.12019151449203491, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 156/516, D Loss: 0.10877197980880737, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 157/516, D Loss: 0.1223592460155487, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 158/516, D Loss: 0.09667295217514038, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 159/516, D Loss: 0.08382153511047363, G Loss: -0.9999955296516418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 160/516, D Loss: 0.11370795965194702, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 161/516, D Loss: 0.12209099531173706, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 162/516, D Loss: 0.09400823712348938, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 163/516, D Loss: 0.08504307270050049, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 164/516, D Loss: 0.09340846538543701, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 165/516, D Loss: 0.1059333086013794, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 166/516, D Loss: 0.07380944490432739, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 167/516, D Loss: 0.0555691123008728, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 168/516, D Loss: 0.10321399569511414, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 169/516, D Loss: 0.08670702576637268, G Loss: -0.9999934434890747\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 170/516, D Loss: 0.08402711153030396, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 171/516, D Loss: 0.08503976464271545, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 172/516, D Loss: 0.09706932306289673, G Loss: -0.9999954104423523\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 173/516, D Loss: 0.1276029646396637, G Loss: -0.9999943971633911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 174/516, D Loss: 0.07376939058303833, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 175/516, D Loss: 0.18410474061965942, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 176/516, D Loss: 0.0799781084060669, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 177/516, D Loss: 0.11128756403923035, G Loss: -0.9999949932098389\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 178/516, D Loss: 0.07569259405136108, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 179/516, D Loss: 0.12237611413002014, G Loss: -0.999995768070221\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 180/516, D Loss: 0.08864101767539978, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 181/516, D Loss: 0.06427586078643799, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 182/516, D Loss: 0.08004474639892578, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 183/516, D Loss: 0.1000223159790039, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 184/516, D Loss: 0.11150878667831421, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 185/516, D Loss: 0.0445232093334198, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 186/516, D Loss: 0.059862762689590454, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 187/516, D Loss: 0.0654517114162445, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 188/516, D Loss: 0.07091403007507324, G Loss: -0.999994695186615\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 189/516, D Loss: 0.11144426465034485, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 190/516, D Loss: 0.08279210329055786, G Loss: -0.9999956488609314\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 191/516, D Loss: 0.08120739459991455, G Loss: -0.9999962449073792\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 192/516, D Loss: 0.09992378950119019, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 193/516, D Loss: 0.08178943395614624, G Loss: -0.999993085861206\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 194/516, D Loss: 0.10763907432556152, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 195/516, D Loss: 0.11832180619239807, G Loss: -0.9999956488609314\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 196/516, D Loss: 0.13739719986915588, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 197/516, D Loss: 0.110493004322052, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 198/516, D Loss: 0.13903141021728516, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 199/516, D Loss: 0.0686497688293457, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 200/516, D Loss: 0.07951021194458008, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 201/516, D Loss: 0.05463528633117676, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 202/516, D Loss: 0.1190742552280426, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 203/516, D Loss: 0.08694013953208923, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 204/516, D Loss: 0.11467123031616211, G Loss: -0.9999971985816956\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 205/516, D Loss: 0.04494151473045349, G Loss: -0.9999961256980896\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 206/516, D Loss: 0.1214112639427185, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 207/516, D Loss: 0.10106462240219116, G Loss: -0.9999962449073792\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 208/516, D Loss: 0.1057615876197815, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 209/516, D Loss: 0.09964492917060852, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 210/516, D Loss: 0.09854793548583984, G Loss: -0.9999963641166687\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 211/516, D Loss: 0.1325547993183136, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 212/516, D Loss: 0.07634252309799194, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 213/516, D Loss: 0.11693501472473145, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 214/516, D Loss: 0.07265830039978027, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 215/516, D Loss: 0.06820222735404968, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 216/516, D Loss: 0.10739657282829285, G Loss: -0.9999969601631165\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 217/516, D Loss: 0.10509315133094788, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 218/516, D Loss: 0.06261420249938965, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 219/516, D Loss: 0.051580190658569336, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 220/516, D Loss: 0.13171735405921936, G Loss: -0.9999966025352478\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 221/516, D Loss: 0.09626531600952148, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 222/516, D Loss: 0.09245225787162781, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 223/516, D Loss: 0.1364535093307495, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 224/516, D Loss: 0.1322963535785675, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 225/516, D Loss: 0.11036920547485352, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 226/516, D Loss: 0.11298233270645142, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 227/516, D Loss: 0.07935965061187744, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 228/516, D Loss: 0.08994638919830322, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 229/516, D Loss: 0.08737808465957642, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 230/516, D Loss: 0.08156439661979675, G Loss: -0.9999969601631165\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 231/516, D Loss: 0.07981470227241516, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 232/516, D Loss: 0.09683269262313843, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 233/516, D Loss: 0.03900071978569031, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 234/516, D Loss: 0.0768442153930664, G Loss: -0.9999951720237732\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 235/516, D Loss: 0.0894981324672699, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 236/516, D Loss: 0.08774283528327942, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 237/516, D Loss: 0.09755566716194153, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 238/516, D Loss: 0.09225478768348694, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 239/516, D Loss: 0.10576680302619934, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 240/516, D Loss: 0.11457377672195435, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 241/516, D Loss: 0.080648273229599, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 242/516, D Loss: 0.06777089834213257, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 243/516, D Loss: 0.08575955033302307, G Loss: -0.9999954104423523\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 244/516, D Loss: 0.10931652784347534, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 245/516, D Loss: 0.11425569653511047, G Loss: -0.9999962449073792\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 246/516, D Loss: 0.07856899499893188, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 247/516, D Loss: 0.09240922331809998, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 248/516, D Loss: 0.0924101173877716, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 249/516, D Loss: 0.05613905191421509, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 250/516, D Loss: 0.05151531100273132, G Loss: -0.9999969601631165\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 251/516, D Loss: 0.11632853746414185, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 252/516, D Loss: 0.07526624202728271, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 253/516, D Loss: 0.09618794918060303, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 254/516, D Loss: 0.08870828151702881, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 255/516, D Loss: 0.11509957909584045, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 256/516, D Loss: 0.10130000114440918, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 257/516, D Loss: 0.0964585542678833, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 2, Batch 258/516, D Loss: 0.11906638741493225, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 259/516, D Loss: 0.08727443218231201, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 260/516, D Loss: 0.07024359703063965, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 261/516, D Loss: 0.10935032367706299, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 262/516, D Loss: 0.12791717052459717, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 263/516, D Loss: 0.05695110559463501, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 264/516, D Loss: 0.09661146998405457, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 265/516, D Loss: 0.10294827818870544, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 266/516, D Loss: 0.08070456981658936, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 267/516, D Loss: 0.09716224670410156, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 268/516, D Loss: 0.09207186102867126, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 269/516, D Loss: 0.12583854794502258, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 270/516, D Loss: 0.08201998472213745, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 271/516, D Loss: 0.12440446019172668, G Loss: -0.9999971985816956\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 272/516, D Loss: 0.08943107724189758, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 273/516, D Loss: 0.10195490717887878, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 274/516, D Loss: 0.10768359899520874, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 275/516, D Loss: 0.0788189172744751, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 276/516, D Loss: 0.10775315761566162, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 277/516, D Loss: 0.08018282055854797, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 278/516, D Loss: 0.09255513548851013, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 279/516, D Loss: 0.08369988203048706, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 280/516, D Loss: 0.10794839262962341, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 281/516, D Loss: 0.08748283982276917, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 282/516, D Loss: 0.07897517085075378, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 283/516, D Loss: 0.07495972514152527, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 284/516, D Loss: 0.06309357285499573, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 285/516, D Loss: 0.05175614356994629, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 286/516, D Loss: 0.09619221091270447, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 287/516, D Loss: 0.09445303678512573, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 288/516, D Loss: 0.046508342027664185, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 289/516, D Loss: 0.10569125413894653, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 290/516, D Loss: 0.06804060935974121, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 291/516, D Loss: 0.06401515007019043, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 292/516, D Loss: 0.07500308752059937, G Loss: -0.9999973177909851\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 293/516, D Loss: 0.06343719363212585, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 294/516, D Loss: 0.09443897008895874, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 295/516, D Loss: 0.053575217723846436, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 296/516, D Loss: 0.09609973430633545, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 297/516, D Loss: 0.0694531798362732, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 298/516, D Loss: 0.10937777161598206, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 299/516, D Loss: 0.07545098662376404, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 300/516, D Loss: 0.1035953164100647, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 301/516, D Loss: 0.12699753046035767, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 302/516, D Loss: 0.07237818837165833, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 303/516, D Loss: 0.09411129355430603, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 304/516, D Loss: 0.06381380558013916, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 305/516, D Loss: 0.07530641555786133, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 306/516, D Loss: 0.13182413578033447, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 307/516, D Loss: 0.08086049556732178, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 308/516, D Loss: 0.10897499322891235, G Loss: -0.9999963641166687\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 309/516, D Loss: 0.044160544872283936, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 310/516, D Loss: 0.14795541763305664, G Loss: -0.9999968409538269\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 311/516, D Loss: 0.09770834445953369, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 312/516, D Loss: 0.11991006135940552, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 313/516, D Loss: 0.07323822379112244, G Loss: -0.9999961256980896\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 314/516, D Loss: 0.07756590843200684, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 315/516, D Loss: 0.10862791538238525, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 316/516, D Loss: 0.10939490795135498, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 317/516, D Loss: 0.11825242638587952, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 318/516, D Loss: 0.13599559664726257, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 319/516, D Loss: 0.07403188943862915, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 320/516, D Loss: 0.07704085111618042, G Loss: -0.9999967217445374\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 321/516, D Loss: 0.08646577596664429, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 322/516, D Loss: 0.12687504291534424, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 323/516, D Loss: 0.13230064511299133, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 324/516, D Loss: 0.09975621104240417, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 325/516, D Loss: 0.1241404116153717, G Loss: -0.9999969601631165\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 326/516, D Loss: 0.08223754167556763, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 327/516, D Loss: 0.06586509943008423, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 328/516, D Loss: 0.08459171652793884, G Loss: -0.9999973177909851\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 329/516, D Loss: 0.054999351501464844, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 330/516, D Loss: 0.10934275388717651, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 331/516, D Loss: 0.11182278394699097, G Loss: -0.9999968409538269\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 332/516, D Loss: 0.08841747045516968, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 333/516, D Loss: 0.09018641710281372, G Loss: -0.9999968409538269\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 334/516, D Loss: 0.09445536136627197, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 335/516, D Loss: 0.10340538620948792, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 336/516, D Loss: 0.0959087610244751, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 337/516, D Loss: 0.07129037380218506, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 338/516, D Loss: 0.08563846349716187, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 339/516, D Loss: 0.0900733470916748, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 340/516, D Loss: 0.0851830542087555, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 341/516, D Loss: 0.10309898853302002, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 342/516, D Loss: 0.07342636585235596, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 343/516, D Loss: 0.11591184139251709, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 344/516, D Loss: 0.09684973955154419, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 345/516, D Loss: 0.07469505071640015, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 346/516, D Loss: 0.08405587077140808, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 347/516, D Loss: 0.08038091659545898, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 348/516, D Loss: 0.14428937435150146, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 349/516, D Loss: 0.12067228555679321, G Loss: -0.9999973177909851\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 350/516, D Loss: 0.06600838899612427, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 351/516, D Loss: 0.10433885455131531, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 352/516, D Loss: 0.1087995171546936, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 353/516, D Loss: 0.08256125450134277, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 354/516, D Loss: 0.08352088928222656, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 355/516, D Loss: 0.06239178776741028, G Loss: -0.9999969601631165\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 356/516, D Loss: 0.09669935703277588, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 357/516, D Loss: 0.09419149160385132, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 358/516, D Loss: 0.14520347118377686, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 359/516, D Loss: 0.07120835781097412, G Loss: -0.9999973177909851\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 360/516, D Loss: 0.0914837121963501, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 361/516, D Loss: 0.10429763793945312, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 362/516, D Loss: 0.08676883578300476, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 363/516, D Loss: 0.06647872924804688, G Loss: -0.9999966025352478\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 364/516, D Loss: 0.09447470307350159, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 365/516, D Loss: 0.13276904821395874, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 366/516, D Loss: 0.11159265041351318, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 367/516, D Loss: 0.07370245456695557, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 368/516, D Loss: 0.05202978849411011, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 369/516, D Loss: 0.1072482168674469, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 370/516, D Loss: 0.06478476524353027, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 371/516, D Loss: 0.0748550295829773, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 372/516, D Loss: 0.028059720993041992, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 373/516, D Loss: 0.07832133769989014, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 374/516, D Loss: 0.10813295841217041, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 375/516, D Loss: 0.08362677693367004, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 376/516, D Loss: 0.05239313840866089, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 377/516, D Loss: 0.06352481245994568, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 378/516, D Loss: 0.11094298958778381, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 379/516, D Loss: 0.10213920474052429, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 380/516, D Loss: 0.1110251247882843, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 381/516, D Loss: 0.07437184453010559, G Loss: -0.9999971985816956\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 382/516, D Loss: 0.09830057621002197, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 383/516, D Loss: 0.09010732173919678, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 384/516, D Loss: 0.09847822785377502, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 385/516, D Loss: 0.09724023938179016, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 386/516, D Loss: 0.13128051161766052, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 387/516, D Loss: 0.09121090173721313, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 388/516, D Loss: 0.10828393697738647, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 389/516, D Loss: 0.0890612006187439, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 390/516, D Loss: 0.08744078874588013, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 391/516, D Loss: 0.09594887495040894, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 392/516, D Loss: 0.099994957447052, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 393/516, D Loss: 0.11070075631141663, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 394/516, D Loss: 0.11941102147102356, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 395/516, D Loss: 0.08431199193000793, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 396/516, D Loss: 0.09860941767692566, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 397/516, D Loss: 0.06758058071136475, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 398/516, D Loss: 0.09828561544418335, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 399/516, D Loss: 0.11543571949005127, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 400/516, D Loss: 0.08862081170082092, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 401/516, D Loss: 0.05137479305267334, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 402/516, D Loss: 0.055400341749191284, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 403/516, D Loss: 0.07328248023986816, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 404/516, D Loss: 0.1001518964767456, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 405/516, D Loss: 0.06670650839805603, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 406/516, D Loss: 0.10009360313415527, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 407/516, D Loss: 0.11082440614700317, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 408/516, D Loss: 0.05688673257827759, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 409/516, D Loss: 0.20692303776741028, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 410/516, D Loss: 0.07350602746009827, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 411/516, D Loss: 0.11234122514724731, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 412/516, D Loss: 0.13839924335479736, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 413/516, D Loss: 0.0915634036064148, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 414/516, D Loss: 0.09323322772979736, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 415/516, D Loss: 0.08528703451156616, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 416/516, D Loss: 0.08459872007369995, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 417/516, D Loss: 0.08410796523094177, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 418/516, D Loss: 0.1073160171508789, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 419/516, D Loss: 0.1035824716091156, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 420/516, D Loss: 0.09574323892593384, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 421/516, D Loss: 0.09372946619987488, G Loss: -0.9999973177909851\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 422/516, D Loss: 0.11000311374664307, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 423/516, D Loss: 0.06595742702484131, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 424/516, D Loss: 0.08332082629203796, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 425/516, D Loss: 0.11345884203910828, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 426/516, D Loss: 0.09500205516815186, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 427/516, D Loss: 0.09561595320701599, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 428/516, D Loss: 0.11352801322937012, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 429/516, D Loss: 0.08257973194122314, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 430/516, D Loss: 0.09231853485107422, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 431/516, D Loss: 0.10321193933486938, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 432/516, D Loss: 0.057654500007629395, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 433/516, D Loss: 0.11247646808624268, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 434/516, D Loss: 0.0746161937713623, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 435/516, D Loss: 0.08106079697608948, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 436/516, D Loss: 0.08595621585845947, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 437/516, D Loss: 0.10162672400474548, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 438/516, D Loss: 0.10165643692016602, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 439/516, D Loss: 0.08335119485855103, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 440/516, D Loss: 0.07924667000770569, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 441/516, D Loss: 0.08478644490242004, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 442/516, D Loss: 0.06890848278999329, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 443/516, D Loss: 0.054327934980392456, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 444/516, D Loss: 0.08544465899467468, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 445/516, D Loss: 0.08416804671287537, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 446/516, D Loss: 0.1287328004837036, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 447/516, D Loss: 0.08046862483024597, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 448/516, D Loss: 0.08905470371246338, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 449/516, D Loss: 0.0820881724357605, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 450/516, D Loss: 0.07540005445480347, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 451/516, D Loss: 0.12676024436950684, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 452/516, D Loss: 0.08130371570587158, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 453/516, D Loss: 0.07959756255149841, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 454/516, D Loss: 0.08177295327186584, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 455/516, D Loss: 0.06945377588272095, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 456/516, D Loss: 0.10148343443870544, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 457/516, D Loss: 0.10703009366989136, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 458/516, D Loss: 0.07517227530479431, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 459/516, D Loss: 0.13201871514320374, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 460/516, D Loss: 0.08287447690963745, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 461/516, D Loss: 0.09013783931732178, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 462/516, D Loss: 0.149185448884964, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 463/516, D Loss: 0.04526522755622864, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 464/516, D Loss: 0.08313784003257751, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 465/516, D Loss: 0.09358826279640198, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 466/516, D Loss: 0.08182045817375183, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 467/516, D Loss: 0.09968045353889465, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 468/516, D Loss: 0.0859733521938324, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 469/516, D Loss: 0.07647275924682617, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 470/516, D Loss: 0.11044442653656006, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 471/516, D Loss: 0.1335284411907196, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 472/516, D Loss: 0.08533033728599548, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 473/516, D Loss: 0.08518326282501221, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 474/516, D Loss: 0.08530077338218689, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 475/516, D Loss: 0.05828368663787842, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 476/516, D Loss: 0.09529712796211243, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 477/516, D Loss: 0.08306223154067993, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 478/516, D Loss: 0.10610687732696533, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 479/516, D Loss: 0.12052607536315918, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 480/516, D Loss: 0.07419228553771973, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 481/516, D Loss: 0.09276682138442993, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 482/516, D Loss: 0.1040225625038147, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 483/516, D Loss: 0.112403005361557, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 484/516, D Loss: 0.08877328038215637, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 485/516, D Loss: 0.11942920088768005, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 486/516, D Loss: 0.1070961058139801, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 487/516, D Loss: 0.11217275261878967, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 488/516, D Loss: 0.07645487785339355, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 489/516, D Loss: 0.09082025289535522, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 490/516, D Loss: 0.054389119148254395, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 491/516, D Loss: 0.09065526723861694, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 492/516, D Loss: 0.08678758144378662, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 493/516, D Loss: 0.13914886116981506, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 494/516, D Loss: 0.08707845211029053, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 495/516, D Loss: 0.09607759118080139, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 496/516, D Loss: 0.16155683994293213, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 497/516, D Loss: 0.11886435747146606, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 498/516, D Loss: 0.08182358741760254, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 499/516, D Loss: 0.11788836121559143, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 500/516, D Loss: 0.09683394432067871, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 501/516, D Loss: 0.07098516821861267, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 502/516, D Loss: 0.11712789535522461, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 503/516, D Loss: 0.08351731300354004, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 504/516, D Loss: 0.08042407035827637, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 505/516, D Loss: 0.08521363139152527, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 506/516, D Loss: 0.07436588406562805, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 507/516, D Loss: 0.0854814350605011, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 508/516, D Loss: 0.07412099838256836, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 509/516, D Loss: 0.10600966215133667, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 510/516, D Loss: 0.12560072541236877, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 511/516, D Loss: 0.09012126922607422, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 512/516, D Loss: 0.12906131148338318, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 513/516, D Loss: 0.08771112561225891, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 514/516, D Loss: 0.08602035045623779, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 515/516, D Loss: 0.0861070454120636, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 516/516, D Loss: 0.10153618454933167, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 1/516, D Loss: 0.10085168480873108, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 2/516, D Loss: 0.10594522953033447, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 3/516, D Loss: 0.105182945728302, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 4/516, D Loss: 0.08822834491729736, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 5/516, D Loss: 0.07959446310997009, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 6/516, D Loss: 0.1335437297821045, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 7/516, D Loss: 0.08591842651367188, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 8/516, D Loss: 0.07590150833129883, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 9/516, D Loss: 0.08981376886367798, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 10/516, D Loss: 0.13298559188842773, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 11/516, D Loss: 0.11816468834877014, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 12/516, D Loss: 0.08104214072227478, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 13/516, D Loss: 0.12853297591209412, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 14/516, D Loss: 0.06223997473716736, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 15/516, D Loss: 0.10635092854499817, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 16/516, D Loss: 0.11782234907150269, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 17/516, D Loss: 0.11227631568908691, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 18/516, D Loss: 0.07641637325286865, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 19/516, D Loss: 0.0781659483909607, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 20/516, D Loss: 0.05797553062438965, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 21/516, D Loss: 0.093658447265625, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 22/516, D Loss: 0.07735961675643921, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 23/516, D Loss: 0.06340506672859192, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 24/516, D Loss: 0.072598397731781, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 25/516, D Loss: 0.06438285112380981, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 26/516, D Loss: 0.04959508776664734, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 27/516, D Loss: 0.09928140044212341, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 28/516, D Loss: 0.08814918994903564, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 29/516, D Loss: 0.050862282514572144, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 30/516, D Loss: 0.10604053735733032, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 31/516, D Loss: 0.10435199737548828, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 32/516, D Loss: 0.09852343797683716, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 33/516, D Loss: 0.08271843194961548, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 34/516, D Loss: 0.06550681591033936, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 35/516, D Loss: 0.11463361978530884, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 36/516, D Loss: 0.08496826887130737, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 37/516, D Loss: 0.11748522520065308, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 38/516, D Loss: 0.07269352674484253, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 39/516, D Loss: 0.09544673562049866, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 40/516, D Loss: 0.10597056150436401, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 41/516, D Loss: 0.087617427110672, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 42/516, D Loss: 0.07982781529426575, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 43/516, D Loss: 0.15231138467788696, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 44/516, D Loss: 0.08167314529418945, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 45/516, D Loss: 0.10438477993011475, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 46/516, D Loss: 0.0877315104007721, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 47/516, D Loss: 0.08494237065315247, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 48/516, D Loss: 0.07567617297172546, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 49/516, D Loss: 0.09600958228111267, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 50/516, D Loss: 0.06443986296653748, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 51/516, D Loss: 0.07388383150100708, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 52/516, D Loss: 0.11160439252853394, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 53/516, D Loss: 0.0929776132106781, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 54/516, D Loss: 0.0964440405368805, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 55/516, D Loss: 0.0729752779006958, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 56/516, D Loss: 0.07969838380813599, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 57/516, D Loss: 0.04093077778816223, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 58/516, D Loss: 0.09501764178276062, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 59/516, D Loss: 0.09377527236938477, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 60/516, D Loss: 0.126512348651886, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 61/516, D Loss: 0.07362079620361328, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 62/516, D Loss: 0.08933815360069275, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 63/516, D Loss: 0.07892873883247375, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 64/516, D Loss: 0.09620743989944458, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 65/516, D Loss: 0.09784039855003357, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 66/516, D Loss: 0.1073850691318512, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 67/516, D Loss: 0.0731419026851654, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 68/516, D Loss: 0.08412307500839233, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 69/516, D Loss: 0.08819037675857544, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 70/516, D Loss: 0.08015179634094238, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 71/516, D Loss: 0.07780173420906067, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 72/516, D Loss: 0.09882664680480957, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 73/516, D Loss: 0.0918952226638794, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 74/516, D Loss: 0.13454869389533997, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 75/516, D Loss: 0.08824974298477173, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 76/516, D Loss: 0.07640501856803894, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 77/516, D Loss: 0.08164322376251221, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 78/516, D Loss: 0.09778669476509094, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 79/516, D Loss: 0.06999778747558594, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 80/516, D Loss: 0.07194048166275024, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 81/516, D Loss: 0.07551607489585876, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 82/516, D Loss: 0.0602228045463562, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 83/516, D Loss: 0.07470643520355225, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 84/516, D Loss: 0.08948063850402832, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 85/516, D Loss: 0.09780478477478027, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 86/516, D Loss: 0.09142851829528809, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 87/516, D Loss: 0.07879707217216492, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 88/516, D Loss: 0.11557871103286743, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 89/516, D Loss: 0.0919385552406311, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 90/516, D Loss: 0.08611366152763367, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 91/516, D Loss: 0.07844746112823486, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 92/516, D Loss: 0.06639158725738525, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 93/516, D Loss: 0.06668823957443237, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 94/516, D Loss: 0.1004612147808075, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 95/516, D Loss: 0.14440873265266418, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 96/516, D Loss: 0.08398157358169556, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 97/516, D Loss: 0.06796830892562866, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 98/516, D Loss: 0.0894482433795929, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 99/516, D Loss: 0.07944858074188232, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 100/516, D Loss: 0.08339002728462219, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 101/516, D Loss: 0.09239727258682251, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 102/516, D Loss: 0.1067700982093811, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 103/516, D Loss: 0.12791690230369568, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 104/516, D Loss: 0.075298011302948, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 105/516, D Loss: 0.04885149002075195, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 106/516, D Loss: 0.13491666316986084, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 107/516, D Loss: 0.14146706461906433, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 108/516, D Loss: 0.06564953923225403, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 109/516, D Loss: 0.12361526489257812, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 110/516, D Loss: 0.07432445883750916, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 111/516, D Loss: 0.05984875559806824, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 112/516, D Loss: 0.09274035692214966, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 113/516, D Loss: 0.11435240507125854, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 114/516, D Loss: 0.09327289462089539, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 115/516, D Loss: 0.13403752446174622, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 116/516, D Loss: 0.11840209364891052, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 117/516, D Loss: 0.0940377414226532, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 118/516, D Loss: 0.08696863055229187, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 119/516, D Loss: 0.12770995497703552, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 120/516, D Loss: 0.07628688216209412, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 121/516, D Loss: 0.06562286615371704, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 122/516, D Loss: 0.045679062604904175, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 123/516, D Loss: 0.08577066659927368, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 124/516, D Loss: 0.0659322738647461, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 125/516, D Loss: 0.09860390424728394, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 126/516, D Loss: 0.1259625256061554, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 127/516, D Loss: 0.0843224823474884, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 128/516, D Loss: 0.07964512705802917, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 129/516, D Loss: 0.08461785316467285, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 130/516, D Loss: 0.08942931890487671, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 131/516, D Loss: 0.10696384310722351, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 132/516, D Loss: 0.07424843311309814, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 133/516, D Loss: 0.10172733664512634, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 134/516, D Loss: 0.09116148948669434, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 135/516, D Loss: 0.033050477504730225, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 136/516, D Loss: 0.07984176278114319, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 137/516, D Loss: 0.09314998984336853, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 138/516, D Loss: 0.06455367803573608, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 139/516, D Loss: 0.07477527856826782, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 3, Batch 140/516, D Loss: 0.06372547149658203, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 141/516, D Loss: 0.08595940470695496, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 142/516, D Loss: 0.1036137044429779, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 143/516, D Loss: 0.081269770860672, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 144/516, D Loss: 0.11027830839157104, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 145/516, D Loss: 0.10239315032958984, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 146/516, D Loss: 0.1255251169204712, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 147/516, D Loss: 0.08691981434822083, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 148/516, D Loss: 0.10615438222885132, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 149/516, D Loss: 0.06559687852859497, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 150/516, D Loss: 0.07767447829246521, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 151/516, D Loss: 0.10735666751861572, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 152/516, D Loss: 0.08729210495948792, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 153/516, D Loss: 0.08178567886352539, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 154/516, D Loss: 0.1301099956035614, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 155/516, D Loss: 0.07065877318382263, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 156/516, D Loss: 0.09750929474830627, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 157/516, D Loss: 0.08473968505859375, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 158/516, D Loss: 0.1101294457912445, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 159/516, D Loss: 0.10536631941795349, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 160/516, D Loss: 0.08166775107383728, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 161/516, D Loss: 0.08091437816619873, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 162/516, D Loss: 0.0756329596042633, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 163/516, D Loss: 0.10137408971786499, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 164/516, D Loss: 0.07860147953033447, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 165/516, D Loss: 0.06905737519264221, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 166/516, D Loss: 0.11686748266220093, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 167/516, D Loss: 0.08081820607185364, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 168/516, D Loss: 0.05407807230949402, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 169/516, D Loss: 0.06264263391494751, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 170/516, D Loss: 0.08108174800872803, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 171/516, D Loss: 0.1322462260723114, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 172/516, D Loss: 0.13367551565170288, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 173/516, D Loss: 0.11617821455001831, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 174/516, D Loss: 0.09084829688072205, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 175/516, D Loss: 0.1222333014011383, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 176/516, D Loss: 0.06658560037612915, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 177/516, D Loss: 0.10788315534591675, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 178/516, D Loss: 0.10636162757873535, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 179/516, D Loss: 0.06770303845405579, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 180/516, D Loss: 0.08581861853599548, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 181/516, D Loss: 0.10095220804214478, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 182/516, D Loss: 0.08261236548423767, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 183/516, D Loss: 0.07135146856307983, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 184/516, D Loss: 0.0898626446723938, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 185/516, D Loss: 0.08700963854789734, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 186/516, D Loss: 0.08684366941452026, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 187/516, D Loss: 0.08359155058860779, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 188/516, D Loss: 0.10024800896644592, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 189/516, D Loss: 0.059240520000457764, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 190/516, D Loss: 0.10340571403503418, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 191/516, D Loss: 0.09329825639724731, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 192/516, D Loss: 0.08011007308959961, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 193/516, D Loss: 0.0838487446308136, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 194/516, D Loss: 0.11771392822265625, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 195/516, D Loss: 0.08349484205245972, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 196/516, D Loss: 0.059412091970443726, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 197/516, D Loss: 0.10573437809944153, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 198/516, D Loss: 0.09004536271095276, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 199/516, D Loss: 0.09679466485977173, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 200/516, D Loss: 0.10393479466438293, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 201/516, D Loss: 0.12339389324188232, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 202/516, D Loss: 0.07138919830322266, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 203/516, D Loss: 0.09151643514633179, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 204/516, D Loss: 0.09293043613433838, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 205/516, D Loss: 0.06945359706878662, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 206/516, D Loss: 0.04072690010070801, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 207/516, D Loss: 0.08468028903007507, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 208/516, D Loss: 0.09128627181053162, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 209/516, D Loss: 0.09529009461402893, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 210/516, D Loss: 0.13998109102249146, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 211/516, D Loss: 0.06617310643196106, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 212/516, D Loss: 0.10112884640693665, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 213/516, D Loss: 0.07093259692192078, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 214/516, D Loss: 0.13606128096580505, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 215/516, D Loss: 0.10066556930541992, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 216/516, D Loss: 0.11212831735610962, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 217/516, D Loss: 0.14617645740509033, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 218/516, D Loss: 0.0794948935508728, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 219/516, D Loss: 0.04887676239013672, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 220/516, D Loss: 0.09063714742660522, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 221/516, D Loss: 0.08537554740905762, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 222/516, D Loss: 0.09481233358383179, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 223/516, D Loss: 0.07456976175308228, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 224/516, D Loss: 0.06800484657287598, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 225/516, D Loss: 0.09351611137390137, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 226/516, D Loss: 0.06650346517562866, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 227/516, D Loss: 0.11155393719673157, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 228/516, D Loss: 0.10167041420936584, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 229/516, D Loss: 0.08738106489181519, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 230/516, D Loss: 0.13532885909080505, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 231/516, D Loss: 0.07891413569450378, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 232/516, D Loss: 0.09641999006271362, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 233/516, D Loss: 0.10656225681304932, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 234/516, D Loss: 0.08719068765640259, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 235/516, D Loss: 0.10018432140350342, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 236/516, D Loss: 0.08627450466156006, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 237/516, D Loss: 0.06894126534461975, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 238/516, D Loss: 0.06153225898742676, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 239/516, D Loss: 0.10893002152442932, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 240/516, D Loss: 0.07758921384811401, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 241/516, D Loss: 0.15096688270568848, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 242/516, D Loss: 0.0773082971572876, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 243/516, D Loss: 0.09615102410316467, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 244/516, D Loss: 0.10554254055023193, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 245/516, D Loss: 0.07476133108139038, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 246/516, D Loss: 0.08617642521858215, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 247/516, D Loss: 0.1450023353099823, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 248/516, D Loss: 0.08339819312095642, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 249/516, D Loss: 0.078280508518219, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 250/516, D Loss: 0.11284869909286499, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 251/516, D Loss: 0.06660589575767517, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 252/516, D Loss: 0.07406920194625854, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 253/516, D Loss: 0.070956289768219, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 254/516, D Loss: 0.10485213994979858, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 255/516, D Loss: 0.11773675680160522, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 256/516, D Loss: 0.07348284125328064, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 257/516, D Loss: 0.14059346914291382, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 258/516, D Loss: 0.11458110809326172, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 259/516, D Loss: 0.0921577513217926, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 260/516, D Loss: 0.09650540351867676, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 261/516, D Loss: 0.09261900186538696, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 262/516, D Loss: 0.09444552659988403, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 263/516, D Loss: 0.10544613003730774, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 264/516, D Loss: 0.0992039144039154, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 265/516, D Loss: 0.0822208821773529, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 266/516, D Loss: 0.08731195330619812, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 267/516, D Loss: 0.10532516241073608, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 268/516, D Loss: 0.06492197513580322, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 269/516, D Loss: 0.07669588923454285, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 270/516, D Loss: 0.07643395662307739, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 271/516, D Loss: 0.12588337063789368, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 272/516, D Loss: 0.07428044080734253, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 273/516, D Loss: 0.10668322443962097, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 274/516, D Loss: 0.06958872079849243, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 275/516, D Loss: 0.08199512958526611, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 276/516, D Loss: 0.08960506319999695, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 277/516, D Loss: 0.123416006565094, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 278/516, D Loss: 0.10964709520339966, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 279/516, D Loss: 0.10073262453079224, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 280/516, D Loss: 0.08751717209815979, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 281/516, D Loss: 0.10083979368209839, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 282/516, D Loss: 0.12864169478416443, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 283/516, D Loss: 0.053041040897369385, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 284/516, D Loss: 0.084418386220932, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 285/516, D Loss: 0.08136206865310669, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 286/516, D Loss: 0.08881127834320068, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 287/516, D Loss: 0.10245609283447266, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 288/516, D Loss: 0.11951592564582825, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 289/516, D Loss: 0.08349117636680603, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 290/516, D Loss: 0.07874974608421326, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 291/516, D Loss: 0.10589224100112915, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 292/516, D Loss: 0.10604608058929443, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 293/516, D Loss: 0.09157642722129822, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 294/516, D Loss: 0.12902578711509705, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 295/516, D Loss: 0.08214473724365234, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 296/516, D Loss: 0.06654113531112671, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 297/516, D Loss: 0.08914405107498169, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 298/516, D Loss: 0.14904451370239258, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 299/516, D Loss: 0.041383177042007446, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 300/516, D Loss: 0.060089945793151855, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 301/516, D Loss: 0.09665784239768982, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 302/516, D Loss: 0.11608487367630005, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 303/516, D Loss: 0.0914868712425232, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 304/516, D Loss: 0.07908284664154053, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 305/516, D Loss: 0.09069916605949402, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 306/516, D Loss: 0.0659189224243164, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 307/516, D Loss: 0.09976553916931152, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 308/516, D Loss: 0.1182490885257721, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 309/516, D Loss: 0.09119048714637756, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 310/516, D Loss: 0.08151954412460327, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 311/516, D Loss: 0.09263348579406738, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 312/516, D Loss: 0.07704406976699829, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 313/516, D Loss: 0.10308316349983215, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 314/516, D Loss: 0.07205644249916077, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 315/516, D Loss: 0.07496386766433716, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 316/516, D Loss: 0.09855818748474121, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 317/516, D Loss: 0.07693177461624146, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 318/516, D Loss: 0.13296294212341309, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 319/516, D Loss: 0.11603274941444397, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 320/516, D Loss: 0.07685720920562744, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 321/516, D Loss: 0.10928195714950562, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 322/516, D Loss: 0.11044463515281677, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 323/516, D Loss: 0.0621505081653595, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 324/516, D Loss: 0.12131333351135254, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 325/516, D Loss: 0.10379299521446228, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 326/516, D Loss: 0.10139590501785278, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 327/516, D Loss: 0.09338200092315674, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 328/516, D Loss: 0.09814178943634033, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 329/516, D Loss: 0.07970038056373596, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 330/516, D Loss: 0.062364429235458374, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 331/516, D Loss: 0.09370705485343933, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 332/516, D Loss: 0.06749218702316284, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 333/516, D Loss: 0.10850274562835693, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 334/516, D Loss: 0.08596101403236389, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 335/516, D Loss: 0.08488568663597107, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 336/516, D Loss: 0.09994691610336304, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 337/516, D Loss: 0.11932343244552612, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 338/516, D Loss: 0.07690665125846863, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 339/516, D Loss: 0.12327706813812256, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 340/516, D Loss: 0.09404131770133972, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 341/516, D Loss: 0.09373477101325989, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 342/516, D Loss: 0.10151863098144531, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 343/516, D Loss: 0.09045678377151489, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 344/516, D Loss: 0.08620741963386536, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 345/516, D Loss: 0.10958313941955566, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 346/516, D Loss: 0.059932053089141846, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 347/516, D Loss: 0.07390815019607544, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 348/516, D Loss: 0.05667462944984436, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 349/516, D Loss: 0.10114717483520508, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 350/516, D Loss: 0.08982571959495544, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 351/516, D Loss: 0.05600571632385254, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 352/516, D Loss: 0.09085202217102051, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 353/516, D Loss: 0.08867606520652771, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 354/516, D Loss: 0.08378654718399048, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 355/516, D Loss: 0.07039928436279297, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 356/516, D Loss: 0.08601534366607666, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 357/516, D Loss: 0.09463393688201904, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 358/516, D Loss: 0.08544591069221497, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 359/516, D Loss: 0.1195574700832367, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 360/516, D Loss: 0.09647464752197266, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 361/516, D Loss: 0.08906129002571106, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 362/516, D Loss: 0.04602694511413574, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 363/516, D Loss: 0.12135204672813416, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 364/516, D Loss: 0.10659992694854736, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 365/516, D Loss: 0.12492042779922485, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 366/516, D Loss: 0.06817784905433655, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 367/516, D Loss: 0.07308024168014526, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 368/516, D Loss: 0.0939667820930481, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 369/516, D Loss: 0.11412873864173889, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 370/516, D Loss: 0.10871779918670654, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 371/516, D Loss: 0.09232461452484131, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 372/516, D Loss: 0.11096218228340149, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 373/516, D Loss: 0.0800333023071289, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 374/516, D Loss: 0.10092133283615112, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 375/516, D Loss: 0.09629008173942566, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 376/516, D Loss: 0.13371506333351135, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 377/516, D Loss: 0.12985968589782715, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 378/516, D Loss: 0.09205770492553711, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 379/516, D Loss: 0.11181160807609558, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 380/516, D Loss: 0.07386904954910278, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 381/516, D Loss: 0.09994757175445557, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 382/516, D Loss: 0.10686784982681274, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 383/516, D Loss: 0.05970710515975952, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 384/516, D Loss: 0.07636702060699463, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 385/516, D Loss: 0.09482723474502563, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 386/516, D Loss: 0.09188568592071533, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 387/516, D Loss: 0.1265791356563568, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 388/516, D Loss: 0.05656510591506958, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 389/516, D Loss: 0.0837043821811676, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 390/516, D Loss: 0.1301027238368988, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 391/516, D Loss: 0.08874726295471191, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 392/516, D Loss: 0.11346635222434998, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 393/516, D Loss: 0.12945416569709778, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 394/516, D Loss: 0.09755086898803711, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 395/516, D Loss: 0.09160229563713074, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 396/516, D Loss: 0.0625830590724945, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 397/516, D Loss: 0.06956559419631958, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 398/516, D Loss: 0.09032303094863892, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 399/516, D Loss: 0.10477665066719055, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 400/516, D Loss: 0.07175850868225098, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 401/516, D Loss: 0.0669221580028534, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 402/516, D Loss: 0.10835987329483032, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 403/516, D Loss: 0.10952913761138916, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 404/516, D Loss: 0.106484055519104, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 405/516, D Loss: 0.18702587485313416, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 406/516, D Loss: 0.1416347622871399, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 407/516, D Loss: 0.08101546764373779, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 408/516, D Loss: 0.09544181823730469, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 409/516, D Loss: 0.08652949333190918, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 410/516, D Loss: 0.09233799576759338, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 411/516, D Loss: 0.13051727414131165, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 412/516, D Loss: 0.10400152206420898, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 413/516, D Loss: 0.07526129484176636, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 414/516, D Loss: 0.10521990060806274, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 415/516, D Loss: 0.049842625856399536, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 416/516, D Loss: 0.07081949710845947, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 417/516, D Loss: 0.09660196304321289, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 418/516, D Loss: 0.054056793451309204, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 419/516, D Loss: 0.07298699021339417, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 420/516, D Loss: 0.11538827419281006, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 421/516, D Loss: 0.11721277236938477, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 422/516, D Loss: 0.10956543684005737, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 423/516, D Loss: 0.09650558233261108, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 424/516, D Loss: 0.14258623123168945, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 425/516, D Loss: 0.0830308198928833, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 426/516, D Loss: 0.06705442070960999, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 427/516, D Loss: 0.12087264657020569, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 428/516, D Loss: 0.09630614519119263, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 429/516, D Loss: 0.12843653559684753, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 430/516, D Loss: 0.06922438740730286, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 431/516, D Loss: 0.089140385389328, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 432/516, D Loss: 0.0733557641506195, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 433/516, D Loss: 0.10358303785324097, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 434/516, D Loss: 0.07702383399009705, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 435/516, D Loss: 0.09959825873374939, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 436/516, D Loss: 0.080698162317276, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 437/516, D Loss: 0.07222700119018555, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 438/516, D Loss: 0.08711522817611694, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 439/516, D Loss: 0.08696717023849487, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 440/516, D Loss: 0.04988214373588562, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 441/516, D Loss: 0.08621710538864136, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 442/516, D Loss: 0.12468379735946655, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 443/516, D Loss: 0.07204294204711914, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 444/516, D Loss: 0.08857306838035583, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 445/516, D Loss: 0.09296473860740662, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 446/516, D Loss: 0.14440429210662842, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 447/516, D Loss: 0.07847446203231812, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 448/516, D Loss: 0.11253386735916138, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 449/516, D Loss: 0.0922752320766449, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 450/516, D Loss: 0.09884724020957947, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 451/516, D Loss: 0.09488379955291748, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 452/516, D Loss: 0.07803183794021606, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 453/516, D Loss: 0.08836188912391663, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 454/516, D Loss: 0.11650294065475464, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 455/516, D Loss: 0.09957119822502136, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 456/516, D Loss: 0.11352986097335815, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 457/516, D Loss: 0.129602313041687, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 458/516, D Loss: 0.14210984110832214, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 459/516, D Loss: 0.09912535548210144, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 460/516, D Loss: 0.09631198644638062, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 461/516, D Loss: 0.08106645941734314, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 462/516, D Loss: 0.10923206806182861, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 463/516, D Loss: 0.10350814461708069, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 464/516, D Loss: 0.1241619884967804, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 465/516, D Loss: 0.08080902695655823, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 466/516, D Loss: 0.08421438932418823, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 467/516, D Loss: 0.07652175426483154, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 468/516, D Loss: 0.1256512999534607, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 469/516, D Loss: 0.07977205514907837, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 470/516, D Loss: 0.09605306386947632, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 471/516, D Loss: 0.11343607306480408, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 472/516, D Loss: 0.1452251672744751, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 473/516, D Loss: 0.07741621136665344, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 474/516, D Loss: 0.10303550958633423, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 475/516, D Loss: 0.06468826532363892, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 476/516, D Loss: 0.06691887974739075, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 477/516, D Loss: 0.0967700183391571, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 478/516, D Loss: 0.10650995373725891, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 479/516, D Loss: 0.060783565044403076, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 480/516, D Loss: 0.056551575660705566, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 481/516, D Loss: 0.05785343050956726, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 482/516, D Loss: 0.122499018907547, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 483/516, D Loss: 0.06845927238464355, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 484/516, D Loss: 0.06788930296897888, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 485/516, D Loss: 0.09749332070350647, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 486/516, D Loss: 0.06160241365432739, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 487/516, D Loss: 0.0678299069404602, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 488/516, D Loss: 0.0924588143825531, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 489/516, D Loss: 0.10343089699745178, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 490/516, D Loss: 0.098856121301651, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 491/516, D Loss: 0.09019139409065247, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 492/516, D Loss: 0.08459416031837463, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 493/516, D Loss: 0.044464707374572754, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 494/516, D Loss: 0.11981779336929321, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 495/516, D Loss: 0.12168166041374207, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 496/516, D Loss: 0.05608165264129639, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 497/516, D Loss: 0.11004045605659485, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 498/516, D Loss: 0.08467033505439758, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 499/516, D Loss: 0.10333043336868286, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 500/516, D Loss: 0.0749911367893219, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 501/516, D Loss: 0.09478244185447693, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 502/516, D Loss: 0.0701073408126831, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 503/516, D Loss: 0.12305039167404175, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 504/516, D Loss: 0.09470641613006592, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 505/516, D Loss: 0.13176953792572021, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 506/516, D Loss: 0.1119765043258667, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 507/516, D Loss: 0.1031225323677063, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 508/516, D Loss: 0.10671883821487427, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 509/516, D Loss: 0.05546888709068298, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 510/516, D Loss: 0.10292312502861023, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 511/516, D Loss: 0.09530952572822571, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 512/516, D Loss: 0.09740757942199707, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 513/516, D Loss: 0.09314039349555969, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 514/516, D Loss: 0.08921581506729126, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 515/516, D Loss: 0.07505267858505249, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 516/516, D Loss: 0.10089918971061707, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 1/516, D Loss: 0.10117226839065552, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 2/516, D Loss: 0.1094886064529419, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 3/516, D Loss: 0.07904809713363647, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 4/516, D Loss: 0.08841073513031006, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 5/516, D Loss: 0.07680115103721619, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 6/516, D Loss: 0.0673089325428009, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 7/516, D Loss: 0.11669266223907471, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 8/516, D Loss: 0.06082111597061157, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 9/516, D Loss: 0.08751612901687622, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 10/516, D Loss: 0.06595835089683533, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 11/516, D Loss: 0.10410237312316895, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 12/516, D Loss: 0.10620355606079102, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 13/516, D Loss: 0.12171125411987305, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 14/516, D Loss: 0.07056719064712524, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 4, Batch 15/516, D Loss: 0.11269667744636536, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 16/516, D Loss: 0.08840197324752808, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 17/516, D Loss: 0.09349295496940613, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 18/516, D Loss: 0.1094963550567627, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 19/516, D Loss: 0.0751168429851532, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 20/516, D Loss: 0.11376944184303284, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 21/516, D Loss: 0.05140221118927002, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 22/516, D Loss: 0.05619385838508606, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 23/516, D Loss: 0.06980538368225098, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 24/516, D Loss: 0.10202282667160034, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 25/516, D Loss: 0.11682024598121643, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 26/516, D Loss: 0.10742038488388062, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 27/516, D Loss: 0.10685506463050842, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 28/516, D Loss: 0.09618854522705078, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 29/516, D Loss: 0.10245248675346375, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 30/516, D Loss: 0.054360806941986084, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 31/516, D Loss: 0.1423969268798828, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 32/516, D Loss: 0.10552328824996948, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 33/516, D Loss: 0.11542052030563354, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 34/516, D Loss: 0.064980149269104, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 35/516, D Loss: 0.09001076221466064, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 36/516, D Loss: 0.08050274848937988, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 37/516, D Loss: 0.08160430192947388, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 38/516, D Loss: 0.08720168471336365, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 39/516, D Loss: 0.07819396257400513, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 40/516, D Loss: 0.10308730602264404, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 41/516, D Loss: 0.08833971619606018, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 42/516, D Loss: 0.09558290243148804, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 43/516, D Loss: 0.10597461462020874, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 44/516, D Loss: 0.1200876533985138, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 45/516, D Loss: 0.14018011093139648, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 46/516, D Loss: 0.10278284549713135, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 47/516, D Loss: 0.0774850845336914, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 48/516, D Loss: 0.09050339460372925, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 49/516, D Loss: 0.04943031072616577, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 50/516, D Loss: 0.08444106578826904, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 51/516, D Loss: 0.07396009564399719, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 52/516, D Loss: 0.07955217361450195, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 53/516, D Loss: 0.09663456678390503, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 54/516, D Loss: 0.06229144334793091, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 55/516, D Loss: 0.14097338914871216, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 56/516, D Loss: 0.08633264899253845, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 57/516, D Loss: 0.0753684937953949, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 58/516, D Loss: 0.07743364572525024, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 59/516, D Loss: 0.05591532588005066, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 60/516, D Loss: 0.10686755180358887, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 61/516, D Loss: 0.0942133367061615, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 62/516, D Loss: 0.05377134680747986, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 63/516, D Loss: 0.08592468500137329, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 64/516, D Loss: 0.1279294788837433, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 65/516, D Loss: 0.11250659823417664, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 66/516, D Loss: 0.14276906847953796, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 67/516, D Loss: 0.10783499479293823, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 68/516, D Loss: 0.1275033950805664, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 69/516, D Loss: 0.14482533931732178, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 70/516, D Loss: 0.09590443968772888, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 71/516, D Loss: 0.10171961784362793, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 72/516, D Loss: 0.11150813102722168, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 73/516, D Loss: 0.11695373058319092, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 74/516, D Loss: 0.06016254425048828, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 75/516, D Loss: 0.10035017132759094, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 76/516, D Loss: 0.11165037751197815, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 77/516, D Loss: 0.10079044103622437, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 78/516, D Loss: 0.10071983933448792, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 79/516, D Loss: 0.06717279553413391, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 80/516, D Loss: 0.07610639929771423, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 81/516, D Loss: 0.09953543543815613, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 82/516, D Loss: 0.06242629885673523, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 83/516, D Loss: 0.08978819847106934, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 84/516, D Loss: 0.12182110548019409, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 85/516, D Loss: 0.07768982648849487, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 86/516, D Loss: 0.1463320553302765, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 87/516, D Loss: 0.08911716938018799, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 88/516, D Loss: 0.09457504749298096, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 89/516, D Loss: 0.1061626672744751, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 90/516, D Loss: 0.0949062705039978, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 91/516, D Loss: 0.10106554627418518, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 92/516, D Loss: 0.12543874979019165, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 93/516, D Loss: 0.13505399227142334, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 94/516, D Loss: 0.08501419425010681, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 95/516, D Loss: 0.0908801257610321, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 96/516, D Loss: 0.07912111282348633, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 97/516, D Loss: 0.11034810543060303, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 98/516, D Loss: 0.1590156853199005, G Loss: -0.9999997615814209\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the cswgan model\n",
    "generator = build_generator(latent_dim, real_data.shape[1])\n",
    "discriminator = build_discriminator(real_data.shape[1])\n",
    "cswgan = build_cswgan(generator, discriminator)\n",
    "discriminator.compile(optimizer='adam', loss=wasserstein_loss)\n",
    "cswgan.compile(optimizer='adam', loss=wasserstein_loss)\n",
    "\n",
    "# Train the Conditional Sig-Wasserstein GAN\n",
    "train_cswgan(generator, discriminator, cswgan, real_data, latent_dim, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b5c0f-b23d-48e2-96dc-e8bc176f00da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
