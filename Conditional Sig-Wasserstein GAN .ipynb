{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881edca4-197a-45bf-b1bd-af504f90fd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f64a4a-7ab4-4a3e-be96-b22c1a6d4e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 33067 entries, 2019-02-18 00:00:00 to 2022-12-31 00:00:00\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   SL NO        33067 non-null  int64  \n",
      " 1   PM2.5        31251 non-null  float64\n",
      " 2   PM10         31790 non-null  float64\n",
      " 3   NO           31396 non-null  float64\n",
      " 4   NO2          32393 non-null  float64\n",
      " 5   Nox          32616 non-null  float64\n",
      " 6   NH3          31705 non-null  float64\n",
      " 7   SO2          32592 non-null  float64\n",
      " 8   CO           32616 non-null  float64\n",
      " 9   Ozone        32473 non-null  float64\n",
      " 10  Benzene      30961 non-null  float64\n",
      " 11  Eth-Benzene  16450 non-null  float64\n",
      " 12  MP-Xylene    22659 non-null  float64\n",
      " 13  WS           31483 non-null  float64\n",
      " 14  WD           31528 non-null  float64\n",
      " 15  SR           20184 non-null  float64\n",
      " 16  BP           21876 non-null  float64\n",
      " 17  AT           31462 non-null  float64\n",
      " 18  RF           32623 non-null  float64\n",
      "dtypes: float64(18), int64(1)\n",
      "memory usage: 5.0 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_14072\\4218735721.py:1: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.read_csv(r\"C:\\Users\\himan\\Desktop\\7th GAN\\updataDataSets.csv\",\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\himan\\Desktop\\7th GAN\\updataDataSets.csv\", \n",
    "                 index_col='From Date', \n",
    "                 parse_dates=['From Date'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "effc6358-a964-4260-a172-8d3c2d1b8ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "                     SL NO  PM2.5   PM10    NO   NO2    Nox    NH3    SO2  \\\n",
      "From Date                                                                   \n",
      "2019-02-18 00:00:00      1  58.04  81.06  1.81  6.40   9.66  27.18  13.77   \n",
      "2019-02-18 01:00:00      2  60.00  82.39  1.91  6.08   9.70  26.46  13.64   \n",
      "2019-02-18 02:00:00      3  55.50  73.42  2.22  6.71  10.86  27.06  13.64   \n",
      "2019-02-18 03:00:00      4  66.56  96.04  2.79  7.00  12.10  26.34  13.72   \n",
      "2019-02-18 04:00:00      5  67.08  97.21  3.29  9.45  15.53  25.16  13.80   \n",
      "\n",
      "                       CO  Ozone  Benzene  Eth-Benzene  MP-Xylene    WS  \\\n",
      "From Date                                                                 \n",
      "2019-02-18 00:00:00  0.49  15.73     0.92          NaN       0.33  1.86   \n",
      "2019-02-18 01:00:00  0.44  18.36     0.84          NaN       0.29  1.54   \n",
      "2019-02-18 02:00:00  0.44  34.54     0.77          NaN       0.23  0.66   \n",
      "2019-02-18 03:00:00  0.45  31.85     0.82          NaN       0.21  0.61   \n",
      "2019-02-18 04:00:00  0.47  24.07     0.74          NaN       0.22  0.54   \n",
      "\n",
      "                         WD    SR       BP     AT    RF  \n",
      "From Date                                                \n",
      "2019-02-18 00:00:00  229.44  0.22  1002.19  20.87  0.00  \n",
      "2019-02-18 01:00:00  217.20  1.46   956.00  18.92  0.24  \n",
      "2019-02-18 02:00:00  197.21   NaN  1001.60  18.33  0.00  \n",
      "2019-02-18 03:00:00  206.44   NaN  1001.41  17.63  0.00  \n",
      "2019-02-18 04:00:00  230.53   NaN   980.67  16.61  0.00  \n"
     ]
    }
   ],
   "source": [
    "print(\"Original Dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85f0412-9a0d-4fba-8b12-afe2009cf18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f314980-0f9c-45ec-b2cf-e85c19b84774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "def build_generator(latent_dim, n_features):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, input_dim=latent_dim, activation='relu'))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(n_features, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef78fb3-f624-46fc-b076-6bb109258810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model\n",
    "def build_discriminator(n_features):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(512, input_dim=n_features, activation='relu'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed4cb6d-37d4-4c10-a900-5881f690073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Conditional Sig-Wasserstein GAN\n",
    "def build_cswgan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3e59b2-07b6-46d0-adaa-33110d17fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4bdd9f3-e29d-4017-b2a3-0719d1a469b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the models\n",
    "latent_dim = 100  # Adjust as needed\n",
    "generator = build_generator(latent_dim, df.shape[1])\n",
    "discriminator = build_discriminator(df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dfd75b5-ed5d-4102-9b55-ae12a1d88592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "latent_dim = 150\n",
    "generator = build_generator(latent_dim, df.shape[1])\n",
    "discriminator = build_discriminator(df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a1c1e1b-c88a-4c8a-b533-36768ccadce7",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Function to generate random noise for the generator\n",
    "def generate_latent_samples(batch_size, latent_dim):\n",
    "    return np.random.randn(batch_size, latent_dim)\n",
    "\n",
    "# Function to get a random batch from the real data\n",
    "def get_real_samples(data, batch_size):\n",
    "    idx = np.random.randint(0, data.shape[0], batch_size)\n",
    "    return data[idx]\n",
    "\n",
    "# Function to train the Conditional Sig-Wasserstein GAN\n",
    "def train_cswgan(generator, discriminator, cswgan, real_data, latent_dim, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(len(real_data) // batch_size):\n",
    "            # Train discriminator\n",
    "            real_samples = get_real_samples(real_data, batch_size)\n",
    "            fake_samples = generator.predict(generate_latent_samples(batch_size, latent_dim))\n",
    "            \n",
    "            discriminator_loss_real = discriminator.train_on_batch(real_samples, -np.ones((batch_size, 1)))\n",
    "            discriminator_loss_fake = discriminator.train_on_batch(fake_samples, np.ones((batch_size, 1)))\n",
    "            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "            # Train generator (via the combined model)\n",
    "            latent_samples = generate_latent_samples(batch_size, latent_dim)\n",
    "            valid_labels = -np.ones((batch_size, 1))\n",
    "            generator_loss = cswgan.train_on_batch(latent_samples, valid_labels)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch + 1}, Batch {_ + 1}/{len(real_data) // batch_size}, D Loss: {discriminator_loss}, G Loss: {generator_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02fc1bf1-fa61-4e96-aceb-d546c162397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 150\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b29cfa9-84b5-47c6-b8ff-8bf44e802b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = df.drop(['SL NO'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2b35c82-2ad4-4278-9ae7-5c0d829d4b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "WARNING:tensorflow:From C:\\Users\\himan\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Epoch 1, Batch 1/516, D Loss: 0.23854487016797066, G Loss: -0.47842010855674744\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 2/516, D Loss: 0.2573514925315976, G Loss: -0.5314461588859558\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 3/516, D Loss: 0.28323200554586947, G Loss: -0.5719108581542969\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 4/516, D Loss: 0.30526203545741737, G Loss: -0.6119959354400635\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 5/516, D Loss: 0.32440510019659996, G Loss: -0.6606863737106323\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 6/516, D Loss: 0.3454045783728361, G Loss: -0.7059348821640015\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 7/516, D Loss: 0.37033022404648364, G Loss: -0.7528724074363708\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 8/516, D Loss: 0.3901883726939559, G Loss: -0.7940185070037842\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 9/516, D Loss: 0.40506018325686455, G Loss: -0.8422592878341675\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 10/516, D Loss: 0.41204486042261124, G Loss: -0.8687787055969238\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 11/516, D Loss: 0.44147349055856466, G Loss: -0.9000293016433716\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 12/516, D Loss: 0.4611175034660846, G Loss: -0.9235447645187378\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 13/516, D Loss: 0.4683462316170335, G Loss: -0.9463803768157959\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 14/516, D Loss: 0.4752883103210479, G Loss: -0.963348925113678\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 15/516, D Loss: 0.4854197850218043, G Loss: -0.9694392681121826\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 16/516, D Loss: 0.48642070475034416, G Loss: -0.9804357290267944\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 17/516, D Loss: 0.491488681640476, G Loss: -0.9860867857933044\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 18/516, D Loss: 0.4799062591046095, G Loss: -0.9897022843360901\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 19/516, D Loss: 0.49392955855000764, G Loss: -0.9927849769592285\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 20/516, D Loss: 0.4956185717601329, G Loss: -0.9939380288124084\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 21/516, D Loss: 0.4961720078717917, G Loss: -0.9958120584487915\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 22/516, D Loss: 0.4953235578723252, G Loss: -0.9968841671943665\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 23/516, D Loss: 0.49009674321860075, G Loss: -0.9970376491546631\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 24/516, D Loss: 0.4962988933548331, G Loss: -0.998005747795105\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 25/516, D Loss: 0.4951585475355387, G Loss: -0.9982376098632812\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 26/516, D Loss: 0.4828417915850878, G Loss: -0.9987168312072754\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 27/516, D Loss: 0.49243314750492573, G Loss: -0.9990139007568359\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 28/516, D Loss: 0.47908082604408264, G Loss: -0.999212384223938\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 29/516, D Loss: 0.4863625206053257, G Loss: -0.9993406534194946\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 30/516, D Loss: 0.49162697698920965, G Loss: -0.9993117451667786\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 31/516, D Loss: 0.49689522269181907, G Loss: -0.9995211958885193\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 32/516, D Loss: 0.4950000233948231, G Loss: -0.9995311498641968\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 33/516, D Loss: 0.49431079532951117, G Loss: -0.9995440244674683\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 34/516, D Loss: 0.4784764349460602, G Loss: -0.9996265172958374\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 35/516, D Loss: 0.49573056818917394, G Loss: -0.9997168183326721\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 36/516, D Loss: 0.4925997368991375, G Loss: -0.9996907711029053\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 37/516, D Loss: 0.4957098327577114, G Loss: -0.9997752904891968\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 38/516, D Loss: 0.49245998496189713, G Loss: -0.9997211694717407\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 39/516, D Loss: 0.4923307290300727, G Loss: -0.9997801184654236\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 40/516, D Loss: 0.49112997483462095, G Loss: -0.9998022317886353\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 41/516, D Loss: 0.4982824204489589, G Loss: -0.9997901916503906\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 42/516, D Loss: 0.487026983872056, G Loss: -0.9997904300689697\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 43/516, D Loss: 0.4969168077223003, G Loss: -0.9998393058776855\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 44/516, D Loss: 0.49176805187016726, G Loss: -0.9998480081558228\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 45/516, D Loss: 0.48148078843951225, G Loss: -0.9998348951339722\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 46/516, D Loss: 0.497608961770311, G Loss: -0.9998560547828674\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 47/516, D Loss: 0.49533064709976315, G Loss: -0.9998819231987\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 48/516, D Loss: 0.4987619249150157, G Loss: -0.9998553991317749\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 49/516, D Loss: 0.4947085538879037, G Loss: -0.9998657703399658\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 50/516, D Loss: 0.4912028843536973, G Loss: -0.9998779296875\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 51/516, D Loss: 0.4959543915465474, G Loss: -0.999865710735321\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 52/516, D Loss: 0.493834282271564, G Loss: -0.999900758266449\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 53/516, D Loss: 0.4933326933532953, G Loss: -0.9998961687088013\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 54/516, D Loss: 0.48966551199555397, G Loss: -0.9999113082885742\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 55/516, D Loss: 0.494158152025193, G Loss: -0.9999172687530518\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 56/516, D Loss: 0.4856232926249504, G Loss: -0.9998932480812073\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 57/516, D Loss: 0.49348671548068523, G Loss: -0.9998983144760132\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 58/516, D Loss: 0.49906880769412965, G Loss: -0.9999085664749146\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 59/516, D Loss: 0.4877620106562972, G Loss: -0.9999130964279175\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 60/516, D Loss: 0.49896425707265735, G Loss: -0.9999062418937683\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 61/516, D Loss: 0.49773491406813264, G Loss: -0.9998924732208252\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 62/516, D Loss: 0.498539618216455, G Loss: -0.9998892545700073\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 63/516, D Loss: 0.49767077388241887, G Loss: -0.9999115467071533\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 64/516, D Loss: 0.4940994679927826, G Loss: -0.9999157786369324\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 65/516, D Loss: 0.49295522924512625, G Loss: -0.9999198913574219\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 66/516, D Loss: 0.4844721546396613, G Loss: -0.9999063014984131\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 67/516, D Loss: 0.49368667928501964, G Loss: -0.999909520149231\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 68/516, D Loss: 0.48779255524277687, G Loss: -0.9999052286148071\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 69/516, D Loss: 0.48310037329792976, G Loss: -0.9999032616615295\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 70/516, D Loss: 0.49470971897244453, G Loss: -0.99989914894104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 71/516, D Loss: 0.49426904739812016, G Loss: -0.9998959302902222\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 72/516, D Loss: 0.49736282089725137, G Loss: -0.9999032616615295\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 73/516, D Loss: 0.4949994063936174, G Loss: -0.9999051094055176\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 74/516, D Loss: 0.49582056049257517, G Loss: -0.9999001026153564\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 75/516, D Loss: 0.499578463670332, G Loss: -0.9999299645423889\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 76/516, D Loss: 0.49434187170118093, G Loss: -0.999921441078186\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 77/516, D Loss: 0.49832125613465905, G Loss: -0.9999202489852905\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 78/516, D Loss: 0.4934213045053184, G Loss: -0.9999042749404907\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 79/516, D Loss: 0.49447256699204445, G Loss: -0.9999291300773621\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 80/516, D Loss: 0.4924114430323243, G Loss: -0.9999054074287415\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 81/516, D Loss: 0.4913885137066245, G Loss: -0.9999096393585205\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 82/516, D Loss: 0.4965398390777409, G Loss: -0.9999045729637146\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 83/516, D Loss: 0.49540121387690306, G Loss: -0.999910295009613\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 84/516, D Loss: 0.48616418801248074, G Loss: -0.9998728632926941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 85/516, D Loss: 0.49545975401997566, G Loss: -0.9999116659164429\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 86/516, D Loss: 0.4958301172591746, G Loss: -0.9999215602874756\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 87/516, D Loss: 0.48828418273478746, G Loss: -0.9999040365219116\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 88/516, D Loss: 0.49944655201397836, G Loss: -0.9999229907989502\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 89/516, D Loss: 0.4998316886340035, G Loss: -0.9999297857284546\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 90/516, D Loss: 0.49515517707914114, G Loss: -0.9999251365661621\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 91/516, D Loss: 0.49383563827723265, G Loss: -0.9999211430549622\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 92/516, D Loss: 0.4954002657905221, G Loss: -0.999922513961792\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 93/516, D Loss: 0.4948639376088977, G Loss: -0.9999189972877502\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 94/516, D Loss: 0.4982762672007084, G Loss: -0.9999212622642517\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 95/516, D Loss: 0.4875823687762022, G Loss: -0.9999099969863892\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 96/516, D Loss: 0.4905467201024294, G Loss: -0.9999390840530396\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 97/516, D Loss: 0.4863935215398669, G Loss: -0.9999143481254578\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 98/516, D Loss: 0.4987103152088821, G Loss: -0.9999103546142578\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 99/516, D Loss: 0.49513442954048514, G Loss: -0.9999276399612427\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 100/516, D Loss: 0.4871239513158798, G Loss: -0.9999361038208008\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 101/516, D Loss: 0.487388102337718, G Loss: -0.999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 102/516, D Loss: 0.49459527200087905, G Loss: -0.9999204874038696\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 103/516, D Loss: 0.49920061527518556, G Loss: -0.9999134540557861\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 104/516, D Loss: 0.49264756171032786, G Loss: -0.9999094009399414\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 105/516, D Loss: 0.4952080622315407, G Loss: -0.9999252557754517\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 106/516, D Loss: 0.49556713132187724, G Loss: -0.9999193549156189\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 107/516, D Loss: 0.4859113935381174, G Loss: -0.9999073147773743\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 108/516, D Loss: 0.49922499986132607, G Loss: -0.9998999834060669\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 109/516, D Loss: 0.4936602911911905, G Loss: -0.9998949766159058\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 110/516, D Loss: 0.499430738913361, G Loss: -0.9999269247055054\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 111/516, D Loss: 0.4958386323414743, G Loss: -0.9999351501464844\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 112/516, D Loss: 0.49988215588382445, G Loss: -0.9999343156814575\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 113/516, D Loss: 0.490441782400012, G Loss: -0.9999105334281921\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 114/516, D Loss: 0.4982135317986831, G Loss: -0.9999385476112366\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 115/516, D Loss: 0.4984029137995094, G Loss: -0.999936580657959\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 116/516, D Loss: 0.4967382657341659, G Loss: -0.9999352693557739\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 117/516, D Loss: 0.4991672105388716, G Loss: -0.9999265074729919\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 118/516, D Loss: 0.488774411380291, G Loss: -0.9999321699142456\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 119/516, D Loss: 0.48930436465889215, G Loss: -0.9999335408210754\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 120/516, D Loss: 0.49752146983519197, G Loss: -0.9999281764030457\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 121/516, D Loss: 0.4988475951831788, G Loss: -0.9999180436134338\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 122/516, D Loss: 0.49358478654175997, G Loss: -0.9999397397041321\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 123/516, D Loss: 0.48910622484982014, G Loss: -0.999932050704956\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 124/516, D Loss: 0.4998646615422331, G Loss: -0.9999426007270813\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 125/516, D Loss: 0.4950039526447654, G Loss: -0.9999120831489563\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 126/516, D Loss: 0.4926519375294447, G Loss: -0.9999328851699829\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 127/516, D Loss: 0.49208209477365017, G Loss: -0.9999164342880249\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 128/516, D Loss: 0.492192467674613, G Loss: -0.9999352693557739\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 129/516, D Loss: 0.49436299968510866, G Loss: -0.9999333620071411\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 130/516, D Loss: 0.48531216010451317, G Loss: -0.9999451041221619\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 131/516, D Loss: 0.49599534599110484, G Loss: -0.9999270439147949\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 132/516, D Loss: 0.4863333534449339, G Loss: -0.9999263286590576\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 133/516, D Loss: 0.4922892665490508, G Loss: -0.9999343752861023\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 134/516, D Loss: 0.498135480331257, G Loss: -0.9999330043792725\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 135/516, D Loss: 0.49292868515476584, G Loss: -0.9999332427978516\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 136/516, D Loss: 0.49175442568957806, G Loss: -0.999931275844574\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 137/516, D Loss: 0.49153292644768953, G Loss: -0.9999446272850037\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 138/516, D Loss: 0.4937464399263263, G Loss: -0.9999421834945679\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 139/516, D Loss: 0.4941799771040678, G Loss: -0.9999402761459351\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 140/516, D Loss: 0.49069683346897364, G Loss: -0.9999473094940186\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 141/516, D Loss: 0.4880100227892399, G Loss: -0.9999228715896606\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 142/516, D Loss: 0.4927487215027213, G Loss: -0.999951958656311\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 143/516, D Loss: 0.49905989394756034, G Loss: -0.9999525547027588\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 144/516, D Loss: 0.4951169895939529, G Loss: -0.9999327063560486\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 145/516, D Loss: 0.4914056295529008, G Loss: -0.9999328851699829\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 146/516, D Loss: 0.49523708363994956, G Loss: -0.9999316334724426\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 147/516, D Loss: 0.4997018770663999, G Loss: -0.9999507665634155\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 148/516, D Loss: 0.491055840626359, G Loss: -0.9999401569366455\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 149/516, D Loss: 0.4914997033774853, G Loss: -0.9999294281005859\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 150/516, D Loss: 0.49782496970146894, G Loss: -0.9999232292175293\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 151/516, D Loss: 0.49906136479694396, G Loss: -0.999942421913147\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 152/516, D Loss: 0.4880609745159745, G Loss: -0.9999553561210632\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 153/516, D Loss: 0.49859554157592356, G Loss: -0.9999407529830933\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 154/516, D Loss: 0.4895333917811513, G Loss: -0.999932587146759\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 155/516, D Loss: 0.49953970083151944, G Loss: -0.9999296069145203\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 156/516, D Loss: 0.49708320293575525, G Loss: -0.9999459981918335\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 157/516, D Loss: 0.49343927996233106, G Loss: -0.9999415278434753\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 158/516, D Loss: 0.4949140762910247, G Loss: -0.9999381303787231\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 159/516, D Loss: 0.4942314140498638, G Loss: -0.9999473094940186\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 160/516, D Loss: 0.497339935740456, G Loss: -0.9999370574951172\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 161/516, D Loss: 0.4934842302463949, G Loss: -0.9999443888664246\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 162/516, D Loss: 0.4945170874707401, G Loss: -0.9999516010284424\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 163/516, D Loss: 0.4938192553818226, G Loss: -0.9999375939369202\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 164/516, D Loss: 0.4949953993782401, G Loss: -0.9999408721923828\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 165/516, D Loss: 0.48964677937328815, G Loss: -0.9999279975891113\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 166/516, D Loss: 0.49540769634768367, G Loss: -0.9999524354934692\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 167/516, D Loss: 0.49863839498721063, G Loss: -0.9999493956565857\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 168/516, D Loss: 0.4985596155747771, G Loss: -0.9999393224716187\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 169/516, D Loss: 0.4967478225007653, G Loss: -0.9999401569366455\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 170/516, D Loss: 0.4866819381713867, G Loss: -0.9999494552612305\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 171/516, D Loss: 0.498338247067295, G Loss: -0.9999411106109619\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 172/516, D Loss: 0.4921598141081631, G Loss: -0.9999434947967529\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 173/516, D Loss: 0.4931363482028246, G Loss: -0.9999256134033203\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 174/516, D Loss: 0.4959074594080448, G Loss: -0.9999409914016724\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 175/516, D Loss: 0.4945250330492854, G Loss: -0.9999423027038574\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 176/516, D Loss: 0.4976924939546734, G Loss: -0.9999462366104126\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 177/516, D Loss: 0.49400911666452885, G Loss: -0.9999447464942932\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 178/516, D Loss: 0.4958479809574783, G Loss: -0.9999479651451111\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 179/516, D Loss: 0.49913981137797236, G Loss: -0.9999337196350098\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 180/516, D Loss: 0.4943778645247221, G Loss: -0.9999608993530273\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 181/516, D Loss: 0.49798090709373355, G Loss: -0.9999440908432007\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 182/516, D Loss: 0.4898733962327242, G Loss: -0.9999504089355469\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 183/516, D Loss: 0.48910728469491005, G Loss: -0.9999461770057678\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 184/516, D Loss: 0.49365793727338314, G Loss: -0.9999262094497681\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 185/516, D Loss: 0.49052052572369576, G Loss: -0.9999421834945679\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 186/516, D Loss: 0.4989424445666373, G Loss: -0.9999523162841797\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 187/516, D Loss: 0.4972085626795888, G Loss: -0.9999464750289917\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 188/516, D Loss: 0.4864000901579857, G Loss: -0.9999450445175171\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 189/516, D Loss: 0.4980534497881308, G Loss: -0.9999468922615051\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 190/516, D Loss: 0.4987553944811225, G Loss: -0.9999523758888245\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 191/516, D Loss: 0.48939610458910465, G Loss: -0.9999474287033081\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 192/516, D Loss: 0.4967589098960161, G Loss: -0.9999587535858154\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 193/516, D Loss: 0.4907678011804819, G Loss: -0.9999467134475708\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 194/516, D Loss: 0.4915925292298198, G Loss: -0.9999527335166931\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 195/516, D Loss: 0.4993913883809, G Loss: -0.9999502897262573\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 196/516, D Loss: 0.4949026727117598, G Loss: -0.999954104423523\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 197/516, D Loss: 0.4956806479021907, G Loss: -0.9999496936798096\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 198/516, D Loss: 0.49015327729284763, G Loss: -0.9999440312385559\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 199/516, D Loss: 0.4977570604532957, G Loss: -0.9999428987503052\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 200/516, D Loss: 0.4951998647302389, G Loss: -0.9999529123306274\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 201/516, D Loss: 0.4921106519177556, G Loss: -0.999954879283905\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 202/516, D Loss: 0.4975344103295356, G Loss: -0.9999627470970154\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 203/516, D Loss: 0.4991455964045599, G Loss: -0.9999493360519409\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 204/516, D Loss: 0.4985523687209934, G Loss: -0.9999483823776245\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 205/516, D Loss: 0.4810060001909733, G Loss: -0.9999423027038574\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 206/516, D Loss: 0.4900714196264744, G Loss: -0.9999547004699707\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 207/516, D Loss: 0.4936852087266743, G Loss: -0.9999574422836304\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 208/516, D Loss: 0.49664957472123206, G Loss: -0.9999489784240723\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 209/516, D Loss: 0.4956384780816734, G Loss: -0.999945878982544\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 210/516, D Loss: 0.49947452323976904, G Loss: -0.9999527335166931\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 211/516, D Loss: 0.49445635825395584, G Loss: -0.9999493360519409\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 212/516, D Loss: 0.48521085176616907, G Loss: -0.9999437928199768\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 213/516, D Loss: 0.49137514643371105, G Loss: -0.9999463558197021\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 214/516, D Loss: 0.4968633889220655, G Loss: -0.9999395608901978\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 215/516, D Loss: 0.48945001885294914, G Loss: -0.9999616146087646\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 216/516, D Loss: 0.4915456483140588, G Loss: -0.9999585151672363\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 217/516, D Loss: 0.49188060592859983, G Loss: -0.9999440908432007\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 218/516, D Loss: 0.4939114232547581, G Loss: -0.9999528527259827\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 219/516, D Loss: 0.4943245416507125, G Loss: -0.9999364018440247\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 220/516, D Loss: 0.495156510733068, G Loss: -0.9999593496322632\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 221/516, D Loss: 0.49872586701530963, G Loss: -0.9999464750289917\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 222/516, D Loss: 0.49682989437133074, G Loss: -0.9999551773071289\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 223/516, D Loss: 0.49532773764804006, G Loss: -0.999948263168335\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 224/516, D Loss: 0.48030027747154236, G Loss: -0.9999641180038452\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 225/516, D Loss: 0.4841327928006649, G Loss: -0.999962329864502\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 226/516, D Loss: 0.49839844240341336, G Loss: -0.9999655485153198\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 227/516, D Loss: 0.49681108607910573, G Loss: -0.9999485015869141\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 228/516, D Loss: 0.49392241751775146, G Loss: -0.9999573826789856\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 229/516, D Loss: 0.4967312242370099, G Loss: -0.9999581575393677\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 230/516, D Loss: 0.49885184806771576, G Loss: -0.9999376535415649\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 231/516, D Loss: 0.49856150313280523, G Loss: -0.9999582171440125\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 232/516, D Loss: 0.49531907122582197, G Loss: -0.9999659061431885\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 233/516, D Loss: 0.49163302034139633, G Loss: -0.9999526143074036\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 234/516, D Loss: 0.49305097525939345, G Loss: -0.9999597072601318\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 235/516, D Loss: 0.4909110413864255, G Loss: -0.9999492168426514\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 236/516, D Loss: 0.4994342540157959, G Loss: -0.9999545812606812\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 237/516, D Loss: 0.4988284243736416, G Loss: -0.9999569058418274\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 238/516, D Loss: 0.48977029230445623, G Loss: -0.9999605417251587\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 239/516, D Loss: 0.4944688752293587, G Loss: -0.9999681711196899\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 240/516, D Loss: 0.4937482983805239, G Loss: -0.9999477863311768\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 241/516, D Loss: 0.4881847593933344, G Loss: -0.9999500513076782\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 242/516, D Loss: 0.49839364213403314, G Loss: -0.9999473094940186\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 243/516, D Loss: 0.49538219440728426, G Loss: -0.9999603629112244\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 244/516, D Loss: 0.49931541143450886, G Loss: -0.9999637007713318\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 245/516, D Loss: 0.4921136610209942, G Loss: -0.9999597072601318\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 246/516, D Loss: 0.4928973070345819, G Loss: -0.999955415725708\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 247/516, D Loss: 0.4900505356490612, G Loss: -0.9999607801437378\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 248/516, D Loss: 0.49063527025282383, G Loss: -0.9999645352363586\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 249/516, D Loss: 0.4901872491464019, G Loss: -0.9999603033065796\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 250/516, D Loss: 0.48705514147877693, G Loss: -0.9999642372131348\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 251/516, D Loss: 0.4956702901981771, G Loss: -0.9999660849571228\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 252/516, D Loss: 0.4910701373592019, G Loss: -0.9999632239341736\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 253/516, D Loss: 0.4866667538881302, G Loss: -0.9999593496322632\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 254/516, D Loss: 0.4979517967440188, G Loss: -0.9999637603759766\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 255/516, D Loss: 0.4989082438405603, G Loss: -0.9999613165855408\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 256/516, D Loss: 0.4951062249019742, G Loss: -0.9999614357948303\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 257/516, D Loss: 0.4836390856653452, G Loss: -0.9999591708183289\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 258/516, D Loss: 0.4919396694749594, G Loss: -0.9999651312828064\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 259/516, D Loss: 0.4986033132299781, G Loss: -0.9999685287475586\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 260/516, D Loss: 0.49743518489412963, G Loss: -0.9999648332595825\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 261/516, D Loss: 0.49481715075671673, G Loss: -0.9999523162841797\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 262/516, D Loss: 0.4916440639644861, G Loss: -0.9999622702598572\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 263/516, D Loss: 0.4900649804621935, G Loss: -0.9999560117721558\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 264/516, D Loss: 0.4987038825638592, G Loss: -0.9999457001686096\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 265/516, D Loss: 0.4972832575440407, G Loss: -0.9999630451202393\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 266/516, D Loss: 0.49652450485154986, G Loss: -0.9999589323997498\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 267/516, D Loss: 0.49582789093255997, G Loss: -0.9999473094940186\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 268/516, D Loss: 0.4985605562105775, G Loss: -0.9999626874923706\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 269/516, D Loss: 0.4988897053990513, G Loss: -0.9999666213989258\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 1, Batch 270/516, D Loss: 0.4950158856809139, G Loss: -0.9999602437019348\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 271/516, D Loss: 0.4947293191216886, G Loss: -0.9999696612358093\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 272/516, D Loss: 0.4974451719317585, G Loss: -0.9999679923057556\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 273/516, D Loss: 0.4990930269123055, G Loss: -0.9999634027481079\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 274/516, D Loss: 0.49917782528791577, G Loss: -0.9999474287033081\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 275/516, D Loss: 0.4856426753103733, G Loss: -0.9999646544456482\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 276/516, D Loss: 0.4905738281086087, G Loss: -0.99997478723526\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 277/516, D Loss: 0.48948016203939915, G Loss: -0.9999616146087646\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 278/516, D Loss: 0.4985479674069211, G Loss: -0.9999572038650513\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 279/516, D Loss: 0.4984315460314974, G Loss: -0.99996018409729\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 280/516, D Loss: 0.4945753738284111, G Loss: -0.999963104724884\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 281/516, D Loss: 0.48998462222516537, G Loss: -0.9999619126319885\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 282/516, D Loss: 0.49595732567831874, G Loss: -0.9999607801437378\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 283/516, D Loss: 0.4928190317004919, G Loss: -0.9999653697013855\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 284/516, D Loss: 0.4905028361827135, G Loss: -0.9999639987945557\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 285/516, D Loss: 0.49252326879650354, G Loss: -0.9999626874923706\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 286/516, D Loss: 0.4881648775190115, G Loss: -0.9999626874923706\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 287/516, D Loss: 0.4892793409526348, G Loss: -0.999964714050293\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 288/516, D Loss: 0.48852972500026226, G Loss: -0.9999645352363586\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 289/516, D Loss: 0.49491497222334146, G Loss: -0.999962568283081\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 290/516, D Loss: 0.4953569616191089, G Loss: -0.9999528527259827\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 291/516, D Loss: 0.4888702752068639, G Loss: -0.9999593496322632\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 292/516, D Loss: 0.4889989262446761, G Loss: -0.9999673962593079\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 293/516, D Loss: 0.49962039064848796, G Loss: -0.9999650120735168\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 294/516, D Loss: 0.4916107412427664, G Loss: -0.9999613761901855\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 295/516, D Loss: 0.4925915854983032, G Loss: -0.9999710321426392\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 296/516, D Loss: 0.4856162266805768, G Loss: -0.999966561794281\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 297/516, D Loss: 0.49090398009866476, G Loss: -0.999961256980896\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 298/516, D Loss: 0.4883818430826068, G Loss: -0.999967098236084\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 299/516, D Loss: 0.48878348898142576, G Loss: -0.9999666213989258\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 300/516, D Loss: 0.495518303476274, G Loss: -0.9999600648880005\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 301/516, D Loss: 0.4989480917574838, G Loss: -0.9999513626098633\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 302/516, D Loss: 0.48962907679378986, G Loss: -0.99996018409729\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 303/516, D Loss: 0.49821786279790103, G Loss: -0.9999629259109497\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 304/516, D Loss: 0.4977896159980446, G Loss: -0.9999681711196899\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 305/516, D Loss: 0.49438214022666216, G Loss: -0.999965250492096\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 306/516, D Loss: 0.4940431700088084, G Loss: -0.9999643564224243\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 307/516, D Loss: 0.4912706632167101, G Loss: -0.9999673366546631\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 308/516, D Loss: 0.4983481267699972, G Loss: -0.9999711513519287\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 309/516, D Loss: 0.49495870526880026, G Loss: -0.9999585151672363\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 310/516, D Loss: 0.4976345186587423, G Loss: -0.9999703764915466\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 311/516, D Loss: 0.4872468365356326, G Loss: -0.9999665021896362\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 312/516, D Loss: 0.49954609281849116, G Loss: -0.9999725222587585\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 313/516, D Loss: 0.4908609613776207, G Loss: -0.9999669790267944\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 314/516, D Loss: 0.4995401992928237, G Loss: -0.9999679327011108\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 315/516, D Loss: 0.49085313361138105, G Loss: -0.9999713897705078\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 316/516, D Loss: 0.49806771834846586, G Loss: -0.9999725818634033\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 317/516, D Loss: 0.4996073651127517, G Loss: -0.9999620318412781\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 318/516, D Loss: 0.49861133634112775, G Loss: -0.9999696016311646\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 319/516, D Loss: 0.4996410262247082, G Loss: -0.9999659061431885\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 320/516, D Loss: 0.4906508279964328, G Loss: -0.9999691247940063\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 321/516, D Loss: 0.499326053482946, G Loss: -0.9999661445617676\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 322/516, D Loss: 0.49275801610201597, G Loss: -0.999966561794281\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 323/516, D Loss: 0.4982272684574127, G Loss: -0.9999675154685974\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 324/516, D Loss: 0.4898431431502104, G Loss: -0.9999669194221497\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 325/516, D Loss: 0.49483776092529297, G Loss: -0.9999728202819824\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 326/516, D Loss: 0.4983226020121947, G Loss: -0.9999774098396301\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 327/516, D Loss: 0.4950705235823989, G Loss: -0.9999674558639526\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 328/516, D Loss: 0.4914831183850765, G Loss: -0.9999759197235107\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 329/516, D Loss: 0.4945430178195238, G Loss: -0.9999682307243347\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 330/516, D Loss: 0.4966475390829146, G Loss: -0.9999706149101257\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 331/516, D Loss: 0.4981400358956307, G Loss: -0.9999768137931824\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 332/516, D Loss: 0.4931283062323928, G Loss: -0.9999693632125854\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 333/516, D Loss: 0.4941357239149511, G Loss: -0.9999721050262451\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 334/516, D Loss: 0.4953800616785884, G Loss: -0.9999724626541138\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 335/516, D Loss: 0.4944131299853325, G Loss: -0.9999728202819824\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 336/516, D Loss: 0.49931984080467373, G Loss: -0.9999679923057556\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 337/516, D Loss: 0.495335029438138, G Loss: -0.9999750852584839\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 338/516, D Loss: 0.49162332341074944, G Loss: -0.9999762773513794\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 339/516, D Loss: 0.48688067495822906, G Loss: -0.9999728202819824\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 340/516, D Loss: 0.49824956979136914, G Loss: -0.9999722242355347\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 341/516, D Loss: 0.4928500968962908, G Loss: -0.9999745488166809\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 342/516, D Loss: 0.4920174963772297, G Loss: -0.9999641180038452\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 343/516, D Loss: 0.4954879879951477, G Loss: -0.9999688267707825\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 344/516, D Loss: 0.49273961456492543, G Loss: -0.9999716281890869\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 345/516, D Loss: 0.49843237281311303, G Loss: -0.9999773502349854\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 346/516, D Loss: 0.4961199597455561, G Loss: -0.9999723434448242\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 347/516, D Loss: 0.4976496612653136, G Loss: -0.9999724626541138\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 348/516, D Loss: 0.49846223555505276, G Loss: -0.9999762773513794\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 349/516, D Loss: 0.4874293012544513, G Loss: -0.9999706745147705\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 350/516, D Loss: 0.4949055169709027, G Loss: -0.9999744296073914\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 351/516, D Loss: 0.49697285681031644, G Loss: -0.9999754428863525\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 1, Batch 352/516, D Loss: 0.4929643380455673, G Loss: -0.9999719858169556\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 353/516, D Loss: 0.4813563395291567, G Loss: -0.9999702572822571\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 354/516, D Loss: 0.4997118064202368, G Loss: -0.9999722838401794\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 355/516, D Loss: 0.49391537299379706, G Loss: -0.9999718070030212\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 356/516, D Loss: 0.49540300853550434, G Loss: -0.9999679923057556\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 357/516, D Loss: 0.4988403582246974, G Loss: -0.9999732971191406\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 358/516, D Loss: 0.4886877965182066, G Loss: -0.9999622106552124\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 359/516, D Loss: 0.4995504350808915, G Loss: -0.9999716877937317\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 360/516, D Loss: 0.497694248566404, G Loss: -0.9999786615371704\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 361/516, D Loss: 0.49538477836176753, G Loss: -0.9999767541885376\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 362/516, D Loss: 0.48702018335461617, G Loss: -0.9999663829803467\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 363/516, D Loss: 0.494301276281476, G Loss: -0.9999723434448242\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 364/516, D Loss: 0.4877204494550824, G Loss: -0.9999785423278809\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 365/516, D Loss: 0.4983651877846569, G Loss: -0.9999723434448242\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 366/516, D Loss: 0.48706522770226, G Loss: -0.9999759793281555\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 367/516, D Loss: 0.49924350407673046, G Loss: -0.9999760389328003\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 368/516, D Loss: 0.4933639266528189, G Loss: -0.9999717473983765\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 369/516, D Loss: 0.4878970803692937, G Loss: -0.9999766945838928\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 370/516, D Loss: 0.49442180385813117, G Loss: -0.99997478723526\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 371/516, D Loss: 0.497815866721794, G Loss: -0.9999765157699585\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 372/516, D Loss: 0.4947058577090502, G Loss: -0.9999737739562988\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 373/516, D Loss: 0.49863929115235806, G Loss: -0.9999727606773376\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 374/516, D Loss: 0.4971903199329972, G Loss: -0.9999730587005615\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 375/516, D Loss: 0.49863172753248364, G Loss: -0.9999647736549377\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 376/516, D Loss: 0.4987743573728949, G Loss: -0.9999716281890869\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 377/516, D Loss: 0.4908506190404296, G Loss: -0.9999778270721436\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 378/516, D Loss: 0.49751609982922673, G Loss: -0.9999735355377197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 379/516, D Loss: 0.4969912215601653, G Loss: -0.999979555606842\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 380/516, D Loss: 0.49559201998636127, G Loss: -0.9999731183052063\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 381/516, D Loss: 0.49328092485666275, G Loss: -0.999977171421051\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 382/516, D Loss: 0.4976001896429807, G Loss: -0.999976396560669\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 383/516, D Loss: 0.49355457350611687, G Loss: -0.999976634979248\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 384/516, D Loss: 0.49521244317293167, G Loss: -0.9999734163284302\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 385/516, D Loss: 0.4874992575496435, G Loss: -0.9999783039093018\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 386/516, D Loss: 0.49538335017859936, G Loss: -0.9999785423278809\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 387/516, D Loss: 0.48312051594257355, G Loss: -0.9999760985374451\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 388/516, D Loss: 0.4984943682793528, G Loss: -0.9999725818634033\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 389/516, D Loss: 0.49408467719331384, G Loss: -0.9999698400497437\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 390/516, D Loss: 0.48855265136808157, G Loss: -0.9999753832817078\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 391/516, D Loss: 0.495442065410316, G Loss: -0.9999808669090271\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 392/516, D Loss: 0.49384720902889967, G Loss: -0.9999743700027466\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 393/516, D Loss: 0.49067838303744793, G Loss: -0.9999735355377197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 394/516, D Loss: 0.4959521652199328, G Loss: -0.9999780058860779\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 395/516, D Loss: 0.49805579031817615, G Loss: -0.9999837875366211\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 1, Batch 396/516, D Loss: 0.4934057593345642, G Loss: -0.9999797344207764\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 397/516, D Loss: 0.4979924960061908, G Loss: -0.9999788999557495\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 398/516, D Loss: 0.4866522643715143, G Loss: -0.9999765157699585\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 399/516, D Loss: 0.4881883207708597, G Loss: -0.999973475933075\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 400/516, D Loss: 0.4920885022729635, G Loss: -0.9999643564224243\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 401/516, D Loss: 0.4913861807435751, G Loss: -0.9999687075614929\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "Epoch 1, Batch 402/516, D Loss: 0.49890396767295897, G Loss: -0.9999805688858032\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 403/516, D Loss: 0.4990755504113622, G Loss: -0.9999768137931824\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 404/516, D Loss: 0.491573816165328, G Loss: -0.9999821186065674\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 405/516, D Loss: 0.49292262364178896, G Loss: -0.9999728202819824\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 1, Batch 406/516, D Loss: 0.49389938171952963, G Loss: -0.9999784827232361\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 407/516, D Loss: 0.48473262321203947, G Loss: -0.9999758005142212\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 408/516, D Loss: 0.4960271776653826, G Loss: -0.9999723434448242\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 1, Batch 409/516, D Loss: 0.49570794263854623, G Loss: -0.9999760389328003\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 410/516, D Loss: 0.4871919769793749, G Loss: -0.9999864101409912\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 411/516, D Loss: 0.492066977545619, G Loss: -0.9999783039093018\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 412/516, D Loss: 0.49490014370530844, G Loss: -0.9999744892120361\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 413/516, D Loss: 0.4969598385505378, G Loss: -0.9999735355377197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 414/516, D Loss: 0.49177027121186256, G Loss: -0.9999717473983765\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 415/516, D Loss: 0.49884787085466087, G Loss: -0.9999662637710571\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 416/516, D Loss: 0.49973285576561466, G Loss: -0.9999802112579346\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 417/516, D Loss: 0.49000777117908, G Loss: -0.9999786615371704\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 418/516, D Loss: 0.49329618318006396, G Loss: -0.9999749064445496\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 419/516, D Loss: 0.49367133481428027, G Loss: -0.9999830722808838\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 420/516, D Loss: 0.49535850528627634, G Loss: -0.9999791383743286\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 421/516, D Loss: 0.49565294198691845, G Loss: -0.9999812841415405\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 422/516, D Loss: 0.48289801739156246, G Loss: -0.9999703764915466\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 423/516, D Loss: 0.4832421224564314, G Loss: -0.9999806880950928\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 424/516, D Loss: 0.49974195545655675, G Loss: -0.9999772310256958\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 425/516, D Loss: 0.4954125117510557, G Loss: -0.999980628490448\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 426/516, D Loss: 0.495219558943063, G Loss: -0.9999781847000122\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 427/516, D Loss: 0.4926405013538897, G Loss: -0.9999808073043823\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 428/516, D Loss: 0.4905052352696657, G Loss: -0.9999765157699585\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 429/516, D Loss: 0.4939693911001086, G Loss: -0.9999732971191406\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 430/516, D Loss: 0.4856878025457263, G Loss: -0.9999771118164062\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 431/516, D Loss: 0.48819766473025084, G Loss: -0.9999767541885376\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 432/516, D Loss: 0.4909366723150015, G Loss: -0.9999794960021973\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 433/516, D Loss: 0.4942460572347045, G Loss: -0.9999780654907227\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 434/516, D Loss: 0.4976866375654936, G Loss: -0.9999798536300659\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 435/516, D Loss: 0.49944554979447275, G Loss: -0.9999778866767883\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 436/516, D Loss: 0.4995459107449278, G Loss: -0.9999793767929077\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 437/516, D Loss: 0.4950560317374766, G Loss: -0.999979555606842\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 438/516, D Loss: 0.4973648877348751, G Loss: -0.9999778866767883\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 439/516, D Loss: 0.49572358233854175, G Loss: -0.9999797344207764\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 440/516, D Loss: 0.49145699944347143, G Loss: -0.9999756813049316\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 441/516, D Loss: 0.495061282068491, G Loss: -0.9999759197235107\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 442/516, D Loss: 0.49156074598431587, G Loss: -0.9999786615371704\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 443/516, D Loss: 0.4945961288176477, G Loss: -0.9999794960021973\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 444/516, D Loss: 0.49569345312193036, G Loss: -0.9999809861183167\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 445/516, D Loss: 0.48804301116615534, G Loss: -0.9999797344207764\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 446/516, D Loss: 0.4937371639534831, G Loss: -0.9999765157699585\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 447/516, D Loss: 0.4931908827275038, G Loss: -0.9999808073043823\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 1, Batch 448/516, D Loss: 0.49262730963528156, G Loss: -0.9999839067459106\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 1, Batch 449/516, D Loss: 0.49421915458515286, G Loss: -0.9999738335609436\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 450/516, D Loss: 0.4939789013005793, G Loss: -0.9999744892120361\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 451/516, D Loss: 0.4870365308597684, G Loss: -0.999984085559845\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 452/516, D Loss: 0.48802806809544563, G Loss: -0.9999850988388062\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 453/516, D Loss: 0.4990636572474614, G Loss: -0.9999798536300659\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 454/516, D Loss: 0.49106718227267265, G Loss: -0.9999722838401794\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 455/516, D Loss: 0.49525059713050723, G Loss: -0.9999821186065674\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 456/516, D Loss: 0.49833477288484573, G Loss: -0.999978244304657\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 457/516, D Loss: 0.4902425426989794, G Loss: -0.9999842643737793\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 458/516, D Loss: 0.4895213097333908, G Loss: -0.9999812841415405\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 459/516, D Loss: 0.4965179811697453, G Loss: -0.9999834895133972\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 460/516, D Loss: 0.49296279810369015, G Loss: -0.9999792575836182\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 461/516, D Loss: 0.49125357437878847, G Loss: -0.9999786019325256\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 462/516, D Loss: 0.49573894729837775, G Loss: -0.9999809265136719\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 463/516, D Loss: 0.49950285159866326, G Loss: -0.999976396560669\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 464/516, D Loss: 0.4993206064682454, G Loss: -0.9999827146530151\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 465/516, D Loss: 0.49804884451441467, G Loss: -0.9999790787696838\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 466/516, D Loss: 0.4962677704170346, G Loss: -0.999981164932251\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 467/516, D Loss: 0.49747355887666345, G Loss: -0.9999783635139465\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 468/516, D Loss: 0.4997274826746434, G Loss: -0.999984622001648\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 469/516, D Loss: 0.4838056340813637, G Loss: -0.9999858736991882\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 470/516, D Loss: 0.48715025559067726, G Loss: -0.9999799132347107\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 471/516, D Loss: 0.4959538229741156, G Loss: -0.9999779462814331\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 472/516, D Loss: 0.4883606778457761, G Loss: -0.9999818801879883\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 473/516, D Loss: 0.49750533048063517, G Loss: -0.9999826550483704\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 474/516, D Loss: 0.4910349100828171, G Loss: -0.999977707862854\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 475/516, D Loss: 0.4915035776793957, G Loss: -0.999981701374054\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 476/516, D Loss: 0.4977544799912721, G Loss: -0.9999856352806091\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 477/516, D Loss: 0.4944599545560777, G Loss: -0.9999747276306152\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 478/516, D Loss: 0.49262687284499407, G Loss: -0.9999798536300659\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 479/516, D Loss: 0.4956644573248923, G Loss: -0.9999822378158569\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 480/516, D Loss: 0.4962123865261674, G Loss: -0.999984860420227\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 481/516, D Loss: 0.49563337955623865, G Loss: -0.9999824166297913\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 482/516, D Loss: 0.4844578579068184, G Loss: -0.9999805688858032\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 483/516, D Loss: 0.48868754133582115, G Loss: -0.9999833106994629\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 484/516, D Loss: 0.49472949793562293, G Loss: -0.9999814033508301\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 485/516, D Loss: 0.49579384084790945, G Loss: -0.9999868869781494\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 486/516, D Loss: 0.4954108507372439, G Loss: -0.9999847412109375\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 487/516, D Loss: 0.48993511218577623, G Loss: -0.9999831914901733\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 488/516, D Loss: 0.4933533677831292, G Loss: -0.9999779462814331\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 489/516, D Loss: 0.4955334132537246, G Loss: -0.9999832510948181\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 490/516, D Loss: 0.491431032307446, G Loss: -0.999981164932251\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 491/516, D Loss: 0.483490277081728, G Loss: -0.9999809265136719\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 492/516, D Loss: 0.4975160830654204, G Loss: -0.999984085559845\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 493/516, D Loss: 0.4904195908457041, G Loss: -0.9999839067459106\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 494/516, D Loss: 0.4909023763611913, G Loss: -0.999984860420227\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 495/516, D Loss: 0.49393888749182224, G Loss: -0.9999810457229614\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 496/516, D Loss: 0.4911081660538912, G Loss: -0.9999841451644897\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 497/516, D Loss: 0.48234246484935284, G Loss: -0.999981701374054\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 498/516, D Loss: 0.4987814725609496, G Loss: -0.9999801516532898\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 499/516, D Loss: 0.4949389426037669, G Loss: -0.9999815821647644\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 500/516, D Loss: 0.49645599722862244, G Loss: -0.999981164932251\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 501/516, D Loss: 0.4962764149531722, G Loss: -0.9999797344207764\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 502/516, D Loss: 0.49850939062889665, G Loss: -0.999987006187439\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1, Batch 503/516, D Loss: 0.4991647418937646, G Loss: -0.9999858140945435\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 504/516, D Loss: 0.4909338867291808, G Loss: -0.9999864101409912\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 505/516, D Loss: 0.4946870571002364, G Loss: -0.9999821186065674\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 506/516, D Loss: 0.49145183339715004, G Loss: -0.9999803304672241\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 507/516, D Loss: 0.4952963087707758, G Loss: -0.9999815821647644\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 508/516, D Loss: 0.49710121587850153, G Loss: -0.9999855756759644\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 509/516, D Loss: 0.49875405617058277, G Loss: -0.9999828338623047\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 510/516, D Loss: 0.4957994781434536, G Loss: -0.9999784231185913\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 511/516, D Loss: 0.4915203731507063, G Loss: -0.9999819397926331\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 1, Batch 512/516, D Loss: 0.4988612262532115, G Loss: -0.999980092048645\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 513/516, D Loss: 0.4887750428169966, G Loss: -0.9999794363975525\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 514/516, D Loss: 0.4985174268949777, G Loss: -0.99997878074646\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 1, Batch 515/516, D Loss: 0.4982708712341264, G Loss: -0.9999799132347107\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 1, Batch 516/516, D Loss: 0.4841921366751194, G Loss: -0.9999827146530151\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 1/516, D Loss: 0.49899416672997177, G Loss: -0.999980092048645\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 2/516, D Loss: 0.4941230211406946, G Loss: -0.9999855756759644\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 3/516, D Loss: 0.49647455965168774, G Loss: -0.9999809265136719\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 4/516, D Loss: 0.49474236695095897, G Loss: -0.9999839067459106\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 5/516, D Loss: 0.49205061327666044, G Loss: -0.9999786615371704\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 6/516, D Loss: 0.49706721235997975, G Loss: -0.9999862909317017\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 7/516, D Loss: 0.4879990993067622, G Loss: -0.999987006187439\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 8/516, D Loss: 0.49467084975913167, G Loss: -0.9999859929084778\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 9/516, D Loss: 0.49452948942780495, G Loss: -0.9999843239784241\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 10/516, D Loss: 0.49525852454826236, G Loss: -0.9999834299087524\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 11/516, D Loss: 0.486054964363575, G Loss: -0.9999815821647644\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 12/516, D Loss: 0.4958170815370977, G Loss: -0.9999814629554749\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 13/516, D Loss: 0.49490883527323604, G Loss: -0.9999838471412659\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 14/516, D Loss: 0.49472720827907324, G Loss: -0.9999830722808838\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 15/516, D Loss: 0.49553261417895555, G Loss: -0.9999874830245972\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 16/516, D Loss: 0.4895497225224972, G Loss: -0.9999861717224121\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 17/516, D Loss: 0.4866866413503885, G Loss: -0.9999829530715942\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 18/516, D Loss: 0.48669161833822727, G Loss: -0.999986469745636\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 19/516, D Loss: 0.48607601691037416, G Loss: -0.9999861717224121\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 20/516, D Loss: 0.4890987006947398, G Loss: -0.9999871253967285\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 21/516, D Loss: 0.49912305583711714, G Loss: -0.9999824166297913\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 22/516, D Loss: 0.491326748393476, G Loss: -0.9999856948852539\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 23/516, D Loss: 0.4977380915079266, G Loss: -0.9999874234199524\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 24/516, D Loss: 0.49488275311887264, G Loss: -0.9999852180480957\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 25/516, D Loss: 0.49310743482783437, G Loss: -0.9999828934669495\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 26/516, D Loss: 0.49414758710190654, G Loss: -0.9999867677688599\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 27/516, D Loss: 0.4907813146710396, G Loss: -0.9999862909317017\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 28/516, D Loss: 0.497078854823485, G Loss: -0.9999854564666748\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 29/516, D Loss: 0.4882564749568701, G Loss: -0.999987781047821\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 30/516, D Loss: 0.499752958319732, G Loss: -0.9999858140945435\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 31/516, D Loss: 0.48923260159790516, G Loss: -0.9999845027923584\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 32/516, D Loss: 0.48606802709400654, G Loss: -0.9999876022338867\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 33/516, D Loss: 0.49908690131269395, G Loss: -0.9999817609786987\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 34/516, D Loss: 0.49800652684643865, G Loss: -0.9999838471412659\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 35/516, D Loss: 0.4957011053338647, G Loss: -0.9999832510948181\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 36/516, D Loss: 0.4884264934808016, G Loss: -0.9999876022338867\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 37/516, D Loss: 0.4901731153950095, G Loss: -0.9999839067459106\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 38/516, D Loss: 0.4913308508694172, G Loss: -0.9999847412109375\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 39/516, D Loss: 0.4960058815777302, G Loss: -0.9999834895133972\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 40/516, D Loss: 0.48798857629299164, G Loss: -0.9999845623970032\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 41/516, D Loss: 0.49447882222011685, G Loss: -0.9999847412109375\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 42/516, D Loss: 0.4872275684028864, G Loss: -0.9999853372573853\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 43/516, D Loss: 0.4917827881872654, G Loss: -0.9999860525131226\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 44/516, D Loss: 0.4899066425859928, G Loss: -0.999987006187439\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 45/516, D Loss: 0.4904597634449601, G Loss: -0.9999839067459106\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 46/516, D Loss: 0.4929211186245084, G Loss: -0.9999849796295166\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 47/516, D Loss: 0.49440956581383944, G Loss: -0.9999861717224121\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 48/516, D Loss: 0.4994100211188197, G Loss: -0.9999855756759644\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 49/516, D Loss: 0.49926335737109184, G Loss: -0.9999850988388062\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 50/516, D Loss: 0.4986493063624948, G Loss: -0.9999845027923584\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 51/516, D Loss: 0.4918476343154907, G Loss: -0.9999876618385315\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 52/516, D Loss: 0.49529066774994135, G Loss: -0.9999894499778748\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 53/516, D Loss: 0.495055909268558, G Loss: -0.9999885559082031\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 54/516, D Loss: 0.4955477165058255, G Loss: -0.9999864101409912\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 55/516, D Loss: 0.4900109749287367, G Loss: -0.999987006187439\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 56/516, D Loss: 0.49103202670812607, G Loss: -0.99998939037323\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 57/516, D Loss: 0.49450989812612534, G Loss: -0.9999860525131226\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 58/516, D Loss: 0.4904547370970249, G Loss: -0.9999877214431763\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 59/516, D Loss: 0.4977614036761224, G Loss: -0.9999858736991882\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 60/516, D Loss: 0.48494491074234247, G Loss: -0.9999845027923584\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 61/516, D Loss: 0.49413478188216686, G Loss: -0.9999832510948181\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 62/516, D Loss: 0.4940882124938071, G Loss: -0.9999812245368958\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 63/516, D Loss: 0.4919492080807686, G Loss: -0.9999873638153076\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 64/516, D Loss: 0.49734957329928875, G Loss: -0.9999848008155823\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 65/516, D Loss: 0.48753724340349436, G Loss: -0.9999870657920837\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 66/516, D Loss: 0.4935881136916578, G Loss: -0.9999845027923584\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 67/516, D Loss: 0.4953983095474541, G Loss: -0.9999855756759644\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 68/516, D Loss: 0.49214066565036774, G Loss: -0.9999887943267822\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 69/516, D Loss: 0.4950537229888141, G Loss: -0.999985933303833\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 70/516, D Loss: 0.4979896785225719, G Loss: -0.9999850988388062\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 71/516, D Loss: 0.4908302091062069, G Loss: -0.9999885559082031\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 72/516, D Loss: 0.492088426835835, G Loss: -0.9999891519546509\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 73/516, D Loss: 0.4957934655249119, G Loss: -0.9999856948852539\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 74/516, D Loss: 0.49982051526603755, G Loss: -0.9999864101409912\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 75/516, D Loss: 0.4936998882330954, G Loss: -0.9999904632568359\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 76/516, D Loss: 0.4981532790698111, G Loss: -0.9999847412109375\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 77/516, D Loss: 0.49508523009717464, G Loss: -0.9999872446060181\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 78/516, D Loss: 0.48764799255877733, G Loss: -0.9999879598617554\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 79/516, D Loss: 0.4978852062486112, G Loss: -0.9999902248382568\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 80/516, D Loss: 0.49664226872846484, G Loss: -0.9999843239784241\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 81/516, D Loss: 0.48684065975248814, G Loss: -0.9999861121177673\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 82/516, D Loss: 0.49868036224506795, G Loss: -0.9999829530715942\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 83/516, D Loss: 0.4955997234210372, G Loss: -0.9999890327453613\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 84/516, D Loss: 0.48793035838752985, G Loss: -0.9999856352806091\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 85/516, D Loss: 0.49135742895305157, G Loss: -0.9999843835830688\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 86/516, D Loss: 0.49625198170542717, G Loss: -0.9999880194664001\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 87/516, D Loss: 0.49973513893201016, G Loss: -0.9999869465827942\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 88/516, D Loss: 0.4944523824378848, G Loss: -0.9999886751174927\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 89/516, D Loss: 0.499425295740366, G Loss: -0.9999868869781494\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 90/516, D Loss: 0.4966360153630376, G Loss: -0.9999868869781494\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 91/516, D Loss: 0.4950399389490485, G Loss: -0.9999884963035583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 92/516, D Loss: 0.4917489206418395, G Loss: -0.9999860525131226\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 93/516, D Loss: 0.48614564910531044, G Loss: -0.999990701675415\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 94/516, D Loss: 0.4946540384553373, G Loss: -0.9999898076057434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 95/516, D Loss: 0.49430244229733944, G Loss: -0.9999854564666748\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 96/516, D Loss: 0.4953581509180367, G Loss: -0.9999848008155823\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 97/516, D Loss: 0.4973321445286274, G Loss: -0.9999918341636658\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 98/516, D Loss: 0.4968156402464956, G Loss: -0.9999877214431763\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 99/516, D Loss: 0.4955267542973161, G Loss: -0.9999828934669495\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 100/516, D Loss: 0.49457752984017134, G Loss: -0.9999903440475464\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 101/516, D Loss: 0.4955217903479934, G Loss: -0.9999871253967285\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 102/516, D Loss: 0.49650361156091094, G Loss: -0.9999898672103882\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 103/516, D Loss: 0.4887175252661109, G Loss: -0.9999886751174927\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 104/516, D Loss: 0.4983733022818342, G Loss: -0.9999891519546509\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 105/516, D Loss: 0.49301948258653283, G Loss: -0.9999887347221375\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 106/516, D Loss: 0.4927973966114223, G Loss: -0.9999881982803345\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 107/516, D Loss: 0.48830429278314114, G Loss: -0.999991238117218\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 108/516, D Loss: 0.4957785657607019, G Loss: -0.9999849200248718\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 109/516, D Loss: 0.49487152975052595, G Loss: -0.9999870657920837\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 110/516, D Loss: 0.4954028744250536, G Loss: -0.9999865889549255\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 111/516, D Loss: 0.49964947227272205, G Loss: -0.9999908208847046\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 112/516, D Loss: 0.49015707336366177, G Loss: -0.9999904036521912\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 113/516, D Loss: 0.4988117488101125, G Loss: -0.9999874830245972\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 114/516, D Loss: 0.496759211178869, G Loss: -0.9999876022338867\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 115/516, D Loss: 0.49281616788357496, G Loss: -0.9999870657920837\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 116/516, D Loss: 0.49069496989250183, G Loss: -0.9999873638153076\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 117/516, D Loss: 0.4995232509681955, G Loss: -0.9999836683273315\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 118/516, D Loss: 0.4919815808534622, G Loss: -0.9999899864196777\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 119/516, D Loss: 0.4991724976571277, G Loss: -0.999984860420227\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 120/516, D Loss: 0.48936459328979254, G Loss: -0.999985933303833\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 121/516, D Loss: 0.49873094668146223, G Loss: -0.9999884366989136\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 122/516, D Loss: 0.4989378332393244, G Loss: -0.999985933303833\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 123/516, D Loss: 0.49427404114976525, G Loss: -0.9999907612800598\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 124/516, D Loss: 0.49575931997969747, G Loss: -0.9999886751174927\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 125/516, D Loss: 0.4953971137292683, G Loss: -0.9999879598617554\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 126/516, D Loss: 0.4888887209817767, G Loss: -0.9999856948852539\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 127/516, D Loss: 0.498354819486849, G Loss: -0.999984860420227\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 128/516, D Loss: 0.4993417866062373, G Loss: -0.9999896287918091\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 129/516, D Loss: 0.4946567853912711, G Loss: -0.9999915361404419\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 130/516, D Loss: 0.4996618021687027, G Loss: -0.9999898076057434\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 131/516, D Loss: 0.49825340090319514, G Loss: -0.999984622001648\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 132/516, D Loss: 0.49077595956623554, G Loss: -0.9999871253967285\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 133/516, D Loss: 0.49971857346827164, G Loss: -0.9999898076057434\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 134/516, D Loss: 0.49126541428267956, G Loss: -0.9999890327453613\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 135/516, D Loss: 0.4987536391709, G Loss: -0.9999909996986389\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 136/516, D Loss: 0.49577282508835196, G Loss: -0.9999869465827942\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 137/516, D Loss: 0.4970766678452492, G Loss: -0.999987006187439\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 138/516, D Loss: 0.4932537223212421, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 139/516, D Loss: 0.4872209560126066, G Loss: -0.9999886155128479\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 140/516, D Loss: 0.49685349548235536, G Loss: -0.9999886751174927\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 141/516, D Loss: 0.49745407700538635, G Loss: -0.9999880790710449\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 142/516, D Loss: 0.4958383128978312, G Loss: -0.9999889135360718\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 143/516, D Loss: 0.4919373570010066, G Loss: -0.9999920129776001\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 144/516, D Loss: 0.49214552994817495, G Loss: -0.9999892115592957\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 145/516, D Loss: 0.49294984759762883, G Loss: -0.9999901056289673\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 146/516, D Loss: 0.49596155993640423, G Loss: -0.9999884963035583\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 147/516, D Loss: 0.48995937034487724, G Loss: -0.9999885559082031\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 148/516, D Loss: 0.48925992753356695, G Loss: -0.9999887347221375\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 149/516, D Loss: 0.4985577892512083, G Loss: -0.9999884366989136\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 150/516, D Loss: 0.4833984896540642, G Loss: -0.9999881386756897\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 151/516, D Loss: 0.49941180972382426, G Loss: -0.9999908804893494\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 152/516, D Loss: 0.49607399152591825, G Loss: -0.999987006187439\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 153/516, D Loss: 0.49549784883856773, G Loss: -0.999989926815033\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 154/516, D Loss: 0.49591698218137026, G Loss: -0.9999892115592957\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 155/516, D Loss: 0.4955277503468096, G Loss: -0.9999918937683105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 156/516, D Loss: 0.4926688685081899, G Loss: -0.9999850988388062\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 157/516, D Loss: 0.4993275295710191, G Loss: -0.9999865293502808\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 158/516, D Loss: 0.4956680191680789, G Loss: -0.9999891519546509\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 159/516, D Loss: 0.49413361959159374, G Loss: -0.9999865293502808\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 160/516, D Loss: 0.49048884119838476, G Loss: -0.9999887347221375\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 161/516, D Loss: 0.487564898096025, G Loss: -0.9999895095825195\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 162/516, D Loss: 0.4933384684845805, G Loss: -0.9999884366989136\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 163/516, D Loss: 0.4951107972301543, G Loss: -0.9999885559082031\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 164/516, D Loss: 0.4950872170738876, G Loss: -0.9999898672103882\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 165/516, D Loss: 0.4886692687869072, G Loss: -0.9999902248382568\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 166/516, D Loss: 0.4973581836093217, G Loss: -0.9999880790710449\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 167/516, D Loss: 0.4981136586284265, G Loss: -0.9999915361404419\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 168/516, D Loss: 0.48456964641809464, G Loss: -0.9999898672103882\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 169/516, D Loss: 0.49989682572777383, G Loss: -0.9999886751174927\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 170/516, D Loss: 0.4997096492443234, G Loss: -0.9999898672103882\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 171/516, D Loss: 0.4904557950794697, G Loss: -0.9999906420707703\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 172/516, D Loss: 0.4992758470471017, G Loss: -0.9999853372573853\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 173/516, D Loss: 0.4969504214823246, G Loss: -0.9999885559082031\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 174/516, D Loss: 0.49239439191296697, G Loss: -0.9999892711639404\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 175/516, D Loss: 0.49576974008232355, G Loss: -0.9999903440475464\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 176/516, D Loss: 0.4899572283029556, G Loss: -0.9999902248382568\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 177/516, D Loss: 0.48725943826138973, G Loss: -0.9999881982803345\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 178/516, D Loss: 0.49101041723042727, G Loss: -0.9999898672103882\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 179/516, D Loss: 0.49743619142100215, G Loss: -0.9999890327453613\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 180/516, D Loss: 0.4956669951789081, G Loss: -0.9999896287918091\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 181/516, D Loss: 0.49032558500766754, G Loss: -0.9999897480010986\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 182/516, D Loss: 0.4961666518356651, G Loss: -0.9999917149543762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 183/516, D Loss: 0.49587216321378946, G Loss: -0.9999904036521912\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 184/516, D Loss: 0.4987079920247197, G Loss: -0.9999931454658508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 185/516, D Loss: 0.4879568452015519, G Loss: -0.9999921917915344\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 186/516, D Loss: 0.49510971922427416, G Loss: -0.9999902248382568\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 187/516, D Loss: 0.4951252690516412, G Loss: -0.9999908208847046\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 188/516, D Loss: 0.4952585631981492, G Loss: -0.9999867081642151\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 189/516, D Loss: 0.49585625901818275, G Loss: -0.9999898076057434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 190/516, D Loss: 0.49482356756925583, G Loss: -0.9999893307685852\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 191/516, D Loss: 0.49437951296567917, G Loss: -0.9999881982803345\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 192/516, D Loss: 0.49814240424893796, G Loss: -0.9999916553497314\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 193/516, D Loss: 0.4936586576513946, G Loss: -0.9999904632568359\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 194/516, D Loss: 0.49947651266120374, G Loss: -0.9999914169311523\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 195/516, D Loss: 0.49504035199061036, G Loss: -0.999989926815033\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 196/516, D Loss: 0.49762042332440615, G Loss: -0.9999883770942688\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 197/516, D Loss: 0.4898402448743582, G Loss: -0.9999921321868896\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 198/516, D Loss: 0.49169868882745504, G Loss: -0.9999874830245972\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 199/516, D Loss: 0.4919325513765216, G Loss: -0.9999896883964539\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 200/516, D Loss: 0.4957962422631681, G Loss: -0.9999890923500061\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 201/516, D Loss: 0.4918335471302271, G Loss: -0.9999900460243225\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 202/516, D Loss: 0.4915296696126461, G Loss: -0.9999911785125732\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 203/516, D Loss: 0.4888839293271303, G Loss: -0.9999930262565613\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 204/516, D Loss: 0.4881236143410206, G Loss: -0.9999918937683105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 205/516, D Loss: 0.4917766693979502, G Loss: -0.9999901056289673\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 206/516, D Loss: 0.48747030086815357, G Loss: -0.9999905228614807\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 207/516, D Loss: 0.49409951735287905, G Loss: -0.9999890327453613\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 208/516, D Loss: 0.48303220607340336, G Loss: -0.9999884366989136\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 209/516, D Loss: 0.4945629667490721, G Loss: -0.9999908804893494\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 210/516, D Loss: 0.49749910179525614, G Loss: -0.9999918937683105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 211/516, D Loss: 0.49149330891668797, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 212/516, D Loss: 0.49439265485852957, G Loss: -0.9999883770942688\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 213/516, D Loss: 0.4931031111627817, G Loss: -0.9999918937683105\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 214/516, D Loss: 0.49002364184707403, G Loss: -0.9999909400939941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 215/516, D Loss: 0.4930073982104659, G Loss: -0.999990701675415\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 216/516, D Loss: 0.4934203941375017, G Loss: -0.999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 217/516, D Loss: 0.4953869367018342, G Loss: -0.9999905824661255\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 218/516, D Loss: 0.49506305484101176, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 219/516, D Loss: 0.4992226796457544, G Loss: -0.9999916553497314\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 220/516, D Loss: 0.49833801039494574, G Loss: -0.9999892711639404\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 221/516, D Loss: 0.49112515337765217, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 222/516, D Loss: 0.4879997540265322, G Loss: -0.9999921321868896\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 223/516, D Loss: 0.49163075722754, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 224/516, D Loss: 0.49439143436029553, G Loss: -0.9999904036521912\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 225/516, D Loss: 0.4954459862783551, G Loss: -0.9999904632568359\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 226/516, D Loss: 0.483458386734128, G Loss: -0.9999903440475464\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 227/516, D Loss: 0.49612764292396605, G Loss: -0.9999876022338867\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 228/516, D Loss: 0.4794958457350731, G Loss: -0.999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 229/516, D Loss: 0.4957511965185404, G Loss: -0.9999929666519165\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 230/516, D Loss: 0.49974051874596626, G Loss: -0.9999889135360718\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 231/516, D Loss: 0.4962878110818565, G Loss: -0.9999895691871643\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 232/516, D Loss: 0.4985207044519484, G Loss: -0.9999896287918091\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 233/516, D Loss: 0.49892199598252773, G Loss: -0.9999908208847046\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 234/516, D Loss: 0.49935486214235425, G Loss: -0.9999885559082031\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 235/516, D Loss: 0.4919099509716034, G Loss: -0.9999887943267822\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 236/516, D Loss: 0.4986366491066292, G Loss: -0.9999924302101135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 237/516, D Loss: 0.49219978461042047, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 238/516, D Loss: 0.49473345652222633, G Loss: -0.9999905824661255\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 239/516, D Loss: 0.4952678829431534, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 240/516, D Loss: 0.493874566629529, G Loss: -0.9999921917915344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 241/516, D Loss: 0.4920986955985427, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 242/516, D Loss: 0.49463175097480416, G Loss: -0.9999926686286926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 243/516, D Loss: 0.49891931971069425, G Loss: -0.9999914765357971\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 244/516, D Loss: 0.4898897763341665, G Loss: -0.9999911189079285\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 245/516, D Loss: 0.4938347265124321, G Loss: -0.9999903440475464\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 246/516, D Loss: 0.49867574113886803, G Loss: -0.999990701675415\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 247/516, D Loss: 0.49061679281294346, G Loss: -0.9999924302101135\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 248/516, D Loss: 0.4929027664475143, G Loss: -0.9999904632568359\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 249/516, D Loss: 0.4960322887636721, G Loss: -0.9999905228614807\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 250/516, D Loss: 0.48457475006580353, G Loss: -0.9999905228614807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 251/516, D Loss: 0.49827163107693195, G Loss: -0.9999892711639404\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 252/516, D Loss: 0.4974338677711785, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 253/516, D Loss: 0.49883360124658793, G Loss: -0.9999943971633911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 254/516, D Loss: 0.4869745261967182, G Loss: -0.9999921321868896\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 255/516, D Loss: 0.4993429278838448, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 256/516, D Loss: 0.49841223435942084, G Loss: -0.9999933242797852\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 257/516, D Loss: 0.49968787786201574, G Loss: -0.9999932050704956\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 258/516, D Loss: 0.4995041709044017, G Loss: -0.9999898076057434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 259/516, D Loss: 0.48922591377049685, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 260/516, D Loss: 0.4895941372960806, G Loss: -0.9999944567680359\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 261/516, D Loss: 0.49365579383447766, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 262/516, D Loss: 0.4951695706695318, G Loss: -0.9999913573265076\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 263/516, D Loss: 0.4898680569604039, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 264/516, D Loss: 0.48589249420911074, G Loss: -0.9999924302101135\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 265/516, D Loss: 0.49554276280105114, G Loss: -0.9999901056289673\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 266/516, D Loss: 0.4918482480570674, G Loss: -0.9999920725822449\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 267/516, D Loss: 0.4969608567189425, G Loss: -0.9999926686286926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 268/516, D Loss: 0.49867035378701985, G Loss: -0.9999939799308777\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 269/516, D Loss: 0.4967019921168685, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 270/516, D Loss: 0.4867723695933819, G Loss: -0.9999908804893494\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 271/516, D Loss: 0.49851088458672166, G Loss: -0.9999920129776001\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 272/516, D Loss: 0.4954122770577669, G Loss: -0.9999932050704956\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 273/516, D Loss: 0.4931227844208479, G Loss: -0.9999920725822449\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 274/516, D Loss: 0.4904916472733021, G Loss: -0.999992311000824\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 275/516, D Loss: 0.4969282238744199, G Loss: -0.9999908208847046\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 276/516, D Loss: 0.4886449780315161, G Loss: -0.9999938607215881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 277/516, D Loss: 0.49869868112728, G Loss: -0.9999933242797852\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 278/516, D Loss: 0.4993709826958366, G Loss: -0.9999925494194031\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 279/516, D Loss: 0.49552293214946985, G Loss: -0.9999918341636658\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 280/516, D Loss: 0.49901582673192024, G Loss: -0.9999903440475464\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 281/516, D Loss: 0.49949634692166, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 282/516, D Loss: 0.49984306836267933, G Loss: -0.9999897480010986\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 283/516, D Loss: 0.4979047717060894, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 284/516, D Loss: 0.49847927829250693, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 285/516, D Loss: 0.49909131677122787, G Loss: -0.9999933242797852\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 286/516, D Loss: 0.4995832768618129, G Loss: -0.9999937415122986\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 287/516, D Loss: 0.4862584322690964, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 288/516, D Loss: 0.4947221796028316, G Loss: -0.9999921321868896\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 289/516, D Loss: 0.49543262319639325, G Loss: -0.9999932050704956\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 290/516, D Loss: 0.49531744373962283, G Loss: -0.9999913573265076\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 291/516, D Loss: 0.489531135186553, G Loss: -0.9999942183494568\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 292/516, D Loss: 0.4993301128852181, G Loss: -0.9999920129776001\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 293/516, D Loss: 0.499603694450343, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 294/516, D Loss: 0.49961776414420456, G Loss: -0.999993085861206\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 295/516, D Loss: 0.495451083406806, G Loss: -0.9999914169311523\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 296/516, D Loss: 0.4952406734228134, G Loss: -0.9999921321868896\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 297/516, D Loss: 0.49014071747660637, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 298/516, D Loss: 0.4885397246107459, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 299/516, D Loss: 0.49593534227460623, G Loss: -0.9999894499778748\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 300/516, D Loss: 0.4987586282659322, G Loss: -0.9999940991401672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 301/516, D Loss: 0.49934317788574845, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 302/516, D Loss: 0.48971476033329964, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 303/516, D Loss: 0.4993050674092956, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 304/516, D Loss: 0.48937087040394545, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 305/516, D Loss: 0.4927388085052371, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 306/516, D Loss: 0.48782163485884666, G Loss: -0.999993622303009\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 307/516, D Loss: 0.4874818604439497, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 308/516, D Loss: 0.4901754418388009, G Loss: -0.9999934434890747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 309/516, D Loss: 0.4873792976140976, G Loss: -0.9999934434890747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 310/516, D Loss: 0.49640033580362797, G Loss: -0.9999944567680359\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 311/516, D Loss: 0.49162217788398266, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 312/516, D Loss: 0.48796271439641714, G Loss: -0.9999942183494568\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 313/516, D Loss: 0.49239237140864134, G Loss: -0.9999942183494568\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 314/516, D Loss: 0.49508373299613595, G Loss: -0.9999919533729553\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 315/516, D Loss: 0.4951228811405599, G Loss: -0.9999920129776001\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 316/516, D Loss: 0.4941501598805189, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 317/516, D Loss: 0.49596933415159583, G Loss: -0.9999942183494568\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 318/516, D Loss: 0.49117649160325527, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 319/516, D Loss: 0.4938632193952799, G Loss: -0.9999923706054688\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 320/516, D Loss: 0.49050338566303253, G Loss: -0.9999934434890747\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 321/516, D Loss: 0.4903085492551327, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 322/516, D Loss: 0.4950498607940972, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 323/516, D Loss: 0.498877854202874, G Loss: -0.9999905228614807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 324/516, D Loss: 0.49709907453507185, G Loss: -0.9999922513961792\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 325/516, D Loss: 0.4932884303852916, G Loss: -0.9999942183494568\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 326/516, D Loss: 0.4982318739639595, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 327/516, D Loss: 0.4991155777242966, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 328/516, D Loss: 0.4945198963396251, G Loss: -0.9999949336051941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 329/516, D Loss: 0.49865525821223855, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 330/516, D Loss: 0.4986186391906813, G Loss: -0.9999923706054688\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 331/516, D Loss: 0.4906176561489701, G Loss: -0.9999931454658508\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 332/516, D Loss: 0.4835267588496208, G Loss: -0.999993622303009\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 333/516, D Loss: 0.49442657455801964, G Loss: -0.9999925494194031\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 334/516, D Loss: 0.4901205515488982, G Loss: -0.999993085861206\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 335/516, D Loss: 0.49577869987115264, G Loss: -0.9999908208847046\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 336/516, D Loss: 0.4911324568092823, G Loss: -0.999994695186615\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 337/516, D Loss: 0.49782192963175476, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 338/516, D Loss: 0.49448292423039675, G Loss: -0.9999915361404419\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 339/516, D Loss: 0.4908474534749985, G Loss: -0.9999932050704956\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 340/516, D Loss: 0.49319801945239305, G Loss: -0.9999929070472717\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 341/516, D Loss: 0.4994790575001389, G Loss: -0.999993622303009\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 342/516, D Loss: 0.49890469037927687, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 343/516, D Loss: 0.49556035827845335, G Loss: -0.9999925494194031\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 344/516, D Loss: 0.4947664188221097, G Loss: -0.9999927878379822\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 345/516, D Loss: 0.48469266574829817, G Loss: -0.9999919533729553\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 346/516, D Loss: 0.4981794657651335, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 347/516, D Loss: 0.4916726788505912, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 348/516, D Loss: 0.4928788635879755, G Loss: -0.9999934434890747\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 349/516, D Loss: 0.4909087745472789, G Loss: -0.9999940395355225\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 350/516, D Loss: 0.4871711805462837, G Loss: -0.999993085861206\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 351/516, D Loss: 0.49363974668085575, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 352/516, D Loss: 0.49796003964729607, G Loss: -0.9999933242797852\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 353/516, D Loss: 0.49191093631088734, G Loss: -0.999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 354/516, D Loss: 0.4871279411017895, G Loss: -0.9999934434890747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 355/516, D Loss: 0.49868126946967095, G Loss: -0.9999933242797852\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 356/516, D Loss: 0.4992814618162811, G Loss: -0.9999940991401672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 357/516, D Loss: 0.4985436585266143, G Loss: -0.9999927878379822\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 358/516, D Loss: 0.49333728663623333, G Loss: -0.9999942183494568\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 359/516, D Loss: 0.49810427287593484, G Loss: -0.9999923706054688\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 360/516, D Loss: 0.4858623808249831, G Loss: -0.9999915957450867\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 361/516, D Loss: 0.49552525812759995, G Loss: -0.9999911785125732\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 362/516, D Loss: 0.4882056200876832, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 363/516, D Loss: 0.49539908207952976, G Loss: -0.9999944567680359\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 364/516, D Loss: 0.4955268348567188, G Loss: -0.9999951124191284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 365/516, D Loss: 0.4978008361067623, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 366/516, D Loss: 0.4978076135739684, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 367/516, D Loss: 0.49478491488844156, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 368/516, D Loss: 0.4923745789565146, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 369/516, D Loss: 0.49069743044674397, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 370/516, D Loss: 0.48702603578567505, G Loss: -0.9999932646751404\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 371/516, D Loss: 0.49163169600069523, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 372/516, D Loss: 0.497556593734771, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 373/516, D Loss: 0.4989088965812698, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 374/516, D Loss: 0.4954644422978163, G Loss: -0.999991774559021\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 375/516, D Loss: 0.4942398779094219, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 376/516, D Loss: 0.4987864790018648, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 377/516, D Loss: 0.4876964595168829, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 378/516, D Loss: 0.49950945581076667, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 379/516, D Loss: 0.4951558839529753, G Loss: -0.9999933838844299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 380/516, D Loss: 0.489623318426311, G Loss: -0.9999945759773254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 381/516, D Loss: 0.49136379919946194, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 382/516, D Loss: 0.49837978393770754, G Loss: -0.9999926686286926\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 383/516, D Loss: 0.4928191932849586, G Loss: -0.9999942183494568\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 384/516, D Loss: 0.49731881683692336, G Loss: -0.9999949336051941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 385/516, D Loss: 0.49202601891011, G Loss: -0.9999951720237732\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 386/516, D Loss: 0.4947374016046524, G Loss: -0.9999949932098389\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 387/516, D Loss: 0.49916123104048893, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 388/516, D Loss: 0.4848423507064581, G Loss: -0.9999934434890747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 389/516, D Loss: 0.49747377075254917, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 390/516, D Loss: 0.4988103867508471, G Loss: -0.9999931454658508\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 391/516, D Loss: 0.4890572912991047, G Loss: -0.9999943971633911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 392/516, D Loss: 0.4932347182184458, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 393/516, D Loss: 0.48166945204138756, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 394/516, D Loss: 0.4960508528165519, G Loss: -0.9999947547912598\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 395/516, D Loss: 0.4919511638581753, G Loss: -0.9999943971633911\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 396/516, D Loss: 0.49349210364744067, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 397/516, D Loss: 0.49625064292922616, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 398/516, D Loss: 0.4911208851262927, G Loss: -0.9999944567680359\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 399/516, D Loss: 0.49590139649808407, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 400/516, D Loss: 0.4911410603672266, G Loss: -0.9999940991401672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 401/516, D Loss: 0.4980291191022843, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 402/516, D Loss: 0.4993807423161343, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 403/516, D Loss: 0.48953283950686455, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 404/516, D Loss: 0.48921657633036375, G Loss: -0.9999945759773254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 405/516, D Loss: 0.4986852556467056, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 406/516, D Loss: 0.4996816836646758, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 407/516, D Loss: 0.4959197356365621, G Loss: -0.9999943971633911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 408/516, D Loss: 0.49414049088954926, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 409/516, D Loss: 0.4888383215293288, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 410/516, D Loss: 0.49971003649989143, G Loss: -0.9999926090240479\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 411/516, D Loss: 0.4874082114547491, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 412/516, D Loss: 0.4876062795519829, G Loss: -0.9999926686286926\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 413/516, D Loss: 0.49247544910758734, G Loss: -0.9999927282333374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 414/516, D Loss: 0.48351941630244255, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 415/516, D Loss: 0.4987556373234838, G Loss: -0.9999939799308777\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 416/516, D Loss: 0.490829361602664, G Loss: -0.9999949932098389\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 417/516, D Loss: 0.48705664090812206, G Loss: -0.9999938607215881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 418/516, D Loss: 0.49857518915086985, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 419/516, D Loss: 0.49518182687461376, G Loss: -0.9999948143959045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 420/516, D Loss: 0.4919182239100337, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 421/516, D Loss: 0.495438848156482, G Loss: -0.9999939799308777\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 422/516, D Loss: 0.499604085664032, G Loss: -0.999990701675415\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 423/516, D Loss: 0.4985911841504276, G Loss: -0.9999947547912598\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 424/516, D Loss: 0.4989577680826187, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 425/516, D Loss: 0.4901150381192565, G Loss: -0.9999949336051941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 426/516, D Loss: 0.4920107927173376, G Loss: -0.9999940991401672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 427/516, D Loss: 0.49554002238437533, G Loss: -0.9999952912330627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 428/516, D Loss: 0.4892009552568197, G Loss: -0.9999939799308777\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 429/516, D Loss: 0.49155144952237606, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 430/516, D Loss: 0.49887963104993105, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 431/516, D Loss: 0.4992706835619174, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 432/516, D Loss: 0.49946495657786727, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 433/516, D Loss: 0.49528278037905693, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 434/516, D Loss: 0.49436213076114655, G Loss: -0.9999962449073792\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 435/516, D Loss: 0.49618758424185216, G Loss: -0.9999947547912598\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 436/516, D Loss: 0.4870689921081066, G Loss: -0.999995768070221\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 437/516, D Loss: 0.4984860779950395, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 438/516, D Loss: 0.4998407981620403, G Loss: -0.9999951720237732\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 2, Batch 439/516, D Loss: 0.49544339906424284, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 440/516, D Loss: 0.49465082958340645, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 441/516, D Loss: 0.47055086866021156, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 442/516, D Loss: 0.49402381014078856, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 443/516, D Loss: 0.4996283209475223, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 444/516, D Loss: 0.49294091761112213, G Loss: -0.9999950528144836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 445/516, D Loss: 0.4963037930428982, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 446/516, D Loss: 0.49055465403944254, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 447/516, D Loss: 0.4937741495668888, G Loss: -0.9999955296516418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 448/516, D Loss: 0.49959572282386944, G Loss: -0.9999949336051941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 449/516, D Loss: 0.493370677344501, G Loss: -0.9999948143959045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 450/516, D Loss: 0.4990453674690798, G Loss: -0.9999939203262329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 451/516, D Loss: 0.4845704436302185, G Loss: -0.9999954104423523\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 452/516, D Loss: 0.4992866846732795, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 453/516, D Loss: 0.49875850754324347, G Loss: -0.9999929070472717\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 454/516, D Loss: 0.49469856079667807, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 455/516, D Loss: 0.4915509959682822, G Loss: -0.9999951720237732\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 456/516, D Loss: 0.4968867546413094, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 457/516, D Loss: 0.4955496331676841, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 458/516, D Loss: 0.496023996733129, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 459/516, D Loss: 0.49173451494425535, G Loss: -0.9999951720237732\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 460/516, D Loss: 0.4911673655733466, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 461/516, D Loss: 0.4915587157011032, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 462/516, D Loss: 0.4943323633633554, G Loss: -0.9999947547912598\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 463/516, D Loss: 0.48233966156840324, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 464/516, D Loss: 0.4924399107694626, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 465/516, D Loss: 0.4984447272727266, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 466/516, D Loss: 0.4858778156340122, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 467/516, D Loss: 0.48796791210770607, G Loss: -0.9999935626983643\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 2, Batch 468/516, D Loss: 0.49703229195438325, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 469/516, D Loss: 0.490079078823328, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 470/516, D Loss: 0.49927663593553007, G Loss: -0.9999951124191284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 471/516, D Loss: 0.4886326873674989, G Loss: -0.9999949336051941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 472/516, D Loss: 0.49566550552845, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 473/516, D Loss: 0.49959951126948, G Loss: -0.9999951124191284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 474/516, D Loss: 0.49186520278453827, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 475/516, D Loss: 0.49612487736158073, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 476/516, D Loss: 0.48940414376556873, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 477/516, D Loss: 0.49905849987408146, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 478/516, D Loss: 0.4915407905355096, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 479/516, D Loss: 0.4851613054051995, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 480/516, D Loss: 0.4921500822529197, G Loss: -0.9999949932098389\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 481/516, D Loss: 0.4910052912309766, G Loss: -0.9999943971633911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 482/516, D Loss: 0.48990029748529196, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 483/516, D Loss: 0.49942353420192376, G Loss: -0.9999967217445374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 484/516, D Loss: 0.49582530511543155, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 485/516, D Loss: 0.4991945523652248, G Loss: -0.9999934434890747\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 486/516, D Loss: 0.49770822119899094, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 487/516, D Loss: 0.49731624173000455, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 488/516, D Loss: 0.4993042352143675, G Loss: -0.9999955296516418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 489/516, D Loss: 0.4997410583891906, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 490/516, D Loss: 0.49178021121770144, G Loss: -0.9999940395355225\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 491/516, D Loss: 0.4881567731499672, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 492/516, D Loss: 0.4951517628505826, G Loss: -0.9999948740005493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 493/516, D Loss: 0.4889049306511879, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 494/516, D Loss: 0.49737856118008494, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 495/516, D Loss: 0.49506646720692515, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 496/516, D Loss: 0.49237739155068994, G Loss: -0.9999945163726807\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 497/516, D Loss: 0.48907236382365227, G Loss: -0.9999942779541016\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 498/516, D Loss: 0.48977086786180735, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 499/516, D Loss: 0.49485500855371356, G Loss: -0.9999947547912598\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 500/516, D Loss: 0.49307019636034966, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 501/516, D Loss: 0.49948009668150917, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 502/516, D Loss: 0.49852111632935703, G Loss: -0.9999946355819702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 503/516, D Loss: 0.48905665799975395, G Loss: -0.9999938607215881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 504/516, D Loss: 0.4971239622682333, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 505/516, D Loss: 0.49584283167496324, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 506/516, D Loss: 0.49029318057000637, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 507/516, D Loss: 0.4912619823589921, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 508/516, D Loss: 0.4887815648689866, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 509/516, D Loss: 0.4796898812055588, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 510/516, D Loss: 0.49877573642879725, G Loss: -0.9999963641166687\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 511/516, D Loss: 0.4954297859221697, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 512/516, D Loss: 0.4978214013390243, G Loss: -0.999994158744812\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 513/516, D Loss: 0.49812616140116006, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 2, Batch 514/516, D Loss: 0.4881993904709816, G Loss: -0.9999938011169434\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 515/516, D Loss: 0.4924871469847858, G Loss: -0.9999940395355225\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 2, Batch 516/516, D Loss: 0.485764117911458, G Loss: -0.9999938607215881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 1/516, D Loss: 0.4984945018077269, G Loss: -0.9999943375587463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 2/516, D Loss: 0.4857568284496665, G Loss: -0.9999924898147583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 3/516, D Loss: 0.4994479630840942, G Loss: -0.9999963641166687\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 4/516, D Loss: 0.49882908537983894, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 5/516, D Loss: 0.4907189663499594, G Loss: -0.9999947547912598\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 6/516, D Loss: 0.49231596710160375, G Loss: -0.999995768070221\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 7/516, D Loss: 0.4981200243346393, G Loss: -0.9999936819076538\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 8/516, D Loss: 0.4978883317671716, G Loss: -0.9999956488609314\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 9/516, D Loss: 0.4955886397510767, G Loss: -0.999993085861206\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 10/516, D Loss: 0.4865391878411174, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 11/516, D Loss: 0.49388699140399694, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 12/516, D Loss: 0.49056100472807884, G Loss: -0.9999963641166687\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 13/516, D Loss: 0.4943103613331914, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 14/516, D Loss: 0.49113696068525314, G Loss: -0.9999949932098389\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 15/516, D Loss: 0.49841212993487716, G Loss: -0.9999955296516418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 16/516, D Loss: 0.4977640474680811, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 17/516, D Loss: 0.4982472824631259, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 18/516, D Loss: 0.4774963427335024, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 19/516, D Loss: 0.4951364267617464, G Loss: -0.9999961256980896\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 20/516, D Loss: 0.4949543126858771, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 21/516, D Loss: 0.4951091278344393, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 22/516, D Loss: 0.4994042016332969, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 23/516, D Loss: 0.48904767632484436, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 24/516, D Loss: 0.4996085665479768, G Loss: -0.9999942183494568\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 25/516, D Loss: 0.49399605160579085, G Loss: -0.9999949336051941\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 26/516, D Loss: 0.4955711583606899, G Loss: -0.9999951720237732\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 27/516, D Loss: 0.48235122859477997, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 28/516, D Loss: 0.4891851330175996, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 29/516, D Loss: 0.48472638707607985, G Loss: -0.9999951720237732\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 30/516, D Loss: 0.49760361621156335, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 31/516, D Loss: 0.49403163604438305, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 32/516, D Loss: 0.4900619210675359, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 33/516, D Loss: 0.49075100291520357, G Loss: -0.9999954104423523\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 34/516, D Loss: 0.4946379866451025, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 35/516, D Loss: 0.4951435485854745, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 36/516, D Loss: 0.4919013511389494, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 37/516, D Loss: 0.49311495991423726, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 38/516, D Loss: 0.4987902684370056, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 39/516, D Loss: 0.48895632289350033, G Loss: -0.999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 40/516, D Loss: 0.4900694703683257, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 41/516, D Loss: 0.499702051456552, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 42/516, D Loss: 0.470110509544611, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 43/516, D Loss: 0.49872854934073985, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 44/516, D Loss: 0.4929842744022608, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 45/516, D Loss: 0.48750873282551765, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 46/516, D Loss: 0.49954234645701945, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 47/516, D Loss: 0.4921362195163965, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 48/516, D Loss: 0.4916428439319134, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 49/516, D Loss: 0.49478259310126305, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 50/516, D Loss: 0.495131176430732, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 51/516, D Loss: 0.49964036836172454, G Loss: -0.9999966025352478\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 52/516, D Loss: 0.48983676359057426, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 53/516, D Loss: 0.49147512950003147, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 54/516, D Loss: 0.49239347549155354, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 55/516, D Loss: 0.4814090710133314, G Loss: -0.9999967217445374\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 56/516, D Loss: 0.49704232229851186, G Loss: -0.9999961256980896\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 57/516, D Loss: 0.48898099828511477, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 58/516, D Loss: 0.4917786242440343, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 59/516, D Loss: 0.4981445281300694, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 60/516, D Loss: 0.4899260876700282, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 61/516, D Loss: 0.49157826881855726, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 62/516, D Loss: 0.499701291177189, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 63/516, D Loss: 0.4955557710491121, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 64/516, D Loss: 0.49010518845170736, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 65/516, D Loss: 0.49638977251015604, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 66/516, D Loss: 0.49740411387756467, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 67/516, D Loss: 0.4993365239351988, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 68/516, D Loss: 0.49717690516263247, G Loss: -0.9999932050704956\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 69/516, D Loss: 0.49579469859600067, G Loss: -0.9999971985816956\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 70/516, D Loss: 0.4938026536256075, G Loss: -0.9999960064888\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 71/516, D Loss: 0.49761551804840565, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 72/516, D Loss: 0.4997719049861189, G Loss: -0.9999955296516418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 73/516, D Loss: 0.49352619610726833, G Loss: -0.9999966025352478\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 74/516, D Loss: 0.4924227357842028, G Loss: -0.9999966025352478\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 75/516, D Loss: 0.4945904668420553, G Loss: -0.9999956488609314\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 76/516, D Loss: 0.48584121372550726, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 77/516, D Loss: 0.495917537715286, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 78/516, D Loss: 0.4993220677715726, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 79/516, D Loss: 0.49488347163423896, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 80/516, D Loss: 0.4951458452269435, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 81/516, D Loss: 0.4927072813734412, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 82/516, D Loss: 0.49476522020995617, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 83/516, D Loss: 0.4995656099054031, G Loss: -0.9999953508377075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 84/516, D Loss: 0.48603878915309906, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 85/516, D Loss: 0.48661129269748926, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 86/516, D Loss: 0.49882983590941876, G Loss: -0.9999955892562866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 87/516, D Loss: 0.4945115060545504, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 88/516, D Loss: 0.49948290089378133, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 89/516, D Loss: 0.4997364103619475, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 90/516, D Loss: 0.4874920602887869, G Loss: -0.9999971985816956\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 91/516, D Loss: 0.4947268362157047, G Loss: -0.999995768070221\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 92/516, D Loss: 0.4949898002669215, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 93/516, D Loss: 0.49942135938908905, G Loss: -0.9999961256980896\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 94/516, D Loss: 0.49380613397806883, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 95/516, D Loss: 0.498983335448429, G Loss: -0.9999971985816956\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 96/516, D Loss: 0.49465392576530576, G Loss: -0.9999963641166687\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 97/516, D Loss: 0.4953995319083333, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 98/516, D Loss: 0.4989094431512058, G Loss: -0.9999957084655762\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 99/516, D Loss: 0.49331021262332797, G Loss: -0.9999954700469971\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 100/516, D Loss: 0.4980438379570842, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 101/516, D Loss: 0.4818742498755455, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 102/516, D Loss: 0.49124209210276604, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 103/516, D Loss: 0.4857498677447438, G Loss: -0.9999962449073792\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 104/516, D Loss: 0.4992423227522522, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 105/516, D Loss: 0.4992579458630644, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 106/516, D Loss: 0.48706996720284224, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 107/516, D Loss: 0.49497056752443314, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 108/516, D Loss: 0.49519150936976075, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 109/516, D Loss: 0.490869365632534, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 110/516, D Loss: 0.4820411838591099, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 111/516, D Loss: 0.49583828914910555, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 112/516, D Loss: 0.49858913489151746, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 113/516, D Loss: 0.4828867092728615, G Loss: -0.9999956488609314\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 114/516, D Loss: 0.4993758417549543, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 115/516, D Loss: 0.4932507611811161, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 116/516, D Loss: 0.4983175710076466, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 117/516, D Loss: 0.4956637746654451, G Loss: -0.9999969601631165\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 118/516, D Loss: 0.49665205739438534, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 119/516, D Loss: 0.48808467015624046, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 120/516, D Loss: 0.4905031844973564, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 121/516, D Loss: 0.4918607138097286, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 122/516, D Loss: 0.4798895865678787, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 123/516, D Loss: 0.4893930209800601, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 124/516, D Loss: 0.4976556703913957, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 125/516, D Loss: 0.4975125198252499, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 126/516, D Loss: 0.4996318948979024, G Loss: -0.9999958872795105\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 127/516, D Loss: 0.4913909276947379, G Loss: -0.9999956488609314\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 128/516, D Loss: 0.4910309063270688, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 129/516, D Loss: 0.49668273353017867, G Loss: -0.9999967217445374\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 130/516, D Loss: 0.4906606962904334, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 131/516, D Loss: 0.487000560387969, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 132/516, D Loss: 0.49777070502750576, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 133/516, D Loss: 0.4906528666615486, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 134/516, D Loss: 0.49929841281846166, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 135/516, D Loss: 0.4919886225834489, G Loss: -0.9999968409538269\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 136/516, D Loss: 0.49597391206771135, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 137/516, D Loss: 0.4880613572895527, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 138/516, D Loss: 0.49835042376071215, G Loss: -0.9999961256980896\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 139/516, D Loss: 0.49798411480151117, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 140/516, D Loss: 0.49633121583610773, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 141/516, D Loss: 0.4970693467184901, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 142/516, D Loss: 0.49253632314503193, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 143/516, D Loss: 0.48697859048843384, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 144/516, D Loss: 0.49741093115881085, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 145/516, D Loss: 0.4950578920543194, G Loss: -0.9999964833259583\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 146/516, D Loss: 0.4972423892468214, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 147/516, D Loss: 0.4956358131021261, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 148/516, D Loss: 0.49892026628367603, G Loss: -0.9999949932098389\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 149/516, D Loss: 0.49869244266301394, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 150/516, D Loss: 0.4986215840326622, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 151/516, D Loss: 0.49860345921479166, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 152/516, D Loss: 0.4952075872570276, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 153/516, D Loss: 0.4991113483556546, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 154/516, D Loss: 0.49551525991410017, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 155/516, D Loss: 0.4938059258274734, G Loss: -0.9999963641166687\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 156/516, D Loss: 0.49780404660850763, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 157/516, D Loss: 0.49662708258256316, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 158/516, D Loss: 0.49848830548580736, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 159/516, D Loss: 0.49856432853266597, G Loss: -0.9999973177909851\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 160/516, D Loss: 0.4956603734754026, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 161/516, D Loss: 0.49142029508948326, G Loss: -0.9999973177909851\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 162/516, D Loss: 0.49571968289092183, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 163/516, D Loss: 0.49747382942587137, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 164/516, D Loss: 0.49823603383265436, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 165/516, D Loss: 0.4965528503526002, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 166/516, D Loss: 0.4842324610799551, G Loss: -0.999997079372406\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 167/516, D Loss: 0.4901013085618615, G Loss: -0.9999969601631165\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 168/516, D Loss: 0.49834941211156547, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 169/516, D Loss: 0.48935358598828316, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 170/516, D Loss: 0.49941863730782643, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 171/516, D Loss: 0.49888032977469265, G Loss: -0.9999962449073792\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 172/516, D Loss: 0.4975266130641103, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 173/516, D Loss: 0.4971564661245793, G Loss: -0.9999960660934448\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 174/516, D Loss: 0.498879601014778, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 175/516, D Loss: 0.4927136776968837, G Loss: -0.999996542930603\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 176/516, D Loss: 0.4879697151482105, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 177/516, D Loss: 0.49961384836933576, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 178/516, D Loss: 0.4951166729442775, G Loss: -0.9999973177909851\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 179/516, D Loss: 0.4861579732969403, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 180/516, D Loss: 0.4958155108615756, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 181/516, D Loss: 0.4951222147792578, G Loss: -0.9999961853027344\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 182/516, D Loss: 0.49988636330817826, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 183/516, D Loss: 0.4898794209584594, G Loss: -0.9999968409538269\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 184/516, D Loss: 0.4988686761353165, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 185/516, D Loss: 0.49475801596418023, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 186/516, D Loss: 0.497615969972685, G Loss: -0.9999968409538269\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 187/516, D Loss: 0.49909636890515685, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 188/516, D Loss: 0.4941017203964293, G Loss: -0.9999971985816956\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 189/516, D Loss: 0.49847541004419327, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 190/516, D Loss: 0.49898668308742344, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 191/516, D Loss: 0.48866196162998676, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 192/516, D Loss: 0.49829716072417796, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 193/516, D Loss: 0.4947051811031997, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 194/516, D Loss: 0.4958045771345496, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 195/516, D Loss: 0.49712524050846696, G Loss: -0.9999968409538269\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 196/516, D Loss: 0.4994058558368124, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 197/516, D Loss: 0.4945812704972923, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 198/516, D Loss: 0.49581221770495176, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 199/516, D Loss: 0.4944954765960574, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 200/516, D Loss: 0.4906515497714281, G Loss: -0.9999958276748657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 201/516, D Loss: 0.49936391442315653, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 202/516, D Loss: 0.486909668892622, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 203/516, D Loss: 0.4932008944451809, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 204/516, D Loss: 0.49268653662875295, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 205/516, D Loss: 0.4982564279343933, G Loss: -0.9999963045120239\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 206/516, D Loss: 0.49464810732752085, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 207/516, D Loss: 0.49751081387512386, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 208/516, D Loss: 0.4930990692228079, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 209/516, D Loss: 0.489003773778677, G Loss: -0.9999959468841553\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 210/516, D Loss: 0.49179536662995815, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 211/516, D Loss: 0.49850998423062265, G Loss: -0.9999973177909851\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 212/516, D Loss: 0.49897593236528337, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 213/516, D Loss: 0.49485269468277693, G Loss: -0.9999971985816956\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 214/516, D Loss: 0.4986071331659332, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 215/516, D Loss: 0.49908920319285244, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 216/516, D Loss: 0.4930164380930364, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 217/516, D Loss: 0.4941716524772346, G Loss: -0.9999966621398926\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 218/516, D Loss: 0.49827378732152283, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 219/516, D Loss: 0.49910317454487085, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 220/516, D Loss: 0.4834135677665472, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 221/516, D Loss: 0.4987538617569953, G Loss: -0.9999969005584717\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 222/516, D Loss: 0.49756917892955244, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 223/516, D Loss: 0.49551715003326535, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 224/516, D Loss: 0.49434660328552127, G Loss: -0.9999968409538269\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 225/516, D Loss: 0.4948770231567323, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 226/516, D Loss: 0.49571057595312595, G Loss: -0.9999967813491821\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 227/516, D Loss: 0.49897922528907657, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 228/516, D Loss: 0.4992193650105037, G Loss: -0.9999969601631165\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 229/516, D Loss: 0.4947294220328331, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 230/516, D Loss: 0.4956509810872376, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 231/516, D Loss: 0.49706687359139323, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 232/516, D Loss: 0.48777985386550426, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 233/516, D Loss: 0.4855445269495249, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 234/516, D Loss: 0.4907527593895793, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 235/516, D Loss: 0.4956816714257002, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 236/516, D Loss: 0.49458677414804697, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 237/516, D Loss: 0.4990589635563083, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 238/516, D Loss: 0.4977014921605587, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 239/516, D Loss: 0.4970754578243941, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 240/516, D Loss: 0.4944896060042083, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 241/516, D Loss: 0.4987042425200343, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 242/516, D Loss: 0.4993496674578637, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 243/516, D Loss: 0.49001584853976965, G Loss: -0.9999968409538269\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 244/516, D Loss: 0.49919049127493054, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 245/516, D Loss: 0.49598780693486333, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 246/516, D Loss: 0.4934518435038626, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 247/516, D Loss: 0.49913916748482734, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 248/516, D Loss: 0.4977891275193542, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 249/516, D Loss: 0.49398157838732004, G Loss: -0.9999971389770508\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 250/516, D Loss: 0.49578990833833814, G Loss: -0.9999966025352478\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 251/516, D Loss: 0.49882211070507765, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 252/516, D Loss: 0.4915862549096346, G Loss: -0.9999969601631165\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 253/516, D Loss: 0.4874286353588104, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 254/516, D Loss: 0.4906951058655977, G Loss: -0.9999967217445374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 255/516, D Loss: 0.49518000101670623, G Loss: -0.9999973177909851\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 256/516, D Loss: 0.49577260902151465, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 257/516, D Loss: 0.4951372630894184, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 258/516, D Loss: 0.4934157785028219, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 259/516, D Loss: 0.4987987605854869, G Loss: -0.9999964237213135\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 260/516, D Loss: 0.49030954390764236, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 261/516, D Loss: 0.49838573252782226, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 262/516, D Loss: 0.49487456772476435, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 263/516, D Loss: 0.48865734692662954, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 264/516, D Loss: 0.49032481759786606, G Loss: -0.9999967217445374\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 265/516, D Loss: 0.49128051847219467, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 266/516, D Loss: 0.4899669038131833, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 267/516, D Loss: 0.4984760463703424, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 268/516, D Loss: 0.49117670953273773, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 269/516, D Loss: 0.4873243719339371, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 270/516, D Loss: 0.4892775621265173, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 271/516, D Loss: 0.4943362814374268, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 272/516, D Loss: 0.4949253033846617, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 273/516, D Loss: 0.4917270168662071, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 274/516, D Loss: 0.48610897548496723, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 275/516, D Loss: 0.49248206289485097, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 276/516, D Loss: 0.4987855408107862, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 277/516, D Loss: 0.4993343483656645, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 278/516, D Loss: 0.49551771115511656, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 279/516, D Loss: 0.49570888467133045, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 280/516, D Loss: 0.489392451941967, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 281/516, D Loss: 0.49945999908959493, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 282/516, D Loss: 0.4997944144997746, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 283/516, D Loss: 0.49523334624245763, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 284/516, D Loss: 0.49624128360301256, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 285/516, D Loss: 0.4978858840186149, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 286/516, D Loss: 0.4934013136662543, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 287/516, D Loss: 0.49515761947259307, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 288/516, D Loss: 0.4920489080250263, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 289/516, D Loss: 0.4984866942977533, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 290/516, D Loss: 0.49565368657931685, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 291/516, D Loss: 0.4942131722345948, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 292/516, D Loss: 0.49487927835434675, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 293/516, D Loss: 0.49129807762801647, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 294/516, D Loss: 0.4954096581786871, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 295/516, D Loss: 0.49176693987101316, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 296/516, D Loss: 0.49905850359937176, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 297/516, D Loss: 0.4956377004273236, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 298/516, D Loss: 0.49881315464153886, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 299/516, D Loss: 0.47958245500922203, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 300/516, D Loss: 0.49124668072909117, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 301/516, D Loss: 0.48732211254537106, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 302/516, D Loss: 0.4969923603348434, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 303/516, D Loss: 0.49143999069929123, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 304/516, D Loss: 0.49470045045018196, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 305/516, D Loss: 0.487134026363492, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 306/516, D Loss: 0.4884882792830467, G Loss: -0.9999970197677612\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 307/516, D Loss: 0.491883029229939, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 308/516, D Loss: 0.4935523336753249, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 309/516, D Loss: 0.4996520671702456, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 310/516, D Loss: 0.4974006889387965, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 311/516, D Loss: 0.4972077568527311, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 312/516, D Loss: 0.4976031631231308, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 313/516, D Loss: 0.4928679075092077, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 314/516, D Loss: 0.4904408985748887, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 315/516, D Loss: 0.49285877123475075, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 316/516, D Loss: 0.49442049907520413, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 317/516, D Loss: 0.4866186250001192, G Loss: -0.9999976754188538\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 318/516, D Loss: 0.49546843115240335, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 319/516, D Loss: 0.49977402413787786, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 320/516, D Loss: 0.4914082270115614, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 321/516, D Loss: 0.4956728434190154, G Loss: -0.9999974370002747\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 322/516, D Loss: 0.4967599513474852, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 323/516, D Loss: 0.49532108986750245, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 324/516, D Loss: 0.48426077142357826, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 325/516, D Loss: 0.4995753385592252, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 326/516, D Loss: 0.49987708839034894, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 327/516, D Loss: 0.49112197291105986, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 328/516, D Loss: 0.4828536920249462, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 329/516, D Loss: 0.4874192588031292, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 330/516, D Loss: 0.49968846043339, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 331/516, D Loss: 0.49559463607147336, G Loss: -0.9999974966049194\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 332/516, D Loss: 0.49485955387353897, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 333/516, D Loss: 0.49755679769441485, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 334/516, D Loss: 0.49528894759714603, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 335/516, D Loss: 0.49876169487833977, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 336/516, D Loss: 0.49134968407452106, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 337/516, D Loss: 0.4752789158374071, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 338/516, D Loss: 0.49906131892930716, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 339/516, D Loss: 0.49585029296576977, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 340/516, D Loss: 0.4951457646675408, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 341/516, D Loss: 0.49405613727867603, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 342/516, D Loss: 0.49549970170482993, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 343/516, D Loss: 0.48936807829886675, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 344/516, D Loss: 0.4880256513133645, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 345/516, D Loss: 0.4913560440763831, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 346/516, D Loss: 0.4863964058458805, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 347/516, D Loss: 0.49786137137562037, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 348/516, D Loss: 0.49960984592325985, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 349/516, D Loss: 0.484868086874485, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 350/516, D Loss: 0.4956343360245228, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 351/516, D Loss: 0.4860881054773927, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 352/516, D Loss: 0.48761253617703915, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 353/516, D Loss: 0.49089332297444344, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 354/516, D Loss: 0.4860482234507799, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 355/516, D Loss: 0.49255717219784856, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 356/516, D Loss: 0.49269252456724644, G Loss: -0.9999977946281433\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 357/516, D Loss: 0.4991092301206663, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 358/516, D Loss: 0.4926917073316872, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 359/516, D Loss: 0.4993887528544292, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 360/516, D Loss: 0.4968495618086308, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 361/516, D Loss: 0.49553106306120753, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 362/516, D Loss: 0.49429236771538854, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 363/516, D Loss: 0.4990094192326069, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 364/516, D Loss: 0.49364595115184784, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 365/516, D Loss: 0.49174800515174866, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 366/516, D Loss: 0.49584183283150196, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 367/516, D Loss: 0.4893071372061968, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 368/516, D Loss: 0.4902803096920252, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 369/516, D Loss: 0.4886149652302265, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 370/516, D Loss: 0.49570884462445974, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 371/516, D Loss: 0.4836817141622305, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 372/516, D Loss: 0.49664139025844634, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 373/516, D Loss: 0.49442478688433766, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 374/516, D Loss: 0.49983808721299283, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 375/516, D Loss: 0.4988249719608575, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 376/516, D Loss: 0.4995265280304011, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 377/516, D Loss: 0.4943757844157517, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 378/516, D Loss: 0.4943487048149109, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 379/516, D Loss: 0.4952188800089061, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 380/516, D Loss: 0.4953604871407151, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 381/516, D Loss: 0.49954015511320904, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 382/516, D Loss: 0.4994435975677334, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 383/516, D Loss: 0.495153047144413, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 384/516, D Loss: 0.4920838363468647, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 385/516, D Loss: 0.48232223093509674, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 386/516, D Loss: 0.4979408551007509, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 387/516, D Loss: 0.49974076126818545, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 388/516, D Loss: 0.4966086207423359, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 389/516, D Loss: 0.49653684068471193, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 390/516, D Loss: 0.4988850516965613, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 391/516, D Loss: 0.48017048835754395, G Loss: -0.9999975562095642\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 392/516, D Loss: 0.4992902355734259, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 393/516, D Loss: 0.49490719893947244, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 394/516, D Loss: 0.49484068946912885, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 395/516, D Loss: 0.49219428189098835, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 396/516, D Loss: 0.49945309123722836, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 397/516, D Loss: 0.4875877108424902, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 398/516, D Loss: 0.494004069827497, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 399/516, D Loss: 0.498137318296358, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 400/516, D Loss: 0.49191159661859274, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 401/516, D Loss: 0.49817016918677837, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 402/516, D Loss: 0.49404551461338997, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 403/516, D Loss: 0.49576249113306403, G Loss: -0.9999973773956299\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 404/516, D Loss: 0.4965979640837759, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 405/516, D Loss: 0.49908098112791777, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 406/516, D Loss: 0.49185808282345533, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 407/516, D Loss: 0.4917915128171444, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 408/516, D Loss: 0.4955338458530605, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 409/516, D Loss: 0.4915181649848819, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 410/516, D Loss: 0.49736522627063096, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 411/516, D Loss: 0.4983948366716504, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 412/516, D Loss: 0.4987876833183691, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 413/516, D Loss: 0.49205461982637644, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 414/516, D Loss: 0.49794367933645844, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 415/516, D Loss: 0.49565583374351263, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 416/516, D Loss: 0.4952533692121506, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 417/516, D Loss: 0.49562310986220837, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 418/516, D Loss: 0.49106514640152454, G Loss: -0.9999977350234985\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 419/516, D Loss: 0.49370358511805534, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 420/516, D Loss: 0.49502429692074656, G Loss: -0.9999967217445374\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 421/516, D Loss: 0.4943956034258008, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 422/516, D Loss: 0.4872873108834028, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 423/516, D Loss: 0.49793570465408266, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 424/516, D Loss: 0.49372662464156747, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 425/516, D Loss: 0.4989638744154945, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 426/516, D Loss: 0.4956910414621234, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 427/516, D Loss: 0.49633453926071525, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 428/516, D Loss: 0.49577387049794197, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 429/516, D Loss: 0.49460004083812237, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 430/516, D Loss: 0.4950443897396326, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 431/516, D Loss: 0.49229490105062723, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 432/516, D Loss: 0.49869315896648914, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 433/516, D Loss: 0.4974121507257223, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 434/516, D Loss: 0.4865037063136697, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 435/516, D Loss: 0.49561897851526737, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 436/516, D Loss: 0.49239133950322866, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 437/516, D Loss: 0.49638926750048995, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 438/516, D Loss: 0.4919367339462042, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 439/516, D Loss: 0.48736259154975414, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 440/516, D Loss: 0.4943636446259916, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 441/516, D Loss: 0.4955312581732869, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 442/516, D Loss: 0.4938414515927434, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 443/516, D Loss: 0.49491115752607584, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 444/516, D Loss: 0.49944935634266585, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 445/516, D Loss: 0.4972194458823651, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 446/516, D Loss: 0.4908446380868554, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 447/516, D Loss: 0.4918088587000966, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 448/516, D Loss: 0.4990921237040311, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 449/516, D Loss: 0.4957051631063223, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 450/516, D Loss: 0.49760968098416924, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 451/516, D Loss: 0.4948976207524538, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 452/516, D Loss: 0.4997610984573839, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 453/516, D Loss: 0.49891774472780526, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 3, Batch 454/516, D Loss: 0.4995074275066145, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 455/516, D Loss: 0.48804096691310406, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 456/516, D Loss: 0.4946814081631601, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 457/516, D Loss: 0.4906746670603752, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 458/516, D Loss: 0.4934740294702351, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 459/516, D Loss: 0.4969452116638422, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 460/516, D Loss: 0.4987861594418064, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 461/516, D Loss: 0.4924126863479614, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 462/516, D Loss: 0.49845028086565435, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 463/516, D Loss: 0.49390015937387943, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 464/516, D Loss: 0.49759868159890175, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 465/516, D Loss: 0.4921104731038213, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 466/516, D Loss: 0.4904612433165312, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 3, Batch 467/516, D Loss: 0.4950324548408389, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 468/516, D Loss: 0.499180925602559, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 469/516, D Loss: 0.49819712981116027, G Loss: -0.9999978542327881\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 470/516, D Loss: 0.4928669077344239, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 471/516, D Loss: 0.4945117458701134, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 472/516, D Loss: 0.49822223593946546, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 473/516, D Loss: 0.49557986203581095, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 474/516, D Loss: 0.49926489597419277, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 475/516, D Loss: 0.49176665488630533, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 476/516, D Loss: 0.48671517986804247, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 477/516, D Loss: 0.4952929178252816, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 478/516, D Loss: 0.4910931047052145, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 479/516, D Loss: 0.4957445543259382, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 480/516, D Loss: 0.4875301122665405, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 481/516, D Loss: 0.49517413321882486, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 482/516, D Loss: 0.4893572460860014, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 483/516, D Loss: 0.49953990266658366, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 484/516, D Loss: 0.4994775706436485, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 485/516, D Loss: 0.4991084521752782, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 486/516, D Loss: 0.4995476351468824, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 487/516, D Loss: 0.4956437097862363, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 488/516, D Loss: 0.4940113229677081, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 489/516, D Loss: 0.4973029587417841, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 490/516, D Loss: 0.4934964128769934, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 491/516, D Loss: 0.49885016633197665, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 492/516, D Loss: 0.49499455094337463, G Loss: -0.999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 493/516, D Loss: 0.49572868878021836, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 494/516, D Loss: 0.49985259817913175, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 495/516, D Loss: 0.4916226156055927, G Loss: -0.9999979734420776\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 496/516, D Loss: 0.4876602366566658, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 497/516, D Loss: 0.4982698116218671, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 498/516, D Loss: 0.49744051741436124, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 499/516, D Loss: 0.4949172455817461, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 500/516, D Loss: 0.49307029554620385, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 501/516, D Loss: 0.4985166505211964, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 502/516, D Loss: 0.4972081985324621, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 503/516, D Loss: 0.4852147102355957, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 504/516, D Loss: 0.4985647432040423, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 505/516, D Loss: 0.49476618506014347, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 506/516, D Loss: 0.49293855763971806, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 3, Batch 507/516, D Loss: 0.49158670380711555, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 508/516, D Loss: 0.4914410188794136, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 509/516, D Loss: 0.49880660756025463, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 510/516, D Loss: 0.4904899839311838, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 511/516, D Loss: 0.493776036426425, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 512/516, D Loss: 0.49852221505716443, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 513/516, D Loss: 0.49131784681230783, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 514/516, D Loss: 0.49568127701058984, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 515/516, D Loss: 0.4945730767212808, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 3, Batch 516/516, D Loss: 0.49513101298362017, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 1/516, D Loss: 0.4931638780981302, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 2/516, D Loss: 0.4936733888462186, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 3/516, D Loss: 0.49485031235963106, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 4/516, D Loss: 0.494794980622828, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 5/516, D Loss: 0.4932923917658627, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 6/516, D Loss: 0.4921119026839733, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 7/516, D Loss: 0.49102081079036, G Loss: -0.9999979138374329\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 8/516, D Loss: 0.4957863362506032, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 9/516, D Loss: 0.4954077987931669, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 10/516, D Loss: 0.4913648422807455, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 11/516, D Loss: 0.4905506446957588, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 12/516, D Loss: 0.48406281508505344, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 13/516, D Loss: 0.4995997106889263, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 14/516, D Loss: 0.49689757521264255, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 15/516, D Loss: 0.4921334907412529, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 16/516, D Loss: 0.49380255909636617, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 17/516, D Loss: 0.49153364822268486, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 18/516, D Loss: 0.4888569489121437, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 19/516, D Loss: 0.49351143138483167, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 20/516, D Loss: 0.48576186038553715, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 21/516, D Loss: 0.4945151205174625, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 22/516, D Loss: 0.49500274332240224, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 23/516, D Loss: 0.4992075900081545, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 24/516, D Loss: 0.48044922202825546, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 25/516, D Loss: 0.49742987379431725, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 26/516, D Loss: 0.4938760884106159, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 27/516, D Loss: 0.486226643435657, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 28/516, D Loss: 0.4989690833026543, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 29/516, D Loss: 0.4993255498702638, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 30/516, D Loss: 0.4977681974414736, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 31/516, D Loss: 0.49657927569933236, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 32/516, D Loss: 0.49144046381115913, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 33/516, D Loss: 0.49497442319989204, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 34/516, D Loss: 0.4932183474302292, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 35/516, D Loss: 0.49143029656261206, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 36/516, D Loss: 0.498997071175836, G Loss: -0.9999982118606567\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 37/516, D Loss: 0.4983210957143456, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 38/516, D Loss: 0.4992077932693064, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 39/516, D Loss: 0.49484901409596205, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 40/516, D Loss: 0.49866958742495626, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 41/516, D Loss: 0.4976848813239485, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 42/516, D Loss: 0.4946523937396705, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 43/516, D Loss: 0.49535275204107165, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 44/516, D Loss: 0.4955584239214659, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 45/516, D Loss: 0.49851256143301725, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 46/516, D Loss: 0.4975752590689808, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 47/516, D Loss: 0.4992083964170888, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 48/516, D Loss: 0.49629205046221614, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 49/516, D Loss: 0.4944626046344638, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 50/516, D Loss: 0.49320924561470747, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 51/516, D Loss: 0.49518230417743325, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 52/516, D Loss: 0.4995204171864316, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 53/516, D Loss: 0.4913852633908391, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 54/516, D Loss: 0.4948083469644189, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 55/516, D Loss: 0.49927810253575444, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 56/516, D Loss: 0.4885156862437725, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 57/516, D Loss: 0.49764180672354996, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 58/516, D Loss: 0.49386273324489594, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 59/516, D Loss: 0.4987718827323988, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 60/516, D Loss: 0.49596702214330435, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 61/516, D Loss: 0.49778111604973674, G Loss: -0.9999985098838806\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 62/516, D Loss: 0.49533758498728275, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 63/516, D Loss: 0.49581363145262003, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 64/516, D Loss: 0.4966735129710287, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 65/516, D Loss: 0.49875970650464296, G Loss: -0.999998152256012\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 66/516, D Loss: 0.498074350762181, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 67/516, D Loss: 0.49103029631078243, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 68/516, D Loss: 0.4948724522255361, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 69/516, D Loss: 0.49086809903383255, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 70/516, D Loss: 0.48356989212334156, G Loss: -0.9999982714653015\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 71/516, D Loss: 0.4956033122725785, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 72/516, D Loss: 0.4987824155250564, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 73/516, D Loss: 0.49363308399915695, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 74/516, D Loss: 0.4896933492273092, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 75/516, D Loss: 0.491053213365376, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 76/516, D Loss: 0.4965596771799028, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 77/516, D Loss: 0.4982002249453217, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 78/516, D Loss: 0.4901754707098007, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 79/516, D Loss: 0.4938274100422859, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 80/516, D Loss: 0.49853746220469475, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 81/516, D Loss: 0.49530196422711015, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 82/516, D Loss: 0.49905328603927046, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 83/516, D Loss: 0.49962275894358754, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 84/516, D Loss: 0.4847346004098654, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 85/516, D Loss: 0.49336196947842836, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 86/516, D Loss: 0.49163611605763435, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 87/516, D Loss: 0.49787475750781596, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 88/516, D Loss: 0.4997240025550127, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 89/516, D Loss: 0.491627118550241, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 90/516, D Loss: 0.49519738741219044, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 91/516, D Loss: 0.4949256144464016, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 92/516, D Loss: 0.4974798725452274, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 93/516, D Loss: 0.495175676420331, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 94/516, D Loss: 0.4954962581396103, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 95/516, D Loss: 0.4954125015065074, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 96/516, D Loss: 0.4977922886610031, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 97/516, D Loss: 0.4836764372885227, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 98/516, D Loss: 0.4991271552280523, G Loss: -0.9999983310699463\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 99/516, D Loss: 0.49428635742515326, G Loss: -0.9999980926513672\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 100/516, D Loss: 0.49921790446387604, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 101/516, D Loss: 0.4971047861035913, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 102/516, D Loss: 0.4931775168515742, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 103/516, D Loss: 0.4932799148373306, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 104/516, D Loss: 0.49293147306889296, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 105/516, D Loss: 0.49896888399962336, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 106/516, D Loss: 0.4934544004499912, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 107/516, D Loss: 0.49853042664472014, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 108/516, D Loss: 0.4934048498980701, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 109/516, D Loss: 0.49775864044204354, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 110/516, D Loss: 0.49386794213205576, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 111/516, D Loss: 0.49482814176008105, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 112/516, D Loss: 0.4911034032702446, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 113/516, D Loss: 0.499441267515067, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 114/516, D Loss: 0.49873929587192833, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 115/516, D Loss: 0.49444909347221255, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 116/516, D Loss: 0.49493355117738247, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 117/516, D Loss: 0.4987469451734796, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 118/516, D Loss: 0.49089083448052406, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 119/516, D Loss: 0.49439611565321684, G Loss: -0.9999980330467224\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 120/516, D Loss: 0.49870705557987094, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 121/516, D Loss: 0.49528508726507425, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 122/516, D Loss: 0.488292982801795, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 123/516, D Loss: 0.49796258681453764, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 124/516, D Loss: 0.49371346458792686, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 125/516, D Loss: 0.4907816518098116, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 126/516, D Loss: 0.498519221553579, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 127/516, D Loss: 0.4816309493035078, G Loss: -0.9999972581863403\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 128/516, D Loss: 0.4953508782200515, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 129/516, D Loss: 0.4844842664897442, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 130/516, D Loss: 0.4939901167526841, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 131/516, D Loss: 0.4855165183544159, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 132/516, D Loss: 0.4964711838401854, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 133/516, D Loss: 0.49876693333499134, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 134/516, D Loss: 0.4995702846790664, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 135/516, D Loss: 0.4920362289994955, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 136/516, D Loss: 0.49921701988205314, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 137/516, D Loss: 0.49365785997360945, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 138/516, D Loss: 0.49562174547463655, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 139/516, D Loss: 0.48803569562733173, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 140/516, D Loss: 0.49954740126850083, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 141/516, D Loss: 0.49906890327110887, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 142/516, D Loss: 0.4991238453076221, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 143/516, D Loss: 0.4952872027643025, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 144/516, D Loss: 0.4888398852199316, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 145/516, D Loss: 0.4977341233752668, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 146/516, D Loss: 0.4951451336964965, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 147/516, D Loss: 0.4889919376000762, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 148/516, D Loss: 0.4975920694414526, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 149/516, D Loss: 0.4889770448207855, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 150/516, D Loss: 0.49904904898721725, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 151/516, D Loss: 0.4891211846843362, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 152/516, D Loss: 0.49577998323366046, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 153/516, D Loss: 0.49175163824111223, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 154/516, D Loss: 0.49571188213303685, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 155/516, D Loss: 0.4955549039877951, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 156/516, D Loss: 0.4972415524534881, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 157/516, D Loss: 0.4984613978303969, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 158/516, D Loss: 0.48779358249157667, G Loss: -0.9999985694885254\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 159/516, D Loss: 0.4935608352534473, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 160/516, D Loss: 0.49430240457877517, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 161/516, D Loss: 0.493393556214869, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 162/516, D Loss: 0.48970398772507906, G Loss: -0.9999983906745911\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 163/516, D Loss: 0.49152379017323256, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 164/516, D Loss: 0.4954061461612582, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 165/516, D Loss: 0.49168083630502224, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 166/516, D Loss: 0.48906077817082405, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 167/516, D Loss: 0.4949582004919648, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 168/516, D Loss: 0.4950704211369157, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 169/516, D Loss: 0.4855802971869707, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 170/516, D Loss: 0.49826220818795264, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 171/516, D Loss: 0.49816493142861873, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 172/516, D Loss: 0.4992203926667571, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 173/516, D Loss: 0.49376616533845663, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 174/516, D Loss: 0.4915775787085295, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 175/516, D Loss: 0.4970415849238634, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 176/516, D Loss: 0.49966019130079076, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 177/516, D Loss: 0.4858338478952646, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 178/516, D Loss: 0.4850585162639618, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 179/516, D Loss: 0.49105662293732166, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 180/516, D Loss: 0.4955272092483938, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 181/516, D Loss: 0.4996495163650252, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 182/516, D Loss: 0.49264081567525864, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 183/516, D Loss: 0.49886026966851205, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 184/516, D Loss: 0.49103296641260386, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 185/516, D Loss: 0.4946626559831202, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 186/516, D Loss: 0.48702723532915115, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 187/516, D Loss: 0.4903114102780819, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 188/516, D Loss: 0.4941647737286985, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 189/516, D Loss: 0.4909638101235032, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 190/516, D Loss: 0.4817755836993456, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 191/516, D Loss: 0.49198101088404655, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 192/516, D Loss: 0.4909934354946017, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 193/516, D Loss: 0.495838831178844, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 194/516, D Loss: 0.494739752728492, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 195/516, D Loss: 0.4924058145843446, G Loss: -0.9999986290931702\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 196/516, D Loss: 0.4958525416441262, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 197/516, D Loss: 0.4957826677709818, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 198/516, D Loss: 0.49044770933687687, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 199/516, D Loss: 0.48660791106522083, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 200/516, D Loss: 0.4991502520861104, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 4, Batch 201/516, D Loss: 0.4969033303204924, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 202/516, D Loss: 0.49590333364903927, G Loss: -0.9999984502792358\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 203/516, D Loss: 0.4969778396189213, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 204/516, D Loss: 0.4945845855399966, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 205/516, D Loss: 0.48930895514786243, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 206/516, D Loss: 0.4954368080943823, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 207/516, D Loss: 0.4952164562419057, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 208/516, D Loss: 0.4970577124040574, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 209/516, D Loss: 0.4938690192066133, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 210/516, D Loss: 0.4909935984760523, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 211/516, D Loss: 0.49788728985004127, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 212/516, D Loss: 0.4963667818810791, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 213/516, D Loss: 0.4936728202737868, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 214/516, D Loss: 0.49557715002447367, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 215/516, D Loss: 0.4901055861264467, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 216/516, D Loss: 0.49147823825478554, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 217/516, D Loss: 0.4988947707461193, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 218/516, D Loss: 0.4978928316850215, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 219/516, D Loss: 0.4982587901176885, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 220/516, D Loss: 0.49253347516059875, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 221/516, D Loss: 0.499807960994076, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 222/516, D Loss: 0.49492002883926034, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 223/516, D Loss: 0.4852629555389285, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 224/516, D Loss: 0.4888995150104165, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 225/516, D Loss: 0.49311092495918274, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 226/516, D Loss: 0.494216556660831, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 227/516, D Loss: 0.4953058739192784, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 228/516, D Loss: 0.49965421232627705, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 229/516, D Loss: 0.49112923070788383, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 230/516, D Loss: 0.49572756281122565, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 231/516, D Loss: 0.4989293720573187, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 232/516, D Loss: 0.49192739091813564, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 233/516, D Loss: 0.4987650029361248, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 234/516, D Loss: 0.4877767777070403, G Loss: -0.9999986886978149\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 235/516, D Loss: 0.49946845817612484, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 236/516, D Loss: 0.4953503692522645, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 237/516, D Loss: 0.49885925033595413, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 238/516, D Loss: 0.49864494835492224, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 239/516, D Loss: 0.4950131829828024, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 240/516, D Loss: 0.4944146443158388, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 241/516, D Loss: 0.4998492780432571, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 242/516, D Loss: 0.4932850790210068, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 243/516, D Loss: 0.49427017383277416, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 244/516, D Loss: 0.4973484007641673, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 245/516, D Loss: 0.4950833395123482, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 246/516, D Loss: 0.4957779487594962, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 247/516, D Loss: 0.49894736614078283, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 248/516, D Loss: 0.4985203293617815, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 249/516, D Loss: 0.49424456525593996, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 250/516, D Loss: 0.4955552062019706, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 251/516, D Loss: 0.4996306396496948, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 252/516, D Loss: 0.49014357291162014, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 253/516, D Loss: 0.49483476765453815, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 254/516, D Loss: 0.4990782617824152, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 255/516, D Loss: 0.4949147352017462, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 256/516, D Loss: 0.4984058877453208, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 257/516, D Loss: 0.49520886735990644, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 258/516, D Loss: 0.48507688753306866, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 259/516, D Loss: 0.49599530175328255, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 260/516, D Loss: 0.4868264440447092, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 261/516, D Loss: 0.4962692563422024, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 262/516, D Loss: 0.4988974314182997, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 263/516, D Loss: 0.4925764533691108, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 264/516, D Loss: 0.49874670431017876, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 265/516, D Loss: 0.4956218861043453, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 266/516, D Loss: 0.49083998426795006, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 267/516, D Loss: 0.4990647860104218, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 268/516, D Loss: 0.4955191765911877, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 269/516, D Loss: 0.49929675075691193, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 270/516, D Loss: 0.4956684159114957, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 271/516, D Loss: 0.4833744764328003, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 272/516, D Loss: 0.4984644640935585, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 273/516, D Loss: 0.48716336861252785, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 274/516, D Loss: 0.48895366955548525, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 275/516, D Loss: 0.4917378118261695, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 276/516, D Loss: 0.4976122269872576, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 277/516, D Loss: 0.49444964807480574, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 278/516, D Loss: 0.4908467587083578, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 279/516, D Loss: 0.48844881262630224, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 280/516, D Loss: 0.4951865114271641, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 281/516, D Loss: 0.4989650041097775, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 282/516, D Loss: 0.4935705466195941, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 283/516, D Loss: 0.49509697081521153, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 284/516, D Loss: 0.4860977753996849, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 285/516, D Loss: 0.48347351141273975, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 286/516, D Loss: 0.49134000577032566, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 287/516, D Loss: 0.4994657121715136, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 288/516, D Loss: 0.49823819287121296, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 289/516, D Loss: 0.48784091882407665, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 290/516, D Loss: 0.4932501404546201, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 291/516, D Loss: 0.4931332105770707, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 292/516, D Loss: 0.4945234898477793, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 293/516, D Loss: 0.4883934762328863, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 294/516, D Loss: 0.49910608935169876, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 295/516, D Loss: 0.4979027225635946, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 296/516, D Loss: 0.49318859446793795, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 297/516, D Loss: 0.4995459788478911, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 298/516, D Loss: 0.49169660080224276, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 299/516, D Loss: 0.49377491418272257, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 300/516, D Loss: 0.49244539625942707, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 301/516, D Loss: 0.49112355802208185, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 302/516, D Loss: 0.49061157554388046, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 303/516, D Loss: 0.49041261710226536, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 304/516, D Loss: 0.49398950207978487, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 305/516, D Loss: 0.4939557337202132, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 306/516, D Loss: 0.4980242904275656, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 307/516, D Loss: 0.49880433653015643, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 308/516, D Loss: 0.49485044553875923, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 309/516, D Loss: 0.48622305411845446, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 310/516, D Loss: 0.4904103958979249, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 311/516, D Loss: 0.4939807644113898, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 312/516, D Loss: 0.4959219880402088, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 313/516, D Loss: 0.4989498107461259, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 314/516, D Loss: 0.49401703057810664, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 315/516, D Loss: 0.4944731341674924, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 316/516, D Loss: 0.49815242155455053, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 317/516, D Loss: 0.49322766764089465, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 318/516, D Loss: 0.49332603020593524, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 319/516, D Loss: 0.49775227322243154, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 320/516, D Loss: 0.4928245432674885, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 321/516, D Loss: 0.49458792712539434, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 322/516, D Loss: 0.4882511356845498, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 323/516, D Loss: 0.4953697775490582, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 324/516, D Loss: 0.48877525608986616, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 325/516, D Loss: 0.48541309498250484, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 326/516, D Loss: 0.498451883206144, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 327/516, D Loss: 0.49779069842770696, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 328/516, D Loss: 0.49091724772006273, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 329/516, D Loss: 0.4984912258805707, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 330/516, D Loss: 0.4948676326312125, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 331/516, D Loss: 0.49572879495099187, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 332/516, D Loss: 0.49515713611617684, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 333/516, D Loss: 0.496103122131899, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 334/516, D Loss: 0.4949889359995723, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 335/516, D Loss: 0.49407226871699095, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 336/516, D Loss: 0.4989175577647984, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 337/516, D Loss: 0.493456756696105, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 338/516, D Loss: 0.492497980594635, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 339/516, D Loss: 0.4947908353060484, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 340/516, D Loss: 0.4949319278821349, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 341/516, D Loss: 0.49560292763635516, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 342/516, D Loss: 0.49257777258753777, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 343/516, D Loss: 0.49959514202782884, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 344/516, D Loss: 0.49287475319579244, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 345/516, D Loss: 0.49485971592366695, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 346/516, D Loss: 0.4917454179376364, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 347/516, D Loss: 0.49200599640607834, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 348/516, D Loss: 0.4909611288458109, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 349/516, D Loss: 0.49146566446870565, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 350/516, D Loss: 0.49104514718055725, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 351/516, D Loss: 0.49955430850968696, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 352/516, D Loss: 0.4953152956441045, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 353/516, D Loss: 0.4952378086745739, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 354/516, D Loss: 0.49409858975559473, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 355/516, D Loss: 0.4993705161032267, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 356/516, D Loss: 0.48740726709365845, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 357/516, D Loss: 0.4903833791613579, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 358/516, D Loss: 0.490952605381608, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 4, Batch 359/516, D Loss: 0.4905374478548765, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 360/516, D Loss: 0.49455504305660725, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 361/516, D Loss: 0.49109594244509935, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 362/516, D Loss: 0.49520717887207866, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 363/516, D Loss: 0.49789001303724945, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 364/516, D Loss: 0.499272744753398, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 365/516, D Loss: 0.49934094143100083, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 366/516, D Loss: 0.4911475796252489, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 367/516, D Loss: 0.4992323498008773, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 368/516, D Loss: 0.4941281881183386, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 369/516, D Loss: 0.49469534819945693, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 370/516, D Loss: 0.4996075989911333, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 371/516, D Loss: 0.4923267774283886, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 372/516, D Loss: 0.49174446146935225, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 373/516, D Loss: 0.4943468878045678, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 374/516, D Loss: 0.4977067031431943, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 375/516, D Loss: 0.4876405019313097, G Loss: -0.9999987483024597\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 376/516, D Loss: 0.4864089172333479, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 377/516, D Loss: 0.4944374714978039, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 378/516, D Loss: 0.4899322958663106, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 379/516, D Loss: 0.49548023473471403, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 380/516, D Loss: 0.4915053239092231, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 381/516, D Loss: 0.49993169648223557, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 382/516, D Loss: 0.4952570511959493, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 383/516, D Loss: 0.49586398946121335, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 384/516, D Loss: 0.4970016044098884, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 385/516, D Loss: 0.4961469518020749, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 386/516, D Loss: 0.4912300268188119, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 387/516, D Loss: 0.4952767309732735, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 388/516, D Loss: 0.4923511347733438, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 389/516, D Loss: 0.4990315214381553, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 390/516, D Loss: 0.4857596131041646, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 391/516, D Loss: 0.4988386214245111, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 392/516, D Loss: 0.49172898195683956, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 393/516, D Loss: 0.47853269800543785, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 394/516, D Loss: 0.49493925366550684, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 395/516, D Loss: 0.494206833653152, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 396/516, D Loss: 0.4939517257735133, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 397/516, D Loss: 0.49718232778832316, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 398/516, D Loss: 0.4989529384765774, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 399/516, D Loss: 0.49791095964610577, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 400/516, D Loss: 0.4937733868137002, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 401/516, D Loss: 0.49043442867696285, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 402/516, D Loss: 0.4993685152148828, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 403/516, D Loss: 0.4985478159505874, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 404/516, D Loss: 0.49848034186288714, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 405/516, D Loss: 0.4972037128172815, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 406/516, D Loss: 0.49485934991389513, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 407/516, D Loss: 0.499324168311432, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 408/516, D Loss: 0.4998660107667092, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 409/516, D Loss: 0.4949126970022917, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 410/516, D Loss: 0.49849505769088864, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 411/516, D Loss: 0.49554091645404696, G Loss: -0.9999988675117493\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 412/516, D Loss: 0.4936955231241882, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 413/516, D Loss: 0.4945387076586485, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 414/516, D Loss: 0.4972425913438201, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 415/516, D Loss: 0.4933017957955599, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 416/516, D Loss: 0.49956085334997624, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 417/516, D Loss: 0.49219041084870696, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 418/516, D Loss: 0.49953641984029673, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 419/516, D Loss: 0.4923727600835264, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 420/516, D Loss: 0.49829676491208375, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 421/516, D Loss: 0.4957884871400893, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 422/516, D Loss: 0.483030304312706, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 423/516, D Loss: 0.49165566358715296, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 424/516, D Loss: 0.49833050766028464, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 425/516, D Loss: 0.4914763392880559, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 426/516, D Loss: 0.485706340521574, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 427/516, D Loss: 0.4901401661336422, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 428/516, D Loss: 0.49892005359288305, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 429/516, D Loss: 0.48850403632968664, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 430/516, D Loss: 0.49490222055464983, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 431/516, D Loss: 0.49355159793049097, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 432/516, D Loss: 0.495326547883451, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 433/516, D Loss: 0.4907914875075221, G Loss: -0.9999989867210388\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 434/516, D Loss: 0.49448916874825954, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 435/516, D Loss: 0.49104054644703865, G Loss: -0.999998927116394\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 436/516, D Loss: 0.4943518224172294, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 437/516, D Loss: 0.49165834579616785, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 438/516, D Loss: 0.484309708699584, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 439/516, D Loss: 0.48982600308954716, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 440/516, D Loss: 0.4917932916432619, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 441/516, D Loss: 0.49056051298975945, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 442/516, D Loss: 0.49931184406159446, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 443/516, D Loss: 0.4993003942654468, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 444/516, D Loss: 0.499572863249341, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 445/516, D Loss: 0.49982372134400066, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 446/516, D Loss: 0.499008507700637, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 447/516, D Loss: 0.4949784683994949, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 448/516, D Loss: 0.49387525115162134, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 449/516, D Loss: 0.4903105925768614, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 4, Batch 450/516, D Loss: 0.4897598596289754, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 451/516, D Loss: 0.49098513554781675, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 4, Batch 452/516, D Loss: 0.49673585616983473, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 453/516, D Loss: 0.49158060923218727, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 454/516, D Loss: 0.49042494781315327, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 455/516, D Loss: 0.49739405140280724, G Loss: -0.9999988079071045\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 456/516, D Loss: 0.492643766105175, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 457/516, D Loss: 0.49334691325202584, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 458/516, D Loss: 0.4947430780157447, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 459/516, D Loss: 0.48822664096951485, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 460/516, D Loss: 0.4969274455215782, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 461/516, D Loss: 0.49499967508018017, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 462/516, D Loss: 0.492290957365185, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 463/516, D Loss: 0.4940653359517455, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 464/516, D Loss: 0.4887751415371895, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 465/516, D Loss: 0.49967529880814254, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 466/516, D Loss: 0.4941061190329492, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 467/516, D Loss: 0.4847599044442177, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 468/516, D Loss: 0.4953177426941693, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 469/516, D Loss: 0.4880850501358509, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 470/516, D Loss: 0.4953094129450619, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 471/516, D Loss: 0.49114312045276165, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 472/516, D Loss: 0.4912756737321615, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 473/516, D Loss: 0.4918963052332401, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 474/516, D Loss: 0.4959632637910545, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 475/516, D Loss: 0.49568200623616576, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 476/516, D Loss: 0.48786760680377483, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 477/516, D Loss: 0.49155303090810776, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 478/516, D Loss: 0.49825330381281674, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 4, Batch 479/516, D Loss: 0.49144536070525646, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 480/516, D Loss: 0.49792316695675254, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 481/516, D Loss: 0.4987493642838672, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 482/516, D Loss: 0.49446855764836073, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 4, Batch 483/516, D Loss: 0.4999036215158412, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 484/516, D Loss: 0.4926606183871627, G Loss: -0.9999990463256836\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 485/516, D Loss: 0.49879760353360325, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 486/516, D Loss: 0.4795115850865841, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 487/516, D Loss: 0.4920574398711324, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 488/516, D Loss: 0.4910604627802968, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 489/516, D Loss: 0.4988055882276967, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 490/516, D Loss: 0.49932268902193755, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 491/516, D Loss: 0.4854632131755352, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 492/516, D Loss: 0.49559486377984285, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 493/516, D Loss: 0.4955948730930686, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 494/516, D Loss: 0.4898305833339691, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 495/516, D Loss: 0.49509961903095245, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 496/516, D Loss: 0.48781745601445436, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 497/516, D Loss: 0.4851383473724127, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 498/516, D Loss: 0.49458430195227265, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 499/516, D Loss: 0.4928753417916596, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 500/516, D Loss: 0.49551862152293324, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 501/516, D Loss: 0.4902216214686632, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 502/516, D Loss: 0.4977411339059472, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 503/516, D Loss: 0.49486339604482055, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 504/516, D Loss: 0.49093012884259224, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 505/516, D Loss: 0.4998048157140147, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 506/516, D Loss: 0.49058972112834454, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 507/516, D Loss: 0.4828891586512327, G Loss: -0.9999991059303284\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 508/516, D Loss: 0.4921824550256133, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 509/516, D Loss: 0.4966506150085479, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 510/516, D Loss: 0.4821118265390396, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 511/516, D Loss: 0.49544576555490494, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 512/516, D Loss: 0.4989662903826684, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 513/516, D Loss: 0.4866117686033249, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 514/516, D Loss: 0.4996259220642969, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 4, Batch 515/516, D Loss: 0.48895581252872944, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 4, Batch 516/516, D Loss: 0.49832946876995265, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 1/516, D Loss: 0.49673476559109986, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 2/516, D Loss: 0.4988906824728474, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 3/516, D Loss: 0.4976466200314462, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 4/516, D Loss: 0.4982518748147413, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 5/516, D Loss: 0.49559727823361754, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 6/516, D Loss: 0.4877633601427078, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 7/516, D Loss: 0.4984493711963296, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 8/516, D Loss: 0.4949575746431947, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 9/516, D Loss: 0.4988427881617099, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 10/516, D Loss: 0.49555416125804186, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 11/516, D Loss: 0.4935833504423499, G Loss: -0.9999992251396179\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 12/516, D Loss: 0.4998100998927839, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 13/516, D Loss: 0.4950127638876438, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 14/516, D Loss: 0.4962547360919416, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 15/516, D Loss: 0.4919736925512552, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 16/516, D Loss: 0.4914899468421936, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 17/516, D Loss: 0.4991362066939473, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 18/516, D Loss: 0.49474994372576475, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 19/516, D Loss: 0.4892779355868697, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 20/516, D Loss: 0.49976658361265436, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 21/516, D Loss: 0.48893107380717993, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 22/516, D Loss: 0.49843452079221606, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 23/516, D Loss: 0.4905023369938135, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 24/516, D Loss: 0.49239770881831646, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 25/516, D Loss: 0.49596368335187435, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 26/516, D Loss: 0.4890531748533249, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 27/516, D Loss: 0.4971217548009008, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 28/516, D Loss: 0.48699029069393873, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 29/516, D Loss: 0.4951443085446954, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 30/516, D Loss: 0.4916498139500618, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 31/516, D Loss: 0.49717177194543183, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 32/516, D Loss: 0.4884000467136502, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 33/516, D Loss: 0.48739514127373695, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 34/516, D Loss: 0.48963354527950287, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 35/516, D Loss: 0.48750828113406897, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 36/516, D Loss: 0.49276123754680157, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 37/516, D Loss: 0.487504156306386, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 38/516, D Loss: 0.49889976216945797, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 39/516, D Loss: 0.4838727321475744, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 40/516, D Loss: 0.489747891202569, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 41/516, D Loss: 0.49164596386253834, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 42/516, D Loss: 0.49559599021449685, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 43/516, D Loss: 0.49417406506836414, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 44/516, D Loss: 0.4939330932684243, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 45/516, D Loss: 0.4907346535474062, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 46/516, D Loss: 0.49453056463971734, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 47/516, D Loss: 0.4993189444649033, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 48/516, D Loss: 0.4957981128245592, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 49/516, D Loss: 0.4939777529798448, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 50/516, D Loss: 0.49469704320654273, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 51/516, D Loss: 0.4935816456563771, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 52/516, D Loss: 0.48370788991451263, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 53/516, D Loss: 0.4973613142501563, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 54/516, D Loss: 0.4992813441203907, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 55/516, D Loss: 0.49516663746908307, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 56/516, D Loss: 0.4931576047092676, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 57/516, D Loss: 0.49516944075003266, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 58/516, D Loss: 0.49073189217597246, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 59/516, D Loss: 0.4951978176832199, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 60/516, D Loss: 0.4965681368485093, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 61/516, D Loss: 0.48861232958734035, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 62/516, D Loss: 0.493485557846725, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 63/516, D Loss: 0.4912087535485625, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 64/516, D Loss: 0.4982954424340278, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 65/516, D Loss: 0.4957445338368416, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 66/516, D Loss: 0.4939413843676448, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 67/516, D Loss: 0.4993013085331768, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 68/516, D Loss: 0.4924698732793331, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 69/516, D Loss: 0.49512985022738576, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 70/516, D Loss: 0.49737677443772554, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 71/516, D Loss: 0.49358417838811874, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 72/516, D Loss: 0.49483419908210635, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 73/516, D Loss: 0.4978507375344634, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 74/516, D Loss: 0.49933657818473876, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 75/516, D Loss: 0.4984533856622875, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 76/516, D Loss: 0.4995685471512843, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 77/516, D Loss: 0.497745122294873, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 78/516, D Loss: 0.4955383096821606, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 79/516, D Loss: 0.4997822170553263, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 80/516, D Loss: 0.4975764842238277, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 81/516, D Loss: 0.4944057557731867, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 82/516, D Loss: 0.4975739405490458, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 83/516, D Loss: 0.4998865605011815, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 84/516, D Loss: 0.49163584411144257, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 85/516, D Loss: 0.4996932315407321, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 86/516, D Loss: 0.4954095548018813, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 87/516, D Loss: 0.4852718152105808, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 88/516, D Loss: 0.49598694453015924, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 89/516, D Loss: 0.4901036974042654, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 90/516, D Loss: 0.49496726877987385, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 91/516, D Loss: 0.49797057593241334, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 92/516, D Loss: 0.49857700732536614, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 93/516, D Loss: 0.4987001328263432, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 94/516, D Loss: 0.4918213002383709, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 95/516, D Loss: 0.4976530196145177, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 96/516, D Loss: 0.4953402243554592, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 97/516, D Loss: 0.4988023911137134, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 98/516, D Loss: 0.49868285027332604, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 99/516, D Loss: 0.4941682848148048, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 100/516, D Loss: 0.49132382217794657, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 101/516, D Loss: 0.4975575746502727, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 102/516, D Loss: 0.4939514882862568, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 103/516, D Loss: 0.49909806763753295, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 104/516, D Loss: 0.4957633507438004, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 105/516, D Loss: 0.4934246586635709, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 106/516, D Loss: 0.48443463537842035, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 107/516, D Loss: 0.4997168893169146, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 108/516, D Loss: 0.49909221014240757, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 109/516, D Loss: 0.4947784640826285, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 110/516, D Loss: 0.4997578476322815, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 111/516, D Loss: 0.49218722991645336, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 112/516, D Loss: 0.495550193823874, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 5, Batch 113/516, D Loss: 0.4934339397586882, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 114/516, D Loss: 0.49418655317276716, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 115/516, D Loss: 0.4975850365590304, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 116/516, D Loss: 0.4969974821433425, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 117/516, D Loss: 0.4863404808565974, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 118/516, D Loss: 0.49387076823040843, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 119/516, D Loss: 0.497732910560444, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 120/516, D Loss: 0.495746458414942, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 121/516, D Loss: 0.49915307358605787, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 122/516, D Loss: 0.49916647968348116, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 123/516, D Loss: 0.4896084377542138, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 124/516, D Loss: 0.4935438307002187, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 125/516, D Loss: 0.4891323335468769, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 126/516, D Loss: 0.4953343807719648, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 127/516, D Loss: 0.494661470875144, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 128/516, D Loss: 0.4786536078900099, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 129/516, D Loss: 0.4982180144870654, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 130/516, D Loss: 0.4926874223165214, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 131/516, D Loss: 0.4921648744493723, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 132/516, D Loss: 0.4999019453243818, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 133/516, D Loss: 0.4941401034593582, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 134/516, D Loss: 0.4908828800544143, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 135/516, D Loss: 0.49929038796108216, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 136/516, D Loss: 0.4947147401981056, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 137/516, D Loss: 0.4939869223162532, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 138/516, D Loss: 0.4987131397938356, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 139/516, D Loss: 0.4909696448594332, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 140/516, D Loss: 0.492961747571826, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 141/516, D Loss: 0.4997348651231732, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 142/516, D Loss: 0.4911028500646353, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 143/516, D Loss: 0.49465891579166055, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 144/516, D Loss: 0.49973571562441066, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 145/516, D Loss: 0.4946832191199064, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 146/516, D Loss: 0.4868217911571264, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 147/516, D Loss: 0.4959237542934716, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 148/516, D Loss: 0.4808623939752579, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 149/516, D Loss: 0.48643500730395317, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 150/516, D Loss: 0.49926774005871266, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 151/516, D Loss: 0.4988780930871144, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 152/516, D Loss: 0.4945130222477019, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 153/516, D Loss: 0.4901324901729822, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 154/516, D Loss: 0.48915214091539383, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 155/516, D Loss: 0.49426026083528996, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 156/516, D Loss: 0.4860636405646801, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 157/516, D Loss: 0.49691214971244335, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 158/516, D Loss: 0.4923781347461045, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 159/516, D Loss: 0.49872523348312825, G Loss: -0.9999993443489075\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 160/516, D Loss: 0.49588692933321, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 161/516, D Loss: 0.4903402077034116, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 162/516, D Loss: 0.4942131428979337, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 163/516, D Loss: 0.49871251778677106, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 164/516, D Loss: 0.49500974733382463, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 165/516, D Loss: 0.489238616079092, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 166/516, D Loss: 0.4937825077213347, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 167/516, D Loss: 0.4979329810012132, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 168/516, D Loss: 0.4896211475133896, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 169/516, D Loss: 0.493922249879688, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 170/516, D Loss: 0.4971817219629884, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 171/516, D Loss: 0.49571327585726976, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 172/516, D Loss: 0.49496956495568156, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 173/516, D Loss: 0.4907166324555874, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 174/516, D Loss: 0.4940348737873137, G Loss: -0.9999991655349731\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 175/516, D Loss: 0.49969910411164165, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 176/516, D Loss: 0.49444616260007024, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 177/516, D Loss: 0.49557124078273773, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 178/516, D Loss: 0.49589076871052384, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 179/516, D Loss: 0.49849981162697077, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 180/516, D Loss: 0.489330418407917, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 181/516, D Loss: 0.4865657286718488, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 182/516, D Loss: 0.49891159124672413, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 183/516, D Loss: 0.487712181173265, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 184/516, D Loss: 0.49261785484850407, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 185/516, D Loss: 0.4986569748725742, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 186/516, D Loss: 0.49120078422129154, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 187/516, D Loss: 0.4956469526514411, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 188/516, D Loss: 0.4959605811163783, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 189/516, D Loss: 0.49917238706257194, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 190/516, D Loss: 0.49883659824263304, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 191/516, D Loss: 0.4941644766367972, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 192/516, D Loss: 0.49523571133613586, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 193/516, D Loss: 0.49513782281428576, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 194/516, D Loss: 0.4993160940357484, G Loss: -0.9999992847442627\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 195/516, D Loss: 0.4950778638012707, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 196/516, D Loss: 0.49499342078343034, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 197/516, D Loss: 0.4997604775126092, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 198/516, D Loss: 0.49368566321209073, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 199/516, D Loss: 0.49895445234142244, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 200/516, D Loss: 0.4978042598813772, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 201/516, D Loss: 0.48823411390185356, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 202/516, D Loss: 0.49952580998069607, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 203/516, D Loss: 0.4933629147708416, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 204/516, D Loss: 0.49935257236938924, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 205/516, D Loss: 0.486393959261477, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 206/516, D Loss: 0.49915138457436115, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 207/516, D Loss: 0.4993135369149968, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 208/516, D Loss: 0.4956314219161868, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 209/516, D Loss: 0.49079636111855507, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 210/516, D Loss: 0.48990759532898664, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 211/516, D Loss: 0.4951285128481686, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 212/516, D Loss: 0.49791910126805305, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 213/516, D Loss: 0.49973031427362, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 214/516, D Loss: 0.4963796744123101, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 215/516, D Loss: 0.49523990927264094, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 216/516, D Loss: 0.4987879579421133, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 217/516, D Loss: 0.4950577267445624, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 218/516, D Loss: 0.49978171665861737, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 219/516, D Loss: 0.48584344424307346, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 220/516, D Loss: 0.48673392925411463, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 221/516, D Loss: 0.4936119872145355, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 222/516, D Loss: 0.4989726576022804, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 223/516, D Loss: 0.49329398619011045, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 224/516, D Loss: 0.49522687029093504, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 225/516, D Loss: 0.4878905490040779, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 226/516, D Loss: 0.49068551417440176, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 227/516, D Loss: 0.4896755740046501, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 228/516, D Loss: 0.49529666593298316, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 229/516, D Loss: 0.49581988295540214, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 230/516, D Loss: 0.49527969770133495, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 231/516, D Loss: 0.49430380621924996, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 232/516, D Loss: 0.4944540364667773, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 233/516, D Loss: 0.49379227543249726, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 234/516, D Loss: 0.499477774836123, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 235/516, D Loss: 0.4982401414308697, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 236/516, D Loss: 0.48264226503670216, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 237/516, D Loss: 0.498291477560997, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 238/516, D Loss: 0.49638150818645954, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 239/516, D Loss: 0.49014967679977417, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 240/516, D Loss: 0.4996489708137233, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 241/516, D Loss: 0.4957360504195094, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 242/516, D Loss: 0.494128602091223, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 243/516, D Loss: 0.4964028256945312, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 244/516, D Loss: 0.4985688452143222, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 245/516, D Loss: 0.4994017121498473, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 246/516, D Loss: 0.49932415143121034, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 247/516, D Loss: 0.4975983272306621, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 248/516, D Loss: 0.4908721186220646, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 249/516, D Loss: 0.49904278153553605, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 250/516, D Loss: 0.4987995676929131, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 251/516, D Loss: 0.4979237327352166, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 252/516, D Loss: 0.49878459179308265, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 253/516, D Loss: 0.49898319703061134, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 254/516, D Loss: 0.4938326384872198, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 255/516, D Loss: 0.49016996659338474, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 256/516, D Loss: 0.4836207181215286, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 257/516, D Loss: 0.49739882117137313, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 258/516, D Loss: 0.49667828041128814, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 259/516, D Loss: 0.4983861012151465, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 260/516, D Loss: 0.4993860403774306, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 261/516, D Loss: 0.49879731878172606, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 262/516, D Loss: 0.49577617831528187, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 263/516, D Loss: 0.49880804563872516, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 264/516, D Loss: 0.49459591414779425, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 265/516, D Loss: 0.49380965158343315, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 266/516, D Loss: 0.4841018430888653, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 267/516, D Loss: 0.49881658935919404, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 268/516, D Loss: 0.4872898804023862, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 269/516, D Loss: 0.49130360409617424, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 270/516, D Loss: 0.4934028475545347, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 271/516, D Loss: 0.49847557093016803, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 272/516, D Loss: 0.49447064893320203, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 273/516, D Loss: 0.4934276696294546, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 274/516, D Loss: 0.4936489602550864, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 275/516, D Loss: 0.4955721218138933, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 276/516, D Loss: 0.48922504112124443, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 277/516, D Loss: 0.49290523305535316, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 278/516, D Loss: 0.4841431975364685, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 279/516, D Loss: 0.49587328964844346, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 280/516, D Loss: 0.4950736528262496, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 281/516, D Loss: 0.4949097465723753, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 282/516, D Loss: 0.4905059989541769, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 283/516, D Loss: 0.4965267542283982, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 284/516, D Loss: 0.4987211520783603, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 285/516, D Loss: 0.48902815394103527, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 286/516, D Loss: 0.49728316324763, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 287/516, D Loss: 0.48457636311650276, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 288/516, D Loss: 0.4915430396795273, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 289/516, D Loss: 0.47498543187975883, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 290/516, D Loss: 0.4913334250450134, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 291/516, D Loss: 0.4824675638228655, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 292/516, D Loss: 0.49522807635366917, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 293/516, D Loss: 0.49033323489129543, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 294/516, D Loss: 0.4989674797980115, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 295/516, D Loss: 0.4897005259990692, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 296/516, D Loss: 0.49370794743299484, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 297/516, D Loss: 0.49815825116820633, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 298/516, D Loss: 0.4956820122897625, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 299/516, D Loss: 0.49861658830195665, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 300/516, D Loss: 0.49907791608711705, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 301/516, D Loss: 0.48980934359133244, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 302/516, D Loss: 0.498938063159585, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 303/516, D Loss: 0.49237282713875175, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 304/516, D Loss: 0.4991972944699228, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 305/516, D Loss: 0.49052649084478617, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 306/516, D Loss: 0.49395984038710594, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 307/516, D Loss: 0.493312350474298, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 308/516, D Loss: 0.4885584516450763, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 309/516, D Loss: 0.49144349060952663, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 310/516, D Loss: 0.4878061804920435, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 311/516, D Loss: 0.49255083221942186, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 312/516, D Loss: 0.4894496602937579, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 313/516, D Loss: 0.497072062920779, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 314/516, D Loss: 0.49650020292028785, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 315/516, D Loss: 0.4992760817403905, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 316/516, D Loss: 0.4925111490301788, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 317/516, D Loss: 0.48601591028273106, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 318/516, D Loss: 0.49940737104043365, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 319/516, D Loss: 0.49291469855234027, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 320/516, D Loss: 0.4965794531162828, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 321/516, D Loss: 0.4922546027228236, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 322/516, D Loss: 0.49385349033400416, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 323/516, D Loss: 0.4776372089982033, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 324/516, D Loss: 0.48589545022696257, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 325/516, D Loss: 0.485476803034544, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 326/516, D Loss: 0.4952217321842909, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 327/516, D Loss: 0.49217408802360296, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 328/516, D Loss: 0.4965588105842471, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 329/516, D Loss: 0.49508949369192123, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 330/516, D Loss: 0.491005583666265, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 331/516, D Loss: 0.48826140724122524, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 332/516, D Loss: 0.4897075267508626, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 333/516, D Loss: 0.4942880989983678, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 334/516, D Loss: 0.49544664146378636, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 335/516, D Loss: 0.49159678258001804, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 336/516, D Loss: 0.4974294048734009, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 337/516, D Loss: 0.4917054232209921, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 338/516, D Loss: 0.4916241364553571, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 339/516, D Loss: 0.4947934071533382, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 340/516, D Loss: 0.49941464263247326, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 341/516, D Loss: 0.49806223646737635, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 342/516, D Loss: 0.49521940061822534, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 343/516, D Loss: 0.4974936409853399, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 344/516, D Loss: 0.48234952986240387, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 345/516, D Loss: 0.49016372486948967, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 346/516, D Loss: 0.49583368841558695, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 347/516, D Loss: 0.49381159199401736, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 348/516, D Loss: 0.4869999922811985, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 349/516, D Loss: 0.49793082871474326, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 350/516, D Loss: 0.4942294699139893, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 351/516, D Loss: 0.49691179348155856, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 352/516, D Loss: 0.49823368177749217, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 353/516, D Loss: 0.4995316950371489, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 354/516, D Loss: 0.4982384155737236, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 355/516, D Loss: 0.49876269535161555, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 356/516, D Loss: 0.4952154173515737, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 357/516, D Loss: 0.4854547306895256, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 358/516, D Loss: 0.4923066580668092, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 359/516, D Loss: 0.49340670369565487, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 360/516, D Loss: 0.4967950670979917, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 361/516, D Loss: 0.4903464801609516, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 362/516, D Loss: 0.49172937497496605, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 363/516, D Loss: 0.49970864941133186, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 364/516, D Loss: 0.49371561128646135, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 365/516, D Loss: 0.49827242991887033, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 366/516, D Loss: 0.4935792996548116, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 367/516, D Loss: 0.4973129187710583, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 368/516, D Loss: 0.492490213830024, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 369/516, D Loss: 0.4967505324166268, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 370/516, D Loss: 0.4943724134936929, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 371/516, D Loss: 0.4987038744147867, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 372/516, D Loss: 0.49854897789191455, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 373/516, D Loss: 0.49878980312496424, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 374/516, D Loss: 0.49185705184936523, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 375/516, D Loss: 0.4937679688446224, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 376/516, D Loss: 0.49181013833731413, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 377/516, D Loss: 0.49096067249774933, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 378/516, D Loss: 0.4947443613782525, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 379/516, D Loss: 0.4927032315172255, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 380/516, D Loss: 0.49935234093572944, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 381/516, D Loss: 0.49727666564285755, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 382/516, D Loss: 0.4987165021011606, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 383/516, D Loss: 0.49622774915769696, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 384/516, D Loss: 0.495431718416512, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 385/516, D Loss: 0.4918832527473569, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 386/516, D Loss: 0.4896828103810549, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 387/516, D Loss: 0.4911946626380086, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 388/516, D Loss: 0.4901153175160289, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 389/516, D Loss: 0.4925283193588257, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 390/516, D Loss: 0.4946075030602515, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 391/516, D Loss: 0.4946582745760679, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 392/516, D Loss: 0.4996574248652905, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 393/516, D Loss: 0.4948005652986467, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 394/516, D Loss: 0.4885928761214018, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 395/516, D Loss: 0.49366720113903284, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 396/516, D Loss: 0.488991130143404, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 397/516, D Loss: 0.4915841370820999, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 398/516, D Loss: 0.49157450441271067, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 399/516, D Loss: 0.4935343712568283, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 400/516, D Loss: 0.49971947429003194, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 401/516, D Loss: 0.4989669028436765, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 402/516, D Loss: 0.4983759638853371, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 403/516, D Loss: 0.49427731707692146, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 404/516, D Loss: 0.494001938495785, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 405/516, D Loss: 0.49121172074228525, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 406/516, D Loss: 0.494211012031883, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 407/516, D Loss: 0.4942283262498677, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 408/516, D Loss: 0.49581358628347516, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 409/516, D Loss: 0.4975408036261797, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 410/516, D Loss: 0.49143541418015957, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 411/516, D Loss: 0.49412885401397943, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 412/516, D Loss: 0.49264366226270795, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 413/516, D Loss: 0.49434276949614286, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 414/516, D Loss: 0.49939226265996695, G Loss: -0.999999463558197\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 415/516, D Loss: 0.493410209659487, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 416/516, D Loss: 0.4847700307145715, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 417/516, D Loss: 0.4954704260453582, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 418/516, D Loss: 0.4875354366376996, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 419/516, D Loss: 0.4920596554875374, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 420/516, D Loss: 0.49385252222418785, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 421/516, D Loss: 0.49317197361961007, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 422/516, D Loss: 0.49838548572734, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 423/516, D Loss: 0.49239349039271474, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 424/516, D Loss: 0.49710620753467083, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 425/516, D Loss: 0.4946123920381069, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 426/516, D Loss: 0.4995642707508523, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 427/516, D Loss: 0.4836427289992571, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 428/516, D Loss: 0.49386297864839435, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 429/516, D Loss: 0.4949356149882078, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 430/516, D Loss: 0.49593101162463427, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 431/516, D Loss: 0.4994096300797537, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 432/516, D Loss: 0.48676462937146425, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 433/516, D Loss: 0.49126006849110126, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 434/516, D Loss: 0.4980811494169757, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 435/516, D Loss: 0.4842627737671137, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 436/516, D Loss: 0.49895476235542446, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 437/516, D Loss: 0.4953005122952163, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 438/516, D Loss: 0.4958909125998616, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 439/516, D Loss: 0.4934673011302948, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 440/516, D Loss: 0.4986055218614638, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 441/516, D Loss: 0.4958851495757699, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 442/516, D Loss: 0.4912207527086139, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 443/516, D Loss: 0.4993142717285082, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 444/516, D Loss: 0.49906845681834966, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 445/516, D Loss: 0.4920916175469756, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 446/516, D Loss: 0.49596366845071316, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 447/516, D Loss: 0.4992009468842298, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 448/516, D Loss: 0.49227646505460143, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 449/516, D Loss: 0.49986244342289865, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 450/516, D Loss: 0.49467448657378554, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 451/516, D Loss: 0.4991015956038609, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 452/516, D Loss: 0.49714027368463576, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 453/516, D Loss: 0.4917253106832504, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 454/516, D Loss: 0.4910486815497279, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 455/516, D Loss: 0.49914808524772525, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 456/516, D Loss: 0.49635213427245617, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 457/516, D Loss: 0.4961775718256831, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 458/516, D Loss: 0.4949107365682721, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 459/516, D Loss: 0.499250880733598, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 460/516, D Loss: 0.4980322499759495, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 461/516, D Loss: 0.4941749544814229, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 462/516, D Loss: 0.4957374739460647, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 463/516, D Loss: 0.4972951845265925, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 464/516, D Loss: 0.4943687985651195, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 465/516, D Loss: 0.49084764532744884, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 466/516, D Loss: 0.49321676790714264, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 467/516, D Loss: 0.4953106096945703, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 468/516, D Loss: 0.48671664483845234, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 469/516, D Loss: 0.4994698209920898, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 470/516, D Loss: 0.4995506112172734, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 471/516, D Loss: 0.4993493970250711, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 472/516, D Loss: 0.4982492213603109, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 473/516, D Loss: 0.4816317930817604, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 474/516, D Loss: 0.49798768921755254, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 475/516, D Loss: 0.49405627185478806, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 476/516, D Loss: 0.4881540257483721, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 477/516, D Loss: 0.49920573126291856, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 478/516, D Loss: 0.48711194656789303, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 479/516, D Loss: 0.4950939556583762, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 480/516, D Loss: 0.48213869519531727, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 481/516, D Loss: 0.4994716345681809, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 482/516, D Loss: 0.48097822070121765, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 483/516, D Loss: 0.49524841364473104, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 484/516, D Loss: 0.4991730435285717, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 5, Batch 485/516, D Loss: 0.49951694265473634, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 486/516, D Loss: 0.4898017141968012, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 487/516, D Loss: 0.495763523504138, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 5, Batch 488/516, D Loss: 0.4965128458570689, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 489/516, D Loss: 0.4916059747338295, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 5, Batch 490/516, D Loss: 0.4958420326001942, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 491/516, D Loss: 0.49473640974611044, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 492/516, D Loss: 0.49905879324069247, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 493/516, D Loss: 0.4983372436836362, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 494/516, D Loss: 0.49494081223383546, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 495/516, D Loss: 0.4928418011404574, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 496/516, D Loss: 0.4968136663082987, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 497/516, D Loss: 0.4945492665283382, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 498/516, D Loss: 0.4945439761504531, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 499/516, D Loss: 0.4991067723603919, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 500/516, D Loss: 0.49600230623036623, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 501/516, D Loss: 0.49126136861741543, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 502/516, D Loss: 0.49362696101889014, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 503/516, D Loss: 0.49695492372848094, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 504/516, D Loss: 0.4948625643737614, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 505/516, D Loss: 0.4876457890495658, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 506/516, D Loss: 0.4958105986006558, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 507/516, D Loss: 0.49189342372119427, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 508/516, D Loss: 0.4883946133777499, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 509/516, D Loss: 0.4944441798143089, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 510/516, D Loss: 0.49173308070749044, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 511/516, D Loss: 0.4904936086386442, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 512/516, D Loss: 0.4924417147412896, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 513/516, D Loss: 0.4958791551180184, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 514/516, D Loss: 0.49898919835686684, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 5, Batch 515/516, D Loss: 0.49323510751128197, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 5, Batch 516/516, D Loss: 0.4899146305397153, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 1/516, D Loss: 0.4971776599995792, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 2/516, D Loss: 0.4935595579445362, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 3/516, D Loss: 0.4948781239800155, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 4/516, D Loss: 0.4994024393381551, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 5/516, D Loss: 0.4974489947780967, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 6/516, D Loss: 0.49003647081553936, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 7/516, D Loss: 0.49521468859165907, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 8/516, D Loss: 0.49305883701890707, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 9/516, D Loss: 0.49811729742214084, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 10/516, D Loss: 0.4958682004362345, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 11/516, D Loss: 0.4880502810701728, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 12/516, D Loss: 0.4992888832348399, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 13/516, D Loss: 0.4916216479614377, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 14/516, D Loss: 0.49488026089966297, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 15/516, D Loss: 0.49622202245518565, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 16/516, D Loss: 0.4925865023396909, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 17/516, D Loss: 0.48563532903790474, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 18/516, D Loss: 0.49035900738090277, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 19/516, D Loss: 0.4903638679534197, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 20/516, D Loss: 0.49313185969367623, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 21/516, D Loss: 0.495707998983562, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 22/516, D Loss: 0.49946733453543857, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 23/516, D Loss: 0.4926952989771962, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 24/516, D Loss: 0.4909689947962761, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 25/516, D Loss: 0.4932048162445426, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 6, Batch 26/516, D Loss: 0.4945633835159242, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 27/516, D Loss: 0.4955504583194852, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 28/516, D Loss: 0.4946900438517332, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 29/516, D Loss: 0.48971070535480976, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 30/516, D Loss: 0.49050307366997004, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 31/516, D Loss: 0.48841308802366257, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 32/516, D Loss: 0.4991270591272041, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 33/516, D Loss: 0.4981768799480051, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 34/516, D Loss: 0.49389418959617615, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 35/516, D Loss: 0.4958180207759142, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 36/516, D Loss: 0.49293615110218525, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 37/516, D Loss: 0.49980480066733435, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 38/516, D Loss: 0.4913655910640955, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 39/516, D Loss: 0.4864438185468316, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 40/516, D Loss: 0.4849818991497159, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 41/516, D Loss: 0.49579919315874577, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 42/516, D Loss: 0.49908331100596115, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 43/516, D Loss: 0.4914000956341624, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 44/516, D Loss: 0.490581220947206, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 45/516, D Loss: 0.4971781966742128, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 46/516, D Loss: 0.49428637931123376, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 47/516, D Loss: 0.48336130008101463, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 48/516, D Loss: 0.49441238353028893, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 49/516, D Loss: 0.49539361195638776, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 50/516, D Loss: 0.4951482596807182, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 51/516, D Loss: 0.4986542910337448, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 52/516, D Loss: 0.49647695175372064, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 53/516, D Loss: 0.4935973258689046, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 54/516, D Loss: 0.49967719946289435, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 55/516, D Loss: 0.4921160452067852, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 56/516, D Loss: 0.4990669940598309, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 57/516, D Loss: 0.4967450578697026, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 58/516, D Loss: 0.4955928847193718, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 59/516, D Loss: 0.4916376955807209, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 60/516, D Loss: 0.4951012539677322, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 61/516, D Loss: 0.49191837944090366, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 62/516, D Loss: 0.48976578563451767, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 63/516, D Loss: 0.4993179136654362, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 64/516, D Loss: 0.4994492706027813, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 65/516, D Loss: 0.4952230560593307, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 66/516, D Loss: 0.4867131020873785, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 67/516, D Loss: 0.4893647423014045, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 68/516, D Loss: 0.4959310912527144, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 69/516, D Loss: 0.49937318405136466, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 70/516, D Loss: 0.4948151567950845, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 71/516, D Loss: 0.4940168410539627, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 72/516, D Loss: 0.48909975308924913, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 73/516, D Loss: 0.4956092815846205, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 74/516, D Loss: 0.49344267370179296, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 75/516, D Loss: 0.497578973416239, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 76/516, D Loss: 0.48483424074947834, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 77/516, D Loss: 0.4955477071925998, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 78/516, D Loss: 0.4977254068944603, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 79/516, D Loss: 0.4991098960163072, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 80/516, D Loss: 0.49884613254107535, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 81/516, D Loss: 0.49700333341024816, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 6, Batch 82/516, D Loss: 0.4997800828277832, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 83/516, D Loss: 0.49543460877612233, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 84/516, D Loss: 0.49982023365737405, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 85/516, D Loss: 0.4984634125139564, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 86/516, D Loss: 0.4957728786394, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 87/516, D Loss: 0.49355506990104914, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 88/516, D Loss: 0.4923909045755863, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 89/516, D Loss: 0.495439185295254, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 90/516, D Loss: 0.4991663711844012, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 91/516, D Loss: 0.49856217054184526, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 92/516, D Loss: 0.4828606955707073, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 93/516, D Loss: 0.49691425380297005, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 94/516, D Loss: 0.49384054727852345, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 95/516, D Loss: 0.4943731902167201, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 96/516, D Loss: 0.4958212892524898, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 97/516, D Loss: 0.4958424190990627, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 98/516, D Loss: 0.4991377325495705, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 99/516, D Loss: 0.4990259204059839, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 100/516, D Loss: 0.48406197130680084, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 101/516, D Loss: 0.49505780870094895, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 102/516, D Loss: 0.498880245257169, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 103/516, D Loss: 0.4987703424412757, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 104/516, D Loss: 0.4993568995851092, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 105/516, D Loss: 0.49340728111565113, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 106/516, D Loss: 0.4997657675703522, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 107/516, D Loss: 0.49536469765007496, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 108/516, D Loss: 0.49705126346088946, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 109/516, D Loss: 0.4909979235380888, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 110/516, D Loss: 0.4829791896045208, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 111/516, D Loss: 0.4955967222340405, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 112/516, D Loss: 0.4915089048445225, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 113/516, D Loss: 0.4894299041479826, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 114/516, D Loss: 0.4917322574183345, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 115/516, D Loss: 0.49961773300310597, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 116/516, D Loss: 0.496524851070717, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 117/516, D Loss: 0.498820714536123, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 118/516, D Loss: 0.49285828694701195, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 119/516, D Loss: 0.49545347411185503, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 120/516, D Loss: 0.49618011759594083, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 121/516, D Loss: 0.4948087725788355, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 122/516, D Loss: 0.4940707525238395, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 123/516, D Loss: 0.4963313811458647, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 124/516, D Loss: 0.49772479850798845, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 125/516, D Loss: 0.49843141227029264, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 126/516, D Loss: 0.49422805616632104, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 127/516, D Loss: 0.49080019909888506, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 128/516, D Loss: 0.4972821499686688, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 129/516, D Loss: 0.49947310582501814, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 130/516, D Loss: 0.4989913875469938, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 131/516, D Loss: 0.4937027604319155, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 132/516, D Loss: 0.49487336399033666, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 133/516, D Loss: 0.484053673222661, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 134/516, D Loss: 0.49324454367160797, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 135/516, D Loss: 0.4995470129069872, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 136/516, D Loss: 0.49053883366286755, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 137/516, D Loss: 0.4981783570256084, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 138/516, D Loss: 0.48819186445325613, G Loss: -0.9999995231628418\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 139/516, D Loss: 0.4954467876814306, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 140/516, D Loss: 0.49810257041826844, G Loss: -0.9999994039535522\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 141/516, D Loss: 0.4948257105425, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 142/516, D Loss: 0.49631414050236344, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 143/516, D Loss: 0.49851506284903735, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 144/516, D Loss: 0.49746895721182227, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 145/516, D Loss: 0.48799449019134045, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 146/516, D Loss: 0.48627969436347485, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 147/516, D Loss: 0.4990624211495742, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 148/516, D Loss: 0.49053812865167856, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 149/516, D Loss: 0.4868198335170746, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 150/516, D Loss: 0.49618566781282425, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 151/516, D Loss: 0.4982329064514488, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 152/516, D Loss: 0.4859167393296957, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 153/516, D Loss: 0.4939982630312443, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 6, Batch 154/516, D Loss: 0.49806026415899396, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 155/516, D Loss: 0.47961796820163727, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 156/516, D Loss: 0.48697065748274326, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 157/516, D Loss: 0.49138381890952587, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 158/516, D Loss: 0.49888589209876955, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 159/516, D Loss: 0.4946794845163822, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 160/516, D Loss: 0.48025016114115715, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 161/516, D Loss: 0.49081189930438995, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 162/516, D Loss: 0.4962846180424094, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 163/516, D Loss: 0.49478515796363354, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 164/516, D Loss: 0.4927331516519189, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 165/516, D Loss: 0.4938569739460945, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 166/516, D Loss: 0.4983607593458146, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 167/516, D Loss: 0.4959779754281044, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 168/516, D Loss: 0.4879086408764124, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 169/516, D Loss: 0.4995143264532089, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 170/516, D Loss: 0.4998351197282318, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 171/516, D Loss: 0.4931969018653035, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 172/516, D Loss: 0.49426588928326964, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 173/516, D Loss: 0.4955700454302132, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 174/516, D Loss: 0.4988568468252197, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 175/516, D Loss: 0.4869570340961218, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 176/516, D Loss: 0.494013886898756, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 177/516, D Loss: 0.48751859460026026, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 178/516, D Loss: 0.49151252489537, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 179/516, D Loss: 0.4933666596189141, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 180/516, D Loss: 0.4997538905008696, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 181/516, D Loss: 0.4853556156158447, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 182/516, D Loss: 0.49438081169500947, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 183/516, D Loss: 0.49874153453856707, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 184/516, D Loss: 0.49983270837401506, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 185/516, D Loss: 0.4943582438863814, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 186/516, D Loss: 0.4871003944426775, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 187/516, D Loss: 0.49383745808154345, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 188/516, D Loss: 0.49868721421808004, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 189/516, D Loss: 0.4981592403491959, G Loss: -0.9999995827674866\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 190/516, D Loss: 0.49830415716860443, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 191/516, D Loss: 0.49102285504341125, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 192/516, D Loss: 0.49601428071036935, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 193/516, D Loss: 0.49392414977774024, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 194/516, D Loss: 0.49428743217140436, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 195/516, D Loss: 0.4905236503109336, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 196/516, D Loss: 0.49092675652354956, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 197/516, D Loss: 0.4955026186071336, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 198/516, D Loss: 0.49402873404324055, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 199/516, D Loss: 0.4928196822293103, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 200/516, D Loss: 0.49578394228592515, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 201/516, D Loss: 0.4936738461256027, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 202/516, D Loss: 0.49492928106337786, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 203/516, D Loss: 0.49565635807812214, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 204/516, D Loss: 0.49509148858487606, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 205/516, D Loss: 0.47586485370993614, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 206/516, D Loss: 0.49083284102380276, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 207/516, D Loss: 0.49074443709105253, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 208/516, D Loss: 0.4961522640660405, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 209/516, D Loss: 0.4948393004015088, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 210/516, D Loss: 0.4912893772125244, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 211/516, D Loss: 0.49787609092891216, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 212/516, D Loss: 0.49963973759440705, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 213/516, D Loss: 0.49703680258244276, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 214/516, D Loss: 0.4993020766414702, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 215/516, D Loss: 0.49631941225379705, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 216/516, D Loss: 0.49295045202597976, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 217/516, D Loss: 0.4967799661681056, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 218/516, D Loss: 0.4878823719918728, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 219/516, D Loss: 0.4985883136978373, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 220/516, D Loss: 0.49928543681744486, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 221/516, D Loss: 0.4805149454623461, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 222/516, D Loss: 0.49926113843685016, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 223/516, D Loss: 0.4957058443687856, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 224/516, D Loss: 0.49698180379346013, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 225/516, D Loss: 0.48844868037849665, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 226/516, D Loss: 0.4996658822055906, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 227/516, D Loss: 0.4946507019922137, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 228/516, D Loss: 0.4913708195090294, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 229/516, D Loss: 0.4942634729668498, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 230/516, D Loss: 0.48966858722269535, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 231/516, D Loss: 0.49891972693149, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 232/516, D Loss: 0.49506401363760233, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 233/516, D Loss: 0.4871062906458974, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 234/516, D Loss: 0.4945178525522351, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 235/516, D Loss: 0.49944134533870965, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 236/516, D Loss: 0.4967047958634794, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 237/516, D Loss: 0.4995299307629466, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 238/516, D Loss: 0.49824592377990484, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 239/516, D Loss: 0.49703523702919483, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 240/516, D Loss: 0.4908538144081831, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 241/516, D Loss: 0.494998537003994, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 242/516, D Loss: 0.49508746061474085, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 243/516, D Loss: 0.49895056092645973, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 244/516, D Loss: 0.4974483388941735, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 245/516, D Loss: 0.49951398896519095, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 246/516, D Loss: 0.4944929713383317, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 247/516, D Loss: 0.4989667044719681, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 248/516, D Loss: 0.49944607209181413, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 249/516, D Loss: 0.4952506311237812, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 250/516, D Loss: 0.49861444986891, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 251/516, D Loss: 0.4952564500272274, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 252/516, D Loss: 0.49525591591373086, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 253/516, D Loss: 0.49521447718143463, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 254/516, D Loss: 0.48607600666582584, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 255/516, D Loss: 0.4985733317444101, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 256/516, D Loss: 0.49853550794068724, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 257/516, D Loss: 0.487772760912776, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 258/516, D Loss: 0.49499132949858904, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 259/516, D Loss: 0.49327511340379715, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 260/516, D Loss: 0.48516871128231287, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 261/516, D Loss: 0.4947051857598126, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 262/516, D Loss: 0.49554547760635614, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 263/516, D Loss: 0.4863724047318101, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 264/516, D Loss: 0.4990734747843817, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 265/516, D Loss: 0.49397985031828284, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 266/516, D Loss: 0.4879560172557831, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 267/516, D Loss: 0.49923870380735025, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 268/516, D Loss: 0.49891248997300863, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 269/516, D Loss: 0.49853175273165107, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 270/516, D Loss: 0.49967007074155845, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 271/516, D Loss: 0.48882682994008064, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 272/516, D Loss: 0.4951242506504059, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 273/516, D Loss: 0.49094248563051224, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 274/516, D Loss: 0.48954854905605316, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 275/516, D Loss: 0.4975788155570626, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 276/516, D Loss: 0.494751482270658, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 277/516, D Loss: 0.4965226303320378, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 278/516, D Loss: 0.4978768879082054, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 279/516, D Loss: 0.49867612728849053, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 280/516, D Loss: 0.4878156539052725, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 281/516, D Loss: 0.495469918474555, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 282/516, D Loss: 0.4983563619898632, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 283/516, D Loss: 0.49924635648494586, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 284/516, D Loss: 0.4993143482715823, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 285/516, D Loss: 0.4957219040952623, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 286/516, D Loss: 0.48426854982972145, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 287/516, D Loss: 0.4937930000014603, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 288/516, D Loss: 0.49143445398658514, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 289/516, D Loss: 0.48744768276810646, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 290/516, D Loss: 0.4945083656348288, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 291/516, D Loss: 0.4915951881557703, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 292/516, D Loss: 0.4921066779643297, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 293/516, D Loss: 0.4994360852288082, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 294/516, D Loss: 0.48923528753221035, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 295/516, D Loss: 0.49908561480697244, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 296/516, D Loss: 0.49507411010563374, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 297/516, D Loss: 0.49298663157969713, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 298/516, D Loss: 0.49949572765035555, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 299/516, D Loss: 0.4967214565258473, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 300/516, D Loss: 0.49745215359143913, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 301/516, D Loss: 0.49776515387929976, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 302/516, D Loss: 0.4991197019116953, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 303/516, D Loss: 0.4933702275156975, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 304/516, D Loss: 0.48986407183110714, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 305/516, D Loss: 0.4971554772928357, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 306/516, D Loss: 0.48911439534276724, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 307/516, D Loss: 0.4946134490892291, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 308/516, D Loss: 0.4982907078228891, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 309/516, D Loss: 0.4949293565005064, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 310/516, D Loss: 0.493589214514941, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 311/516, D Loss: 0.4935126034542918, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 312/516, D Loss: 0.49540140852332115, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 313/516, D Loss: 0.4937344454228878, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 314/516, D Loss: 0.4958251570351422, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 315/516, D Loss: 0.49988671739265556, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 316/516, D Loss: 0.4896974340081215, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 317/516, D Loss: 0.49089138116687536, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 318/516, D Loss: 0.498866863315925, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 319/516, D Loss: 0.4944805158302188, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 320/516, D Loss: 0.49886789661832154, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 321/516, D Loss: 0.4892057729884982, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 322/516, D Loss: 0.49624037463217974, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 323/516, D Loss: 0.4940836476162076, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 324/516, D Loss: 0.4973097303882241, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 325/516, D Loss: 0.49911121441982687, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 326/516, D Loss: 0.4955936479382217, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 327/516, D Loss: 0.4952872088178992, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 328/516, D Loss: 0.48995577823370695, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 329/516, D Loss: 0.49505135836079717, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 330/516, D Loss: 0.49894120648968965, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 331/516, D Loss: 0.4948566975072026, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 332/516, D Loss: 0.4926583217456937, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 333/516, D Loss: 0.4953342145308852, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 334/516, D Loss: 0.49374020425602794, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 335/516, D Loss: 0.4887011284008622, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 336/516, D Loss: 0.4993955023237504, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 337/516, D Loss: 0.4993899259134196, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 338/516, D Loss: 0.49850218987558037, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 339/516, D Loss: 0.4906873060390353, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 340/516, D Loss: 0.4918448682874441, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 341/516, D Loss: 0.4796989746391773, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 342/516, D Loss: 0.49470656970515847, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 343/516, D Loss: 0.4956270339898765, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 344/516, D Loss: 0.49509990122169256, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 345/516, D Loss: 0.49485153006389737, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 346/516, D Loss: 0.4912548651918769, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 347/516, D Loss: 0.49489377345889807, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 348/516, D Loss: 0.495506867300719, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 349/516, D Loss: 0.49366111215204, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 350/516, D Loss: 0.4892611689865589, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 351/516, D Loss: 0.4931676285341382, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 352/516, D Loss: 0.4985052878037095, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 353/516, D Loss: 0.49503639666363597, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 354/516, D Loss: 0.49817470950074494, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 355/516, D Loss: 0.4996665420476347, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 356/516, D Loss: 0.4876468740403652, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 357/516, D Loss: 0.48793999571353197, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 358/516, D Loss: 0.499387405696325, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 359/516, D Loss: 0.4901371132582426, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 360/516, D Loss: 0.4977224045433104, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 361/516, D Loss: 0.49271390680223703, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 362/516, D Loss: 0.49859349080361426, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 363/516, D Loss: 0.49894986383151263, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 364/516, D Loss: 0.49477969016879797, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 365/516, D Loss: 0.4869785001501441, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 366/516, D Loss: 0.494573054369539, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 367/516, D Loss: 0.4946459853090346, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 368/516, D Loss: 0.49887266336008906, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 369/516, D Loss: 0.4998262101144064, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 370/516, D Loss: 0.49170124996453524, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 371/516, D Loss: 0.49431834695860744, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 372/516, D Loss: 0.4915188029408455, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 373/516, D Loss: 0.49810886883642524, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 374/516, D Loss: 0.49569142144173384, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 375/516, D Loss: 0.4893098399043083, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 376/516, D Loss: 0.4945172630250454, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 377/516, D Loss: 0.4875468499958515, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 378/516, D Loss: 0.4903709013015032, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 379/516, D Loss: 0.4956992161460221, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 380/516, D Loss: 0.4963406592141837, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 381/516, D Loss: 0.4962243544869125, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 382/516, D Loss: 0.49527572514489293, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 383/516, D Loss: 0.493423571344465, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 384/516, D Loss: 0.49842675344552845, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 385/516, D Loss: 0.49973645448335446, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 386/516, D Loss: 0.49755271850153804, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 387/516, D Loss: 0.4926305254921317, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 388/516, D Loss: 0.4957770584151149, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 389/516, D Loss: 0.4914839845150709, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 390/516, D Loss: 0.494839983060956, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 391/516, D Loss: 0.491153865121305, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 392/516, D Loss: 0.4977063359692693, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 393/516, D Loss: 0.4945879653096199, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 394/516, D Loss: 0.4958034437149763, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 395/516, D Loss: 0.4949426562525332, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 396/516, D Loss: 0.49105390906333923, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 397/516, D Loss: 0.49129354674369097, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 398/516, D Loss: 0.4960424220189452, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 399/516, D Loss: 0.491694625467062, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 400/516, D Loss: 0.48408584855496883, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 401/516, D Loss: 0.4923927551135421, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 402/516, D Loss: 0.49744697706773877, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 403/516, D Loss: 0.49517699889838696, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 404/516, D Loss: 0.4887529406696558, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 405/516, D Loss: 0.49518396705389023, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 406/516, D Loss: 0.4886895380914211, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 407/516, D Loss: 0.49406177550554276, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 408/516, D Loss: 0.49883109552320093, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 409/516, D Loss: 0.4997278225782793, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 410/516, D Loss: 0.49560018768534064, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 411/516, D Loss: 0.4991180615616031, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 412/516, D Loss: 0.4929369315505028, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 413/516, D Loss: 0.49545975401997566, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 414/516, D Loss: 0.49519370030611753, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 415/516, D Loss: 0.49316436517983675, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 416/516, D Loss: 0.4997807564213872, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 417/516, D Loss: 0.49474661191925406, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 418/516, D Loss: 0.4989302880130708, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 419/516, D Loss: 0.4997836191323586, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 420/516, D Loss: 0.49436035053804517, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 421/516, D Loss: 0.4839134030044079, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 422/516, D Loss: 0.49414446437731385, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 423/516, D Loss: 0.4927538731135428, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 424/516, D Loss: 0.4959447951987386, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 425/516, D Loss: 0.49175821524113417, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 426/516, D Loss: 0.49165147822350264, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 427/516, D Loss: 0.49459588853642344, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 428/516, D Loss: 0.4992398195900023, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 429/516, D Loss: 0.4935361356474459, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 430/516, D Loss: 0.4943367475643754, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 431/516, D Loss: 0.49760535592213273, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 432/516, D Loss: 0.4981069025816396, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 433/516, D Loss: 0.484426885843277, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 434/516, D Loss: 0.4950965438038111, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 435/516, D Loss: 0.4950485257431865, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 436/516, D Loss: 0.4964967027772218, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 437/516, D Loss: 0.4969388898462057, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 438/516, D Loss: 0.4957448001950979, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 439/516, D Loss: 0.4981277104234323, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 440/516, D Loss: 0.4912687577307224, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 441/516, D Loss: 0.4955535684712231, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 442/516, D Loss: 0.49864616082049906, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 443/516, D Loss: 0.49862294748891145, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 444/516, D Loss: 0.49258580012246966, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 445/516, D Loss: 0.4948843694292009, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 446/516, D Loss: 0.48958934750407934, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 447/516, D Loss: 0.4959592279046774, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 448/516, D Loss: 0.48864868097007275, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 449/516, D Loss: 0.4988985873060301, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 450/516, D Loss: 0.4941805200651288, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 451/516, D Loss: 0.49334988137707114, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 452/516, D Loss: 0.4957113806158304, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 453/516, D Loss: 0.49049975629895926, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 454/516, D Loss: 0.4958912283182144, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 455/516, D Loss: 0.4952910551801324, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 456/516, D Loss: 0.49261499661952257, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 457/516, D Loss: 0.4916259590536356, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 458/516, D Loss: 0.48893956653773785, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 459/516, D Loss: 0.49569705221801996, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 460/516, D Loss: 0.49126650486141443, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 461/516, D Loss: 0.498952106339857, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 462/516, D Loss: 0.49505011225119233, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 463/516, D Loss: 0.4991947724483907, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 464/516, D Loss: 0.4945529098622501, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 465/516, D Loss: 0.4997149427072145, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 466/516, D Loss: 0.49430802231654525, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 467/516, D Loss: 0.49075501412153244, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 468/516, D Loss: 0.488981444388628, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 469/516, D Loss: 0.49180738255381584, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 470/516, D Loss: 0.4925961159169674, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 471/516, D Loss: 0.4896689224988222, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 472/516, D Loss: 0.49073573015630245, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 473/516, D Loss: 0.49314116686582565, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 474/516, D Loss: 0.4884574543684721, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 475/516, D Loss: 0.49567822460085154, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 476/516, D Loss: 0.4923510644584894, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 477/516, D Loss: 0.49570137169212103, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 478/516, D Loss: 0.49891139892861247, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 479/516, D Loss: 0.49903295299736783, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 480/516, D Loss: 0.4989927925635129, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 481/516, D Loss: 0.49380616564303637, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 482/516, D Loss: 0.4955324358306825, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 483/516, D Loss: 0.4959575990214944, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 484/516, D Loss: 0.494159035384655, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 485/516, D Loss: 0.4959256211295724, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 486/516, D Loss: 0.4935171054676175, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 6, Batch 487/516, D Loss: 0.4971849888097495, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 488/516, D Loss: 0.495612938888371, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 489/516, D Loss: 0.4930600509978831, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 490/516, D Loss: 0.49532147590070963, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 491/516, D Loss: 0.4947524731978774, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 492/516, D Loss: 0.49531498830765486, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 493/516, D Loss: 0.48692939803004265, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 494/516, D Loss: 0.4939445308409631, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 495/516, D Loss: 0.495033270213753, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 496/516, D Loss: 0.4914600709453225, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 497/516, D Loss: 0.49534385185688734, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 498/516, D Loss: 0.4919103756546974, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 499/516, D Loss: 0.49958823242923245, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 500/516, D Loss: 0.49366861023008823, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 501/516, D Loss: 0.4951409315690398, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 502/516, D Loss: 0.49757518945261836, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 503/516, D Loss: 0.4968798451591283, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 504/516, D Loss: 0.4825798813253641, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 505/516, D Loss: 0.49658720451407135, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 506/516, D Loss: 0.49757578410208225, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 507/516, D Loss: 0.48971693217754364, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 508/516, D Loss: 0.4942840072326362, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 509/516, D Loss: 0.4940614476799965, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 510/516, D Loss: 0.49461480230093, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 6, Batch 511/516, D Loss: 0.4939298680983484, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 512/516, D Loss: 0.49606033228337765, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 6, Batch 513/516, D Loss: 0.49480999913066626, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 514/516, D Loss: 0.49450419563800097, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 515/516, D Loss: 0.49216651637107134, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 6, Batch 516/516, D Loss: 0.49343956261873245, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 1/516, D Loss: 0.4868935775011778, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 2/516, D Loss: 0.49525056360289454, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 3/516, D Loss: 0.4948608912527561, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 4/516, D Loss: 0.4911509221419692, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 5/516, D Loss: 0.49758459255099297, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 6/516, D Loss: 0.48044830560684204, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 7/516, D Loss: 0.4953570398502052, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 8/516, D Loss: 0.48966530431061983, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 9/516, D Loss: 0.49924631731119007, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 10/516, D Loss: 0.49582721339538693, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 11/516, D Loss: 0.49410383915528655, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 12/516, D Loss: 0.4895719178020954, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 13/516, D Loss: 0.4939097221940756, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 14/516, D Loss: 0.4986817123135552, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 15/516, D Loss: 0.4995075663900934, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 16/516, D Loss: 0.49024161137640476, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 17/516, D Loss: 0.4870894756168127, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 18/516, D Loss: 0.49891112511977553, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 19/516, D Loss: 0.48313748091459274, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 20/516, D Loss: 0.4987948368070647, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 21/516, D Loss: 0.4963383877184242, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 22/516, D Loss: 0.49226739909499884, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 23/516, D Loss: 0.4938751724548638, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 24/516, D Loss: 0.4986588852480054, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 25/516, D Loss: 0.48340854048728943, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 26/516, D Loss: 0.4956174800172448, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 27/516, D Loss: 0.4913067538291216, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 28/516, D Loss: 0.49325886322185397, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 29/516, D Loss: 0.4873749325051904, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 30/516, D Loss: 0.4871575217694044, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 31/516, D Loss: 0.48378749936819077, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 32/516, D Loss: 0.4893677029758692, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 33/516, D Loss: 0.4943063505925238, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 34/516, D Loss: 0.481943279504776, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 35/516, D Loss: 0.4930627979338169, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 36/516, D Loss: 0.49477965384721756, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 37/516, D Loss: 0.482548750936985, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 38/516, D Loss: 0.49561600340530276, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 39/516, D Loss: 0.4902008827775717, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 40/516, D Loss: 0.4952522343955934, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 41/516, D Loss: 0.4989175081718713, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 42/516, D Loss: 0.49638797249644995, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 43/516, D Loss: 0.49650299502536654, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 44/516, D Loss: 0.4942756397649646, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 45/516, D Loss: 0.49938686267705634, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 46/516, D Loss: 0.4996647978841793, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 47/516, D Loss: 0.4858705494552851, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 48/516, D Loss: 0.49099214281886816, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 49/516, D Loss: 0.4991766073508188, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 50/516, D Loss: 0.49502644035965204, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 51/516, D Loss: 0.4941746676340699, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 52/516, D Loss: 0.4947234424762428, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 53/516, D Loss: 0.487761901691556, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 54/516, D Loss: 0.4945433810353279, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 55/516, D Loss: 0.49682040559127927, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 56/516, D Loss: 0.4878285862505436, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 57/516, D Loss: 0.49940072739263996, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 58/516, D Loss: 0.4917483329772949, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 59/516, D Loss: 0.4976591723971069, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 60/516, D Loss: 0.49889871827326715, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 61/516, D Loss: 0.49816184083465487, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 62/516, D Loss: 0.49304421059787273, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 63/516, D Loss: 0.49852420354727656, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 64/516, D Loss: 0.4980210717767477, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 65/516, D Loss: 0.4926521899178624, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 66/516, D Loss: 0.499317066045478, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 67/516, D Loss: 0.4989383651409298, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 68/516, D Loss: 0.4964722846634686, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 69/516, D Loss: 0.4921453492715955, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 70/516, D Loss: 0.496951543726027, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 71/516, D Loss: 0.49764343723654747, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 72/516, D Loss: 0.49510553758591413, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 73/516, D Loss: 0.4996751218859572, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 74/516, D Loss: 0.4916129820048809, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 75/516, D Loss: 0.48305890522897243, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 76/516, D Loss: 0.498386844759807, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 77/516, D Loss: 0.4969855803065002, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 78/516, D Loss: 0.4995981734828092, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 79/516, D Loss: 0.49091146793216467, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 80/516, D Loss: 0.4903464652597904, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 81/516, D Loss: 0.4894085843116045, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 82/516, D Loss: 0.4982499268371612, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 83/516, D Loss: 0.49478630209341645, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 84/516, D Loss: 0.49936523084761575, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 85/516, D Loss: 0.4925274942070246, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 86/516, D Loss: 0.49828574049752206, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 87/516, D Loss: 0.4915224891155958, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 88/516, D Loss: 0.4992193710641004, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 89/516, D Loss: 0.4945745402947068, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 90/516, D Loss: 0.4994003754109144, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 91/516, D Loss: 0.49462653044611216, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 92/516, D Loss: 0.4983357930323109, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 93/516, D Loss: 0.49513908941298723, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 94/516, D Loss: 0.4928389359265566, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 95/516, D Loss: 0.48621615022420883, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 96/516, D Loss: 0.48866562731564045, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 97/516, D Loss: 0.4944429346360266, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 98/516, D Loss: 0.4973223083652556, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 99/516, D Loss: 0.4952356917783618, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 100/516, D Loss: 0.4919151160866022, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 101/516, D Loss: 0.49125683307647705, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 102/516, D Loss: 0.49716210807673633, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 103/516, D Loss: 0.4956218972802162, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 104/516, D Loss: 0.49842534761410207, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 105/516, D Loss: 0.49937641475116834, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 106/516, D Loss: 0.4954461017623544, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 107/516, D Loss: 0.4980429830029607, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 108/516, D Loss: 0.4923092359676957, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 109/516, D Loss: 0.4937150441110134, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 110/516, D Loss: 0.4959965702146292, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 111/516, D Loss: 0.4822582844644785, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 112/516, D Loss: 0.4851339031010866, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 113/516, D Loss: 0.4957854407839477, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 114/516, D Loss: 0.4942894969135523, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 115/516, D Loss: 0.493075355887413, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 116/516, D Loss: 0.4993575020926073, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 117/516, D Loss: 0.4982469752430916, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 118/516, D Loss: 0.4904886092990637, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 119/516, D Loss: 0.49900688289199024, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 120/516, D Loss: 0.495389056392014, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 121/516, D Loss: 0.4938288643024862, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 122/516, D Loss: 0.48710061237215996, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 123/516, D Loss: 0.49940620572306216, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 124/516, D Loss: 0.4975289637222886, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 125/516, D Loss: 0.4881672039628029, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 126/516, D Loss: 0.499178814177867, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 127/516, D Loss: 0.49343133671209216, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 128/516, D Loss: 0.49953469380852766, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 129/516, D Loss: 0.48856454342603683, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 130/516, D Loss: 0.4926503044553101, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 131/516, D Loss: 0.49807692144531757, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 132/516, D Loss: 0.49852344300597906, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 133/516, D Loss: 0.495258956681937, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 134/516, D Loss: 0.4954966902732849, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 135/516, D Loss: 0.4952273992821574, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 136/516, D Loss: 0.48026988096535206, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 137/516, D Loss: 0.4952500252984464, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 138/516, D Loss: 0.4905201317742467, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 139/516, D Loss: 0.486931124702096, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 140/516, D Loss: 0.49807703541591763, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 141/516, D Loss: 0.4936540983617306, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 142/516, D Loss: 0.4949195650406182, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 143/516, D Loss: 0.4955140561796725, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 144/516, D Loss: 0.4914617883041501, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 145/516, D Loss: 0.4947643247433007, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 146/516, D Loss: 0.499103486770764, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 147/516, D Loss: 0.49907994212117046, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 148/516, D Loss: 0.49143843073397875, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 149/516, D Loss: 0.48821026273071766, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 150/516, D Loss: 0.4928557421080768, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 151/516, D Loss: 0.4934356645680964, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 152/516, D Loss: 0.49944870464969426, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 153/516, D Loss: 0.4928857907652855, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 154/516, D Loss: 0.4888989804312587, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 155/516, D Loss: 0.4913159739226103, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 156/516, D Loss: 0.484366899356246, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 157/516, D Loss: 0.4986956153297797, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 158/516, D Loss: 0.49735282780602574, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 159/516, D Loss: 0.4974501107353717, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 160/516, D Loss: 0.4953448325395584, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 161/516, D Loss: 0.4995463039376773, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 162/516, D Loss: 0.49749576300382614, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 163/516, D Loss: 0.4982679641107097, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 164/516, D Loss: 0.49372061155736446, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 165/516, D Loss: 0.495411217212677, G Loss: -0.9999997019767761\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 166/516, D Loss: 0.4961362003814429, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 167/516, D Loss: 0.49865659745410085, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 168/516, D Loss: 0.49929682567017153, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 169/516, D Loss: 0.4939685519784689, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 170/516, D Loss: 0.48505161330103874, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 171/516, D Loss: 0.4880364406853914, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 172/516, D Loss: 0.49602519255131483, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 173/516, D Loss: 0.499372357618995, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 174/516, D Loss: 0.4943669242784381, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 175/516, D Loss: 0.4837423376739025, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 176/516, D Loss: 0.49149818532168865, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 177/516, D Loss: 0.4969261798541993, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 178/516, D Loss: 0.4992837617173791, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 179/516, D Loss: 0.4954235013574362, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 180/516, D Loss: 0.49757425114512444, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 181/516, D Loss: 0.4990051173372194, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 182/516, D Loss: 0.495730172842741, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 183/516, D Loss: 0.4884916581213474, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 184/516, D Loss: 0.4909132346510887, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 185/516, D Loss: 0.4948991681449115, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 186/516, D Loss: 0.4997339414258022, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 187/516, D Loss: 0.49914051668019965, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 188/516, D Loss: 0.4963151444680989, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 189/516, D Loss: 0.48995836824178696, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 190/516, D Loss: 0.4957734942436218, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 191/516, D Loss: 0.49958946017432027, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 192/516, D Loss: 0.49524184595793486, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 193/516, D Loss: 0.48464223090559244, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 194/516, D Loss: 0.4960068822838366, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 195/516, D Loss: 0.4917005440220237, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 196/516, D Loss: 0.4894292429089546, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 197/516, D Loss: 0.498740749200806, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 198/516, D Loss: 0.4974771768320352, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 199/516, D Loss: 0.49200531747192144, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 200/516, D Loss: 0.48188966512680054, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 201/516, D Loss: 0.49432193627581, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 202/516, D Loss: 0.4957703850232065, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 203/516, D Loss: 0.498900706297718, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 204/516, D Loss: 0.49242666456848383, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 205/516, D Loss: 0.4987267457181588, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 206/516, D Loss: 0.4923525075428188, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 207/516, D Loss: 0.4907387010753155, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 208/516, D Loss: 0.49840983923058957, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 209/516, D Loss: 0.4821444172412157, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 210/516, D Loss: 0.4945714776404202, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 211/516, D Loss: 0.4981965629849583, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 212/516, D Loss: 0.4811149165034294, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 213/516, D Loss: 0.49346516840159893, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 214/516, D Loss: 0.4972183075733483, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 215/516, D Loss: 0.49574053613469005, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 216/516, D Loss: 0.49421956576406956, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 217/516, D Loss: 0.4867314165458083, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 218/516, D Loss: 0.49738233583047986, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 219/516, D Loss: 0.4980971263721585, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 220/516, D Loss: 0.48658081237226725, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 221/516, D Loss: 0.4941869559697807, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 222/516, D Loss: 0.49502317421138287, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 223/516, D Loss: 0.4920956427231431, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 224/516, D Loss: 0.4931137771345675, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 225/516, D Loss: 0.4947392242029309, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 226/516, D Loss: 0.49271646328270435, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 227/516, D Loss: 0.49123453721404076, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 228/516, D Loss: 0.4987569614313543, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 229/516, D Loss: 0.4882360342890024, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 230/516, D Loss: 0.4953638152219355, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 231/516, D Loss: 0.49669019831344485, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 232/516, D Loss: 0.4882833594456315, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 233/516, D Loss: 0.49729068484157324, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 234/516, D Loss: 0.49689476983621716, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 235/516, D Loss: 0.4857225939631462, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 236/516, D Loss: 0.49762755003757775, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 237/516, D Loss: 0.4871665183454752, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 238/516, D Loss: 0.49562488961964846, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 239/516, D Loss: 0.49301604088395834, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 240/516, D Loss: 0.4950731312856078, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 241/516, D Loss: 0.4895466351881623, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 242/516, D Loss: 0.4976292836945504, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 243/516, D Loss: 0.49815522076096386, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 244/516, D Loss: 0.49530906043946743, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 245/516, D Loss: 0.49954641179647297, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 246/516, D Loss: 0.48851355258375406, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 247/516, D Loss: 0.49770578369498253, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 248/516, D Loss: 0.49822826962918043, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 249/516, D Loss: 0.48165497183799744, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 250/516, D Loss: 0.49573358707129955, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 251/516, D Loss: 0.49920513085089624, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 252/516, D Loss: 0.4950732500292361, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 253/516, D Loss: 0.49712269846349955, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 254/516, D Loss: 0.4921298734843731, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 255/516, D Loss: 0.49102557450532913, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 256/516, D Loss: 0.49543679784983397, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 257/516, D Loss: 0.4856165563687682, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 258/516, D Loss: 0.49810300732497126, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 259/516, D Loss: 0.4956071935594082, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 260/516, D Loss: 0.48872219771146774, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 261/516, D Loss: 0.48919717501848936, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 262/516, D Loss: 0.49536077538505197, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 263/516, D Loss: 0.49728339607827365, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 264/516, D Loss: 0.49483919283375144, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 265/516, D Loss: 0.4993354332400486, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 266/516, D Loss: 0.49050653725862503, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 267/516, D Loss: 0.4968600154388696, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 268/516, D Loss: 0.48725381307303905, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 269/516, D Loss: 0.494915665127337, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 270/516, D Loss: 0.4926790678873658, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 271/516, D Loss: 0.4901094678789377, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 272/516, D Loss: 0.499340801325161, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 273/516, D Loss: 0.4942948133684695, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 274/516, D Loss: 0.49289345368742943, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 275/516, D Loss: 0.49385801423341036, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 276/516, D Loss: 0.4967570537701249, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 277/516, D Loss: 0.4956861613318324, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 278/516, D Loss: 0.49965459897066467, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 279/516, D Loss: 0.4996671069820877, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 280/516, D Loss: 0.492364545352757, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 281/516, D Loss: 0.4997035649139434, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 282/516, D Loss: 0.48393446765840054, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 283/516, D Loss: 0.48933089803904295, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 284/516, D Loss: 0.49836921866517514, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 285/516, D Loss: 0.4952898663468659, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 286/516, D Loss: 0.4917384432628751, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 287/516, D Loss: 0.490768451243639, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 288/516, D Loss: 0.4838607758283615, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 289/516, D Loss: 0.4850859884172678, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 290/516, D Loss: 0.4959355294704437, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 291/516, D Loss: 0.4954031081870198, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 292/516, D Loss: 0.4920617053285241, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 293/516, D Loss: 0.4923396585509181, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 294/516, D Loss: 0.4835228808224201, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 295/516, D Loss: 0.4957141145132482, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 296/516, D Loss: 0.4984617242589593, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 297/516, D Loss: 0.49124675430357456, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 298/516, D Loss: 0.4891922250390053, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 299/516, D Loss: 0.49389606015756726, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 300/516, D Loss: 0.4967903501819819, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 301/516, D Loss: 0.49001077376306057, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 302/516, D Loss: 0.4955413076095283, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 303/516, D Loss: 0.4880117066204548, G Loss: -0.9999996423721313\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 304/516, D Loss: 0.49574167653918266, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 305/516, D Loss: 0.4937305934727192, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 306/516, D Loss: 0.4952410524711013, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 307/516, D Loss: 0.49707075604237616, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 308/516, D Loss: 0.49949677736731246, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 309/516, D Loss: 0.48649662360548973, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 310/516, D Loss: 0.4936245051212609, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 311/516, D Loss: 0.4861964089795947, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 312/516, D Loss: 0.49507120344787836, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 313/516, D Loss: 0.4996437357622199, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 314/516, D Loss: 0.4914678754284978, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 315/516, D Loss: 0.48957390151917934, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 316/516, D Loss: 0.4835636503994465, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 317/516, D Loss: 0.49746780819259584, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 318/516, D Loss: 0.4990250749979168, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 319/516, D Loss: 0.49055307265371084, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 320/516, D Loss: 0.4911908498033881, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 321/516, D Loss: 0.49935771111631766, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 322/516, D Loss: 0.4922040207311511, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 323/516, D Loss: 0.489756828173995, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 324/516, D Loss: 0.4939988008700311, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 325/516, D Loss: 0.4931427394039929, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 326/516, D Loss: 0.4995007887482643, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 327/516, D Loss: 0.49366925517097116, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 328/516, D Loss: 0.48096982575953007, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 329/516, D Loss: 0.49928213004022837, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 330/516, D Loss: 0.4936223034746945, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 331/516, D Loss: 0.4989289757795632, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 332/516, D Loss: 0.49184424988925457, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 333/516, D Loss: 0.4914231998845935, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 334/516, D Loss: 0.4994302070699632, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 335/516, D Loss: 0.4982079000910744, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 336/516, D Loss: 0.49971677115536295, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 337/516, D Loss: 0.49797406466677785, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 338/516, D Loss: 0.4953634813427925, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 339/516, D Loss: 0.4949195352382958, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 340/516, D Loss: 0.482703959569335, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 341/516, D Loss: 0.4985327674075961, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 342/516, D Loss: 0.4915148764848709, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 343/516, D Loss: 0.4992627830361016, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 344/516, D Loss: 0.49860774737317115, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 345/516, D Loss: 0.4982111647259444, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 346/516, D Loss: 0.47904815897345543, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 347/516, D Loss: 0.4994258830556646, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 348/516, D Loss: 0.4991264307172969, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 349/516, D Loss: 0.49467454152181745, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 350/516, D Loss: 0.49101204331964254, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 351/516, D Loss: 0.499822084166226, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 352/516, D Loss: 0.4978621762711555, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 353/516, D Loss: 0.4957926031202078, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 354/516, D Loss: 0.48385852202773094, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 355/516, D Loss: 0.49787619756534696, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 356/516, D Loss: 0.49852194963023067, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 357/516, D Loss: 0.48853233736008406, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 358/516, D Loss: 0.48299382999539375, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 359/516, D Loss: 0.4969639517366886, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 360/516, D Loss: 0.4995104483095929, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 361/516, D Loss: 0.499494485615287, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 362/516, D Loss: 0.4948553713038564, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 363/516, D Loss: 0.499166184454225, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 364/516, D Loss: 0.49578020814806223, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 365/516, D Loss: 0.4992047842242755, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 366/516, D Loss: 0.49166602455079556, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 367/516, D Loss: 0.49915986094856635, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 368/516, D Loss: 0.4990445839939639, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 369/516, D Loss: 0.48941364977508783, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 370/516, D Loss: 0.4951000539585948, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 371/516, D Loss: 0.4990252975258045, G Loss: -0.9999997615814209\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 372/516, D Loss: 0.4931240687146783, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 373/516, D Loss: 0.49937484523979947, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 374/516, D Loss: 0.4976909193210304, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 375/516, D Loss: 0.4927242547273636, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 376/516, D Loss: 0.491162502206862, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 377/516, D Loss: 0.4906187327578664, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 378/516, D Loss: 0.49725629878230393, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 379/516, D Loss: 0.49499244382604957, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 380/516, D Loss: 0.49200909212231636, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 381/516, D Loss: 0.4935978399589658, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 382/516, D Loss: 0.49851282802410424, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 383/516, D Loss: 0.49062257446348667, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 384/516, D Loss: 0.49963031680090353, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 385/516, D Loss: 0.4845798956230283, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 386/516, D Loss: 0.49891287356149405, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 387/516, D Loss: 0.4919560169801116, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 388/516, D Loss: 0.49489133059978485, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 389/516, D Loss: 0.4874769579619169, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 390/516, D Loss: 0.49869555328041315, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 391/516, D Loss: 0.49429360684007406, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 392/516, D Loss: 0.48333126306533813, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 393/516, D Loss: 0.497451632283628, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 394/516, D Loss: 0.49690481647849083, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 395/516, D Loss: 0.492503241635859, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 396/516, D Loss: 0.49533100659027696, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 397/516, D Loss: 0.4972891779616475, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 398/516, D Loss: 0.4903440149500966, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 399/516, D Loss: 0.48809206020087004, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 400/516, D Loss: 0.48663877230137587, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 401/516, D Loss: 0.49489169381558895, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 402/516, D Loss: 0.49561822693794966, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 403/516, D Loss: 0.49540381832048297, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 404/516, D Loss: 0.48302749916911125, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 405/516, D Loss: 0.4995132255135104, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 406/516, D Loss: 0.4921197071671486, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 407/516, D Loss: 0.4920148206874728, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 408/516, D Loss: 0.49961406938382424, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 409/516, D Loss: 0.4984046088065952, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 410/516, D Loss: 0.4974601590074599, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 411/516, D Loss: 0.4981233251746744, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 412/516, D Loss: 0.49557917658239603, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 413/516, D Loss: 0.49493974167853594, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 414/516, D Loss: 0.49961657560197636, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 415/516, D Loss: 0.49152159318327904, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 416/516, D Loss: 0.48885519057512283, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 417/516, D Loss: 0.4995359070599079, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 418/516, D Loss: 0.49821111815981567, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 419/516, D Loss: 0.49191930890083313, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 420/516, D Loss: 0.4944221177138388, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 421/516, D Loss: 0.4974077641963959, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 422/516, D Loss: 0.495422872249037, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 423/516, D Loss: 0.4914009654894471, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 424/516, D Loss: 0.4956619651056826, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 425/516, D Loss: 0.4920330699533224, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 426/516, D Loss: 0.48660399578511715, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 427/516, D Loss: 0.49500588653609157, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 428/516, D Loss: 0.4990938328555785, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 429/516, D Loss: 0.4988822928862646, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 430/516, D Loss: 0.4869841989129782, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 431/516, D Loss: 0.4968174919486046, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 432/516, D Loss: 0.49997069917662884, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 433/516, D Loss: 0.4988113932777196, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 434/516, D Loss: 0.4941661115735769, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 435/516, D Loss: 0.4930837582796812, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 436/516, D Loss: 0.49980935630446766, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 437/516, D Loss: 0.4923071558587253, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 438/516, D Loss: 0.49467004369944334, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 439/516, D Loss: 0.4968632566742599, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 440/516, D Loss: 0.4877557549625635, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 441/516, D Loss: 0.49720116215758026, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 442/516, D Loss: 0.498671037144959, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 443/516, D Loss: 0.4977900399826467, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 444/516, D Loss: 0.49718510429374874, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 445/516, D Loss: 0.48009148240089417, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 446/516, D Loss: 0.4911686824634671, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 447/516, D Loss: 0.49087215680629015, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 448/516, D Loss: 0.4910877197980881, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 449/516, D Loss: 0.4992792451521382, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 450/516, D Loss: 0.4953391128219664, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 451/516, D Loss: 0.49890361027792096, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 452/516, D Loss: 0.49459945037961006, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 453/516, D Loss: 0.49557920079678297, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 454/516, D Loss: 0.49934843700611964, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 455/516, D Loss: 0.4987692255526781, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 456/516, D Loss: 0.4927397249266505, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 457/516, D Loss: 0.4891681717708707, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 458/516, D Loss: 0.4950303053483367, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 459/516, D Loss: 0.4971457328647375, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 460/516, D Loss: 0.48637777753174305, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 461/516, D Loss: 0.4911749418824911, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 462/516, D Loss: 0.4806559309363365, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 463/516, D Loss: 0.4855903424322605, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 464/516, D Loss: 0.49417567253112793, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 465/516, D Loss: 0.4952409057877958, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 466/516, D Loss: 0.4945734739303589, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 467/516, D Loss: 0.49702374869957566, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 468/516, D Loss: 0.49887367710471153, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 469/516, D Loss: 0.48574656900018454, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 470/516, D Loss: 0.4908643728122115, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 471/516, D Loss: 0.4926038687117398, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 472/516, D Loss: 0.4949221406131983, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 473/516, D Loss: 0.498597779776901, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 474/516, D Loss: 0.4885610584169626, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 475/516, D Loss: 0.4982654391787946, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 476/516, D Loss: 0.49354392057284713, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 477/516, D Loss: 0.4994817845290527, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 478/516, D Loss: 0.498790018260479, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 479/516, D Loss: 0.4988571845460683, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 480/516, D Loss: 0.49376095412299037, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 481/516, D Loss: 0.4991790273343213, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 482/516, D Loss: 0.4887642776593566, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 483/516, D Loss: 0.49412955436855555, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 484/516, D Loss: 0.49059934355318546, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 485/516, D Loss: 0.49263452738523483, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 486/516, D Loss: 0.4985675283242017, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 487/516, D Loss: 0.49387934571132064, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 488/516, D Loss: 0.49488911870867014, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 489/516, D Loss: 0.49964795348932967, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 490/516, D Loss: 0.4938307390548289, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 491/516, D Loss: 0.4991007271455601, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 492/516, D Loss: 0.49875743547454476, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 493/516, D Loss: 0.4956839024089277, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 494/516, D Loss: 0.4933626130223274, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 495/516, D Loss: 0.49251160956919193, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 496/516, D Loss: 0.4843709859997034, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 497/516, D Loss: 0.49797843024134636, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 498/516, D Loss: 0.4989386423258111, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 499/516, D Loss: 0.4946923037059605, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 500/516, D Loss: 0.4788813143968582, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 501/516, D Loss: 0.4977878713980317, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 502/516, D Loss: 0.49170773569494486, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 503/516, D Loss: 0.49871971434913576, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 504/516, D Loss: 0.4945653504692018, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 505/516, D Loss: 0.4937659972347319, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 506/516, D Loss: 0.4927161755040288, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 507/516, D Loss: 0.48759909346699715, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 508/516, D Loss: 0.486736211925745, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 509/516, D Loss: 0.4907955341041088, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 510/516, D Loss: 0.4869503453373909, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 511/516, D Loss: 0.4992693818639964, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 7, Batch 512/516, D Loss: 0.4994216596824117, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 513/516, D Loss: 0.4882259825244546, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 7, Batch 514/516, D Loss: 0.49313610466197133, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 7, Batch 515/516, D Loss: 0.4985880630556494, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 7, Batch 516/516, D Loss: 0.4954850496724248, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 1/516, D Loss: 0.4860146176069975, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 2/516, D Loss: 0.48816959746181965, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 8, Batch 3/516, D Loss: 0.4956707856617868, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 4/516, D Loss: 0.4990576688433066, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 5/516, D Loss: 0.4952811081893742, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 6/516, D Loss: 0.49264192301779985, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 7/516, D Loss: 0.4976595665793866, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 8/516, D Loss: 0.49704819824546576, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 9/516, D Loss: 0.4987484239973128, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 10/516, D Loss: 0.49831507389899343, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 11/516, D Loss: 0.4914651494473219, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 12/516, D Loss: 0.4955644872970879, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 13/516, D Loss: 0.4994742622366175, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 14/516, D Loss: 0.4952135174535215, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 15/516, D Loss: 0.4916773298755288, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 16/516, D Loss: 0.4948627664707601, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 17/516, D Loss: 0.49181649181991816, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 18/516, D Loss: 0.4914031410589814, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 19/516, D Loss: 0.4956996813416481, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 20/516, D Loss: 0.4914040993899107, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 21/516, D Loss: 0.4953476241789758, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 22/516, D Loss: 0.49810451606754214, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 23/516, D Loss: 0.49640430114232004, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 24/516, D Loss: 0.4904269389808178, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 25/516, D Loss: 0.49800135660916567, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 26/516, D Loss: 0.4995029685087502, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 27/516, D Loss: 0.4954167581163347, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 28/516, D Loss: 0.4880135664716363, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 29/516, D Loss: 0.4994181705988012, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 30/516, D Loss: 0.4990508530754596, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 31/516, D Loss: 0.49355866061523557, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 32/516, D Loss: 0.4850422032177448, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 33/516, D Loss: 0.4994823606684804, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 34/516, D Loss: 0.49378177616745234, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 35/516, D Loss: 0.4871201300993562, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 36/516, D Loss: 0.4920480530709028, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 37/516, D Loss: 0.48951074853539467, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 38/516, D Loss: 0.4877155087888241, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 39/516, D Loss: 0.4804860055446625, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 40/516, D Loss: 0.49938450718764216, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 41/516, D Loss: 0.4995094889891334, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 42/516, D Loss: 0.48139422945678234, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 43/516, D Loss: 0.49825075187254697, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 44/516, D Loss: 0.49866740475408733, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 45/516, D Loss: 0.4947067005559802, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 46/516, D Loss: 0.4934471370652318, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 47/516, D Loss: 0.49568571662530303, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 48/516, D Loss: 0.49359348602592945, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 49/516, D Loss: 0.4920512745156884, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 50/516, D Loss: 0.49528341786935925, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 51/516, D Loss: 0.4944586958736181, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 52/516, D Loss: 0.492929314263165, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 53/516, D Loss: 0.48736960254609585, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 54/516, D Loss: 0.49881725769955665, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 55/516, D Loss: 0.49945829063653946, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 56/516, D Loss: 0.49475991958752275, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 57/516, D Loss: 0.4945398014970124, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 58/516, D Loss: 0.49209201987832785, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 59/516, D Loss: 0.49961351676029153, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 60/516, D Loss: 0.4955882178619504, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 61/516, D Loss: 0.49968149929190986, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 62/516, D Loss: 0.4961162463296205, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 63/516, D Loss: 0.49218586925417185, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 64/516, D Loss: 0.4939254242926836, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 65/516, D Loss: 0.49309273762628436, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 66/516, D Loss: 0.49491624580696225, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 67/516, D Loss: 0.4983224568422884, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 68/516, D Loss: 0.49554124707356095, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 69/516, D Loss: 0.49431432131677866, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 70/516, D Loss: 0.494543285574764, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 71/516, D Loss: 0.48878856003284454, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 72/516, D Loss: 0.4933312078937888, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 73/516, D Loss: 0.4977434426546097, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 74/516, D Loss: 0.49853327055461705, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 75/516, D Loss: 0.4962565971072763, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 76/516, D Loss: 0.4953582310117781, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 77/516, D Loss: 0.48980408255010843, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 78/516, D Loss: 0.4989342533517629, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 79/516, D Loss: 0.4911801014095545, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 80/516, D Loss: 0.49786523659713566, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 81/516, D Loss: 0.4908843617886305, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 82/516, D Loss: 0.4948742208071053, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 83/516, D Loss: 0.493648674339056, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 84/516, D Loss: 0.49343918543308973, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 85/516, D Loss: 0.4996542847948149, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 86/516, D Loss: 0.4968950622715056, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 87/516, D Loss: 0.4885009601712227, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 88/516, D Loss: 0.4983333572745323, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 89/516, D Loss: 0.4947575959376991, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 90/516, D Loss: 0.49138746317476034, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 91/516, D Loss: 0.49882979656104, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 92/516, D Loss: 0.49970270149060525, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 93/516, D Loss: 0.4991531518753618, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 94/516, D Loss: 0.49468389293178916, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 95/516, D Loss: 0.4949432983994484, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 96/516, D Loss: 0.49971737404121086, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 97/516, D Loss: 0.4924282478168607, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 98/516, D Loss: 0.49324572365731, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 99/516, D Loss: 0.49910943064605817, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 100/516, D Loss: 0.4978116061538458, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 101/516, D Loss: 0.49065238144248724, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 102/516, D Loss: 0.48934500478208065, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 103/516, D Loss: 0.4860012633726001, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 104/516, D Loss: 0.4956078282557428, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 105/516, D Loss: 0.49519592989236116, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 106/516, D Loss: 0.4902343228459358, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 107/516, D Loss: 0.49475561035797, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 108/516, D Loss: 0.4946163361892104, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 109/516, D Loss: 0.49392046546563506, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 110/516, D Loss: 0.4978886893950403, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 111/516, D Loss: 0.49868818174581975, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 112/516, D Loss: 0.49517921125516295, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 113/516, D Loss: 0.49764391826465726, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 114/516, D Loss: 0.48072995245456696, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 115/516, D Loss: 0.49059934727847576, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 116/516, D Loss: 0.49262550892308354, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 117/516, D Loss: 0.4982542305951938, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 118/516, D Loss: 0.48751540295779705, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 119/516, D Loss: 0.4978659301996231, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 120/516, D Loss: 0.49182267021387815, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 121/516, D Loss: 0.49937339208554476, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 122/516, D Loss: 0.49879788514226675, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 123/516, D Loss: 0.481425991281867, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 124/516, D Loss: 0.4935548542998731, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 125/516, D Loss: 0.4848170094192028, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 126/516, D Loss: 0.49439114006236196, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 127/516, D Loss: 0.49930236767977476, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 128/516, D Loss: 0.4873416991904378, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 129/516, D Loss: 0.49584023747593164, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 130/516, D Loss: 0.49918766116024926, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 131/516, D Loss: 0.49511126335710287, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 132/516, D Loss: 0.4968651575036347, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 133/516, D Loss: 0.4953434318304062, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 134/516, D Loss: 0.49472605297341943, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 135/516, D Loss: 0.4893871136009693, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 136/516, D Loss: 0.4992121158284135, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 137/516, D Loss: 0.4949915185570717, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 138/516, D Loss: 0.4928036811761558, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 139/516, D Loss: 0.49510507099330425, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 140/516, D Loss: 0.48732963763177395, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 141/516, D Loss: 0.49553790129721165, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 142/516, D Loss: 0.4884534329175949, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 143/516, D Loss: 0.4948757980018854, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 144/516, D Loss: 0.49854088050778955, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 145/516, D Loss: 0.4912533340975642, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 146/516, D Loss: 0.48748145531862974, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 147/516, D Loss: 0.4906314779073, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 148/516, D Loss: 0.49089082051068544, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 149/516, D Loss: 0.49853297986555845, G Loss: -0.9999998211860657\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 150/516, D Loss: 0.49499526899307966, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 151/516, D Loss: 0.49462118465453386, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 152/516, D Loss: 0.49946773052215576, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 153/516, D Loss: 0.4952694005332887, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 154/516, D Loss: 0.499551021552179, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 155/516, D Loss: 0.4957032287493348, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 156/516, D Loss: 0.48745032493025064, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 157/516, D Loss: 0.49916946107987314, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 158/516, D Loss: 0.4947559959255159, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 159/516, D Loss: 0.4907648582011461, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 160/516, D Loss: 0.4981011411873624, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 161/516, D Loss: 0.4993765613762662, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 162/516, D Loss: 0.4920227834954858, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 163/516, D Loss: 0.49190292600542307, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 164/516, D Loss: 0.49432934587821364, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 165/516, D Loss: 0.47612059488892555, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 166/516, D Loss: 0.49495363188907504, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 167/516, D Loss: 0.495358232408762, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 168/516, D Loss: 0.49556363513693213, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 169/516, D Loss: 0.4907934395596385, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 170/516, D Loss: 0.4929468841291964, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 171/516, D Loss: 0.4826998673379421, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 172/516, D Loss: 0.4936868050135672, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 173/516, D Loss: 0.49324524542316794, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 174/516, D Loss: 0.488458763808012, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 175/516, D Loss: 0.4894607048481703, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 176/516, D Loss: 0.4944385960698128, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 177/516, D Loss: 0.49433637550100684, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 178/516, D Loss: 0.4886274794116616, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 179/516, D Loss: 0.49757656431756914, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 180/516, D Loss: 0.4915589364245534, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 181/516, D Loss: 0.495527695864439, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 182/516, D Loss: 0.4967961290385574, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 183/516, D Loss: 0.49775029555894434, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 184/516, D Loss: 0.49980671584489755, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 185/516, D Loss: 0.49139756988734007, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 186/516, D Loss: 0.4954078425653279, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 187/516, D Loss: 0.4998714414105052, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 188/516, D Loss: 0.49514595372602344, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 189/516, D Loss: 0.49090343434363604, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 190/516, D Loss: 0.4989583204733208, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 191/516, D Loss: 0.49196578189730644, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 192/516, D Loss: 0.4909997954964638, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 193/516, D Loss: 0.4967322670854628, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 194/516, D Loss: 0.49434397788718343, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 195/516, D Loss: 0.49446365470066667, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 196/516, D Loss: 0.498108574654907, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 197/516, D Loss: 0.4995550649764482, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 198/516, D Loss: 0.47899709455668926, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 199/516, D Loss: 0.49780403007753193, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 200/516, D Loss: 0.49892535735853016, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 201/516, D Loss: 0.48779469449073076, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 202/516, D Loss: 0.4893891680985689, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 203/516, D Loss: 0.4954785732552409, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 204/516, D Loss: 0.49959558399859816, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 205/516, D Loss: 0.48755494970828295, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 206/516, D Loss: 0.4805116653442383, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 207/516, D Loss: 0.49576148902997375, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 208/516, D Loss: 0.4950930429622531, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 209/516, D Loss: 0.4956126194447279, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 210/516, D Loss: 0.4913497669622302, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 211/516, D Loss: 0.49436521949246526, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 212/516, D Loss: 0.49157870560884476, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 213/516, D Loss: 0.4986952708568424, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 214/516, D Loss: 0.49881499051116407, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 215/516, D Loss: 0.4854924827814102, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 216/516, D Loss: 0.49519895017147064, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 217/516, D Loss: 0.4993982397718355, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 218/516, D Loss: 0.49111437052488327, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 219/516, D Loss: 0.4846018236130476, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 220/516, D Loss: 0.4949108101427555, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 221/516, D Loss: 0.49531410867348313, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 222/516, D Loss: 0.49967163483961485, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 223/516, D Loss: 0.4982809841167182, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 224/516, D Loss: 0.4982631674502045, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 225/516, D Loss: 0.4897295478731394, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 226/516, D Loss: 0.48747032042592764, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 8, Batch 227/516, D Loss: 0.4944169265218079, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 228/516, D Loss: 0.4948062947951257, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 229/516, D Loss: 0.49566480284556746, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 230/516, D Loss: 0.49469581292942166, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 231/516, D Loss: 0.47729629650712013, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 232/516, D Loss: 0.4904723633080721, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 233/516, D Loss: 0.49767059623263776, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 234/516, D Loss: 0.4946790928952396, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 235/516, D Loss: 0.48511756770312786, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 236/516, D Loss: 0.49892676435410976, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 237/516, D Loss: 0.4988222747342661, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 238/516, D Loss: 0.49478621361777186, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 239/516, D Loss: 0.4985889912350103, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 240/516, D Loss: 0.4905853336676955, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 241/516, D Loss: 0.4959530425257981, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 242/516, D Loss: 0.49756237817928195, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 243/516, D Loss: 0.4928836110047996, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 244/516, D Loss: 0.4958493784070015, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 245/516, D Loss: 0.4945009732618928, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 246/516, D Loss: 0.4957571206614375, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 247/516, D Loss: 0.49591609789058566, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 248/516, D Loss: 0.4980076311621815, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 249/516, D Loss: 0.49103870429098606, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 250/516, D Loss: 0.4889443116262555, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 251/516, D Loss: 0.49797632615081966, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 252/516, D Loss: 0.49386212322860956, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 253/516, D Loss: 0.49626975785940886, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 254/516, D Loss: 0.4916204046458006, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 255/516, D Loss: 0.4846005057916045, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 256/516, D Loss: 0.4931420208886266, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 257/516, D Loss: 0.4876819057390094, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 258/516, D Loss: 0.48802823200821877, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 259/516, D Loss: 0.491280410438776, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 260/516, D Loss: 0.49167686607688665, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 261/516, D Loss: 0.4954054052941501, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 262/516, D Loss: 0.49971238637226634, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 263/516, D Loss: 0.4990160590969026, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 264/516, D Loss: 0.49551939219236374, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 265/516, D Loss: 0.4869271507486701, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 266/516, D Loss: 0.48487892001867294, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 267/516, D Loss: 0.4931640811264515, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 268/516, D Loss: 0.4948023818433285, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 269/516, D Loss: 0.49584618769586086, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 270/516, D Loss: 0.4871709244325757, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 271/516, D Loss: 0.4959899918176234, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 272/516, D Loss: 0.4988490843679756, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 8, Batch 273/516, D Loss: 0.4931087833829224, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 274/516, D Loss: 0.49510546028614044, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 275/516, D Loss: 0.4933081902563572, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 276/516, D Loss: 0.4953417060896754, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 277/516, D Loss: 0.4880885789170861, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 278/516, D Loss: 0.48949337378144264, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 279/516, D Loss: 0.4986009866697714, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 280/516, D Loss: 0.4987968420609832, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 281/516, D Loss: 0.4995202344143763, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 282/516, D Loss: 0.49590506590902805, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 283/516, D Loss: 0.49951970274560153, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 284/516, D Loss: 0.49428913183510303, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 8, Batch 285/516, D Loss: 0.4857721170410514, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 286/516, D Loss: 0.4808224570006132, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 287/516, D Loss: 0.4996883094427176, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 288/516, D Loss: 0.4985507163219154, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 289/516, D Loss: 0.48626344557851553, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 290/516, D Loss: 0.4911126121878624, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 291/516, D Loss: 0.4966049245558679, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 292/516, D Loss: 0.4929442103020847, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 293/516, D Loss: 0.49588477052748203, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 294/516, D Loss: 0.49427947448566556, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 295/516, D Loss: 0.49934662150917575, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 296/516, D Loss: 0.49502874352037907, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 297/516, D Loss: 0.4989119980018586, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 298/516, D Loss: 0.4990145663032308, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 299/516, D Loss: 0.49365365179255605, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 300/516, D Loss: 0.4990246890229173, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 301/516, D Loss: 0.49880853621289134, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 302/516, D Loss: 0.49894073884934187, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 303/516, D Loss: 0.49411086458712816, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 304/516, D Loss: 0.49910421116510406, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 305/516, D Loss: 0.4984675586456433, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 306/516, D Loss: 0.4904298111796379, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 307/516, D Loss: 0.4983127914601937, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 308/516, D Loss: 0.4969246124383062, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 309/516, D Loss: 0.49086822755634785, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 310/516, D Loss: 0.4947337508201599, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 311/516, D Loss: 0.4947039857506752, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 312/516, D Loss: 0.4980976404622197, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 313/516, D Loss: 0.49500522017478943, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 314/516, D Loss: 0.4985608758870512, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 315/516, D Loss: 0.49569477839395404, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 316/516, D Loss: 0.49533371068537235, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 317/516, D Loss: 0.48513935785740614, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 318/516, D Loss: 0.49917883990565315, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 319/516, D Loss: 0.49958845117362216, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 320/516, D Loss: 0.4990612076362595, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 321/516, D Loss: 0.4994749036268331, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 322/516, D Loss: 0.4959813077002764, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 323/516, D Loss: 0.49843100854195654, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 324/516, D Loss: 0.4987212276319042, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 325/516, D Loss: 0.49563284078612924, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 326/516, D Loss: 0.48787317145615816, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 327/516, D Loss: 0.4926723982207477, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 328/516, D Loss: 0.4934915592893958, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 329/516, D Loss: 0.49563897121697664, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 330/516, D Loss: 0.4986717876745388, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 331/516, D Loss: 0.49498790968209505, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 332/516, D Loss: 0.4986574900103733, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 333/516, D Loss: 0.497772335074842, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 334/516, D Loss: 0.4895023722201586, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 335/516, D Loss: 0.4955457290634513, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 336/516, D Loss: 0.49533108714967966, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 337/516, D Loss: 0.4986648600315675, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 338/516, D Loss: 0.4901995388790965, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 339/516, D Loss: 0.48685481864959, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 340/516, D Loss: 0.4933704030700028, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 341/516, D Loss: 0.49842187913600355, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 342/516, D Loss: 0.49457664834335446, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 343/516, D Loss: 0.4920079717412591, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 344/516, D Loss: 0.49452014453709126, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 345/516, D Loss: 0.4981876784004271, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 346/516, D Loss: 0.4909619837999344, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 347/516, D Loss: 0.48994603753089905, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 348/516, D Loss: 0.49857013660948724, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 349/516, D Loss: 0.49906393472338095, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 350/516, D Loss: 0.49428024142980576, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 351/516, D Loss: 0.4982754939701408, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 352/516, D Loss: 0.48782416619360447, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 353/516, D Loss: 0.49582238448783755, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 354/516, D Loss: 0.48605429753661156, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 355/516, D Loss: 0.49489593086764216, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 356/516, D Loss: 0.4947468535974622, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 357/516, D Loss: 0.4896774599328637, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 358/516, D Loss: 0.49107710644602776, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 359/516, D Loss: 0.4964827219955623, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 360/516, D Loss: 0.490555252879858, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 361/516, D Loss: 0.48967150412499905, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 362/516, D Loss: 0.49434523237869143, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 363/516, D Loss: 0.4989453051239252, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 8, Batch 364/516, D Loss: 0.4953424734994769, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 365/516, D Loss: 0.49117399752140045, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 366/516, D Loss: 0.4914437383413315, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 367/516, D Loss: 0.4921242808923125, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 368/516, D Loss: 0.49625040451064706, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 369/516, D Loss: 0.4991203440586105, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 370/516, D Loss: 0.4826106820255518, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 371/516, D Loss: 0.49510273011401296, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 372/516, D Loss: 0.49118276219815016, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 373/516, D Loss: 0.4929121225140989, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 374/516, D Loss: 0.49986771935073193, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 375/516, D Loss: 0.49453592114150524, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 376/516, D Loss: 0.487269027158618, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 8, Batch 377/516, D Loss: 0.4962154310196638, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 378/516, D Loss: 0.4996072775684297, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 379/516, D Loss: 0.4932103753089905, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 380/516, D Loss: 0.4829293619841337, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 381/516, D Loss: 0.4940962716937065, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 382/516, D Loss: 0.49690703907981515, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 383/516, D Loss: 0.4911979166790843, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 384/516, D Loss: 0.482435105368495, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 385/516, D Loss: 0.4980059894733131, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 386/516, D Loss: 0.4912364883348346, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 387/516, D Loss: 0.4901078809052706, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 388/516, D Loss: 0.49910277844173834, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 389/516, D Loss: 0.4986776439473033, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 390/516, D Loss: 0.4890922736376524, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 391/516, D Loss: 0.4949351316317916, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 392/516, D Loss: 0.4971210309304297, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 393/516, D Loss: 0.4979401994496584, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 394/516, D Loss: 0.4902205169200897, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 395/516, D Loss: 0.49492752319201827, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 396/516, D Loss: 0.4954606438986957, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 397/516, D Loss: 0.4992900177021511, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 398/516, D Loss: 0.4920064778998494, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 399/516, D Loss: 0.4990971302613616, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 400/516, D Loss: 0.49976715094817337, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 401/516, D Loss: 0.499162292922847, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 402/516, D Loss: 0.4932812084443867, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 403/516, D Loss: 0.494586281478405, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 404/516, D Loss: 0.49958207106101327, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 405/516, D Loss: 0.49659203831106424, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 406/516, D Loss: 0.48983122780919075, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 407/516, D Loss: 0.4877212792634964, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 408/516, D Loss: 0.4921035831794143, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 409/516, D Loss: 0.49425119906663895, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 410/516, D Loss: 0.49345958698540926, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 411/516, D Loss: 0.49031368270516396, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 412/516, D Loss: 0.48788725957274437, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 413/516, D Loss: 0.49717600643634796, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 414/516, D Loss: 0.499762719089631, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 415/516, D Loss: 0.4892942663282156, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 416/516, D Loss: 0.4938942869193852, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 417/516, D Loss: 0.4927997374907136, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 418/516, D Loss: 0.4993456430383958, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 419/516, D Loss: 0.49169215746223927, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 420/516, D Loss: 0.4957403768785298, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 421/516, D Loss: 0.49216508865356445, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 422/516, D Loss: 0.49590569781139493, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 423/516, D Loss: 0.4931462104432285, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 424/516, D Loss: 0.49129656329751015, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 425/516, D Loss: 0.4946327265352011, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 426/516, D Loss: 0.4886932419613004, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 427/516, D Loss: 0.4845844078809023, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 428/516, D Loss: 0.49763788864947855, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 429/516, D Loss: 0.4861173778772354, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 430/516, D Loss: 0.49038159660995007, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 431/516, D Loss: 0.4898163955658674, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 432/516, D Loss: 0.49833721888717264, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 433/516, D Loss: 0.4897886700928211, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 434/516, D Loss: 0.49856673646718264, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 435/516, D Loss: 0.49902102956548333, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 436/516, D Loss: 0.48867045249789953, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 437/516, D Loss: 0.48571949638426304, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 438/516, D Loss: 0.49763621343299747, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 439/516, D Loss: 0.49830723786726594, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 440/516, D Loss: 0.4887327067553997, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 441/516, D Loss: 0.49685739586129785, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 442/516, D Loss: 0.49976064058137126, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 443/516, D Loss: 0.4977713688276708, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 444/516, D Loss: 0.49955741537269205, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 445/516, D Loss: 0.4981763979885727, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 446/516, D Loss: 0.49498963356018066, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 447/516, D Loss: 0.4881320334970951, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 448/516, D Loss: 0.4955390254035592, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 449/516, D Loss: 0.49595012376084924, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 450/516, D Loss: 0.4954462694004178, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 451/516, D Loss: 0.487675410695374, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 452/516, D Loss: 0.4936905037611723, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 453/516, D Loss: 0.4883246961981058, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 454/516, D Loss: 0.4918458377942443, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 455/516, D Loss: 0.49414587393403053, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 456/516, D Loss: 0.4961810999084264, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 457/516, D Loss: 0.49571821046993136, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 458/516, D Loss: 0.48738094605505466, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 459/516, D Loss: 0.49831775145139545, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 460/516, D Loss: 0.4968328378163278, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 461/516, D Loss: 0.4990086134057492, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 462/516, D Loss: 0.49376193480566144, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 463/516, D Loss: 0.4995738238794729, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 464/516, D Loss: 0.4817572943866253, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 465/516, D Loss: 0.4970633767079562, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 466/516, D Loss: 0.4993197413859889, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 467/516, D Loss: 0.49415634712204337, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 468/516, D Loss: 0.49424260249361396, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 469/516, D Loss: 0.4952662531286478, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 470/516, D Loss: 0.49866387713700533, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 471/516, D Loss: 0.49544420279562473, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 472/516, D Loss: 0.4903044914826751, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 473/516, D Loss: 0.4944667425006628, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 474/516, D Loss: 0.4815116636455059, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 475/516, D Loss: 0.48947896622121334, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 476/516, D Loss: 0.49573579616844654, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 477/516, D Loss: 0.49545331997796893, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 478/516, D Loss: 0.49722669553011656, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 479/516, D Loss: 0.49864699330646545, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 480/516, D Loss: 0.49799928977154195, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 481/516, D Loss: 0.49406685261055827, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 482/516, D Loss: 0.49933396792039275, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 483/516, D Loss: 0.4932572180405259, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 484/516, D Loss: 0.49403677927330136, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 485/516, D Loss: 0.49808112729806453, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 486/516, D Loss: 0.49945199012290686, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 487/516, D Loss: 0.4900649366900325, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 488/516, D Loss: 0.4931607390753925, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 489/516, D Loss: 0.495267360471189, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 490/516, D Loss: 0.4904835419729352, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 491/516, D Loss: 0.4854311440140009, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 492/516, D Loss: 0.493798254057765, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 8, Batch 493/516, D Loss: 0.4940249305218458, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 494/516, D Loss: 0.49347765604034066, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 495/516, D Loss: 0.4944590316154063, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 496/516, D Loss: 0.4906120980158448, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 497/516, D Loss: 0.4987065790919587, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 498/516, D Loss: 0.4947905424050987, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 499/516, D Loss: 0.49250707402825356, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 500/516, D Loss: 0.4954531220719218, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 501/516, D Loss: 0.4933733851648867, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 502/516, D Loss: 0.4985078191384673, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 503/516, D Loss: 0.49991775046510156, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 8, Batch 504/516, D Loss: 0.4987248219549656, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 505/516, D Loss: 0.49919236928690225, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 506/516, D Loss: 0.4879072215408087, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 507/516, D Loss: 0.49926014512311667, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 508/516, D Loss: 0.48649896308779716, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 509/516, D Loss: 0.48427318781614304, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 510/516, D Loss: 0.4952740091830492, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 511/516, D Loss: 0.49776747007854283, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 512/516, D Loss: 0.49531716480851173, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 513/516, D Loss: 0.48714106529951096, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 514/516, D Loss: 0.49895911349449307, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 8, Batch 515/516, D Loss: 0.4958308357745409, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 8, Batch 516/516, D Loss: 0.490756350569427, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 1/516, D Loss: 0.4950180174782872, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 2/516, D Loss: 0.4923926293849945, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 3/516, D Loss: 0.4913632683455944, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 4/516, D Loss: 0.4949099696241319, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 5/516, D Loss: 0.4993468578904867, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 6/516, D Loss: 0.4959376356564462, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 7/516, D Loss: 0.4843804044649005, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 8/516, D Loss: 0.4945852062664926, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 9/516, D Loss: 0.4873892245814204, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 10/516, D Loss: 0.48972376994788647, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 11/516, D Loss: 0.4932047100737691, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 12/516, D Loss: 0.49669721396639943, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 13/516, D Loss: 0.49216306395828724, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 14/516, D Loss: 0.4988315731752664, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 15/516, D Loss: 0.4943724391050637, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 16/516, D Loss: 0.4911188008263707, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 17/516, D Loss: 0.49251194950193167, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 18/516, D Loss: 0.49073976930230856, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 19/516, D Loss: 0.4994248647708446, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 20/516, D Loss: 0.493115097284317, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 21/516, D Loss: 0.49872553802561015, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 22/516, D Loss: 0.4937396375462413, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 23/516, D Loss: 0.4994282443076372, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 24/516, D Loss: 0.4975307446438819, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 25/516, D Loss: 0.49693273729644716, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 26/516, D Loss: 0.48343217745423317, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 27/516, D Loss: 0.49549078987911344, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 28/516, D Loss: 0.49166553653776646, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 29/516, D Loss: 0.48800311610102654, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 30/516, D Loss: 0.49796309089288116, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 31/516, D Loss: 0.48994954861700535, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 32/516, D Loss: 0.49214204400777817, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 33/516, D Loss: 0.4928544876165688, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 34/516, D Loss: 0.4993803385295905, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 35/516, D Loss: 0.4909315714612603, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 36/516, D Loss: 0.4904713584110141, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 37/516, D Loss: 0.4946643216535449, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 38/516, D Loss: 0.4911585049703717, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 39/516, D Loss: 0.4819279611110687, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 40/516, D Loss: 0.49895793781615794, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 41/516, D Loss: 0.49608216201886535, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 42/516, D Loss: 0.4976054090075195, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 43/516, D Loss: 0.4914889968931675, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 44/516, D Loss: 0.49898288829717785, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 45/516, D Loss: 0.49586982605978847, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 46/516, D Loss: 0.4982257392257452, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 47/516, D Loss: 0.4992023243685253, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 48/516, D Loss: 0.4873654404655099, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 49/516, D Loss: 0.4882365018129349, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 50/516, D Loss: 0.49844525824300945, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 51/516, D Loss: 0.4919008044525981, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 52/516, D Loss: 0.49867834663018584, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 53/516, D Loss: 0.490870987996459, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 54/516, D Loss: 0.4983157159294933, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 55/516, D Loss: 0.4966570897959173, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 56/516, D Loss: 0.48341599106788635, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 57/516, D Loss: 0.49950321816140786, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 58/516, D Loss: 0.4988586518447846, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 59/516, D Loss: 0.49543593591079116, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 60/516, D Loss: 0.4940335499122739, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 61/516, D Loss: 0.4930624570697546, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 62/516, D Loss: 0.4905117014423013, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 63/516, D Loss: 0.4913229690864682, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 64/516, D Loss: 0.49134867638349533, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 65/516, D Loss: 0.49742000084370375, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 66/516, D Loss: 0.4948937501758337, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 67/516, D Loss: 0.49679771647788584, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 68/516, D Loss: 0.4918683534488082, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 69/516, D Loss: 0.4864264400675893, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 70/516, D Loss: 0.4997002010641154, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 71/516, D Loss: 0.49699728679843247, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 72/516, D Loss: 0.4941326901316643, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 73/516, D Loss: 0.4953217743895948, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 74/516, D Loss: 0.4956399844959378, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 75/516, D Loss: 0.4832503702491522, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 76/516, D Loss: 0.49477245984598994, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 77/516, D Loss: 0.4918129909783602, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 78/516, D Loss: 0.49100111331790686, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 79/516, D Loss: 0.49749221559613943, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 80/516, D Loss: 0.49485205858945847, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 81/516, D Loss: 0.4948315843939781, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 82/516, D Loss: 0.4912536945194006, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 83/516, D Loss: 0.49895980721339583, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 84/516, D Loss: 0.49636380979791284, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 85/516, D Loss: 0.49193219747394323, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 86/516, D Loss: 0.496580109000206, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 87/516, D Loss: 0.48853496741503477, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 88/516, D Loss: 0.4990634564310312, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 89/516, D Loss: 0.49344389140605927, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 90/516, D Loss: 0.4899824596941471, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 91/516, D Loss: 0.4795128330588341, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 92/516, D Loss: 0.48735386598855257, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 93/516, D Loss: 0.4994737791130319, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 94/516, D Loss: 0.4990185610949993, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 95/516, D Loss: 0.4878308102488518, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 96/516, D Loss: 0.49173816852271557, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 97/516, D Loss: 0.4887411557137966, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 98/516, D Loss: 0.4944447120651603, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 99/516, D Loss: 0.4861353328451514, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 100/516, D Loss: 0.497918629553169, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 101/516, D Loss: 0.48696470260620117, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 102/516, D Loss: 0.4950156887061894, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 103/516, D Loss: 0.49339192640036345, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 104/516, D Loss: 0.49673953861929476, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 105/516, D Loss: 0.48512748908251524, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 106/516, D Loss: 0.49890235799830407, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 107/516, D Loss: 0.4897817289456725, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 108/516, D Loss: 0.49573484249413013, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 109/516, D Loss: 0.4894418762996793, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 110/516, D Loss: 0.4957405086606741, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 111/516, D Loss: 0.498841765569523, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 112/516, D Loss: 0.49053427390754223, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 113/516, D Loss: 0.4943702518939972, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 114/516, D Loss: 0.49013177305459976, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 115/516, D Loss: 0.4920786712318659, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 116/516, D Loss: 0.48867468908429146, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 117/516, D Loss: 0.4977160089183599, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 118/516, D Loss: 0.4915153877809644, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 119/516, D Loss: 0.4974265615455806, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 120/516, D Loss: 0.4945611981675029, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 121/516, D Loss: 0.4891924522817135, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 122/516, D Loss: 0.4929028614424169, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 123/516, D Loss: 0.4919233312830329, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 124/516, D Loss: 0.4948262069374323, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 125/516, D Loss: 0.499553387373453, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 126/516, D Loss: 0.4922678559087217, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 127/516, D Loss: 0.4887897027656436, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 128/516, D Loss: 0.49931598047260195, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 129/516, D Loss: 0.49985631834715605, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 130/516, D Loss: 0.49472280126065016, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 131/516, D Loss: 0.4949192339554429, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 132/516, D Loss: 0.4919159831479192, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 133/516, D Loss: 0.49033912271261215, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 134/516, D Loss: 0.4968681784812361, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 135/516, D Loss: 0.49560653837397695, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 136/516, D Loss: 0.49637112603522837, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 137/516, D Loss: 0.4988479313906282, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 138/516, D Loss: 0.4868345418944955, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 139/516, D Loss: 0.49847532191779464, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 140/516, D Loss: 0.4871172197163105, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 141/516, D Loss: 0.4890692727640271, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 142/516, D Loss: 0.4848153190687299, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 143/516, D Loss: 0.49581683054566383, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 144/516, D Loss: 0.49983322907064576, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 145/516, D Loss: 0.49406873853877187, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 146/516, D Loss: 0.49086258839815855, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 147/516, D Loss: 0.49941344326362014, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 148/516, D Loss: 0.49130212515592575, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 149/516, D Loss: 0.4988104607909918, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 150/516, D Loss: 0.49576009530574083, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 151/516, D Loss: 0.4936240529641509, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 152/516, D Loss: 0.4995882431976497, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 153/516, D Loss: 0.49986270237423014, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 154/516, D Loss: 0.4980516079813242, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 155/516, D Loss: 0.49304184364154935, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 156/516, D Loss: 0.49960575019940734, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 157/516, D Loss: 0.4957774206995964, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 158/516, D Loss: 0.4916868396103382, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 159/516, D Loss: 0.49607411213219166, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 160/516, D Loss: 0.49467811919748783, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 161/516, D Loss: 0.4963092131074518, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 162/516, D Loss: 0.4990465056616813, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 163/516, D Loss: 0.4995650822529569, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 164/516, D Loss: 0.4969261768274009, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 165/516, D Loss: 0.49745549680665135, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 166/516, D Loss: 0.48476940114051104, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 167/516, D Loss: 0.4804720263928175, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 168/516, D Loss: 0.4953404953703284, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 169/516, D Loss: 0.49560045590624213, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 170/516, D Loss: 0.49552351608872414, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 171/516, D Loss: 0.4917707182466984, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 172/516, D Loss: 0.49896928609814495, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 173/516, D Loss: 0.4867541743442416, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 174/516, D Loss: 0.49304004153236747, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 175/516, D Loss: 0.4905776446685195, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 176/516, D Loss: 0.49915780837181956, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 177/516, D Loss: 0.49186784587800503, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 178/516, D Loss: 0.49963164480868727, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 179/516, D Loss: 0.48112835735082626, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 180/516, D Loss: 0.4936403017491102, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 181/516, D Loss: 0.49793781666085124, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 182/516, D Loss: 0.498315695556812, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 183/516, D Loss: 0.4946703859604895, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 184/516, D Loss: 0.4917075103148818, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 185/516, D Loss: 0.49076868686825037, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 186/516, D Loss: 0.49515603110194206, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 187/516, D Loss: 0.48986424691975117, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 188/516, D Loss: 0.4954854203388095, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 189/516, D Loss: 0.4962482287082821, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 190/516, D Loss: 0.4890059847384691, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 191/516, D Loss: 0.4954606359824538, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 192/516, D Loss: 0.4957773978821933, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 193/516, D Loss: 0.4934424292296171, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 194/516, D Loss: 0.4988264099229127, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 195/516, D Loss: 0.47735594399273396, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 196/516, D Loss: 0.48987035639584064, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 197/516, D Loss: 0.49945552146527916, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 198/516, D Loss: 0.4923776858486235, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 199/516, D Loss: 0.49265037290751934, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 200/516, D Loss: 0.4904763735830784, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 201/516, D Loss: 0.4959106296300888, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 202/516, D Loss: 0.4927952499128878, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 203/516, D Loss: 0.49982668479788117, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 204/516, D Loss: 0.48847019486129284, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 205/516, D Loss: 0.4990144540788606, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 206/516, D Loss: 0.49613651959225535, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 207/516, D Loss: 0.496097233844921, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 208/516, D Loss: 0.49916577944532037, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 209/516, D Loss: 0.49536112509667873, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 210/516, D Loss: 0.4993518704432063, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 211/516, D Loss: 0.4995180426049046, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 212/516, D Loss: 0.4992913238238543, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 213/516, D Loss: 0.4928975310176611, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 214/516, D Loss: 0.49866335501428694, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 215/516, D Loss: 0.4947922062128782, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 216/516, D Loss: 0.4997007733909413, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 217/516, D Loss: 0.4930014703422785, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 218/516, D Loss: 0.49175730627030134, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 219/516, D Loss: 0.4989443066297099, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 220/516, D Loss: 0.49492472130805254, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 221/516, D Loss: 0.4990175812272355, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 222/516, D Loss: 0.4859232883900404, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 223/516, D Loss: 0.48059970885515213, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 224/516, D Loss: 0.4946358888410032, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 225/516, D Loss: 0.49919399089412764, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 226/516, D Loss: 0.4962628921493888, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 227/516, D Loss: 0.4872699733823538, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 228/516, D Loss: 0.49122216645628214, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 229/516, D Loss: 0.4954713387414813, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 230/516, D Loss: 0.49344513518735766, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 231/516, D Loss: 0.49388155061751604, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 232/516, D Loss: 0.4981336882337928, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 233/516, D Loss: 0.4951769094914198, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 234/516, D Loss: 0.49346638936549425, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 235/516, D Loss: 0.4839951917529106, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 236/516, D Loss: 0.4804081693291664, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 237/516, D Loss: 0.49270342802628875, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 238/516, D Loss: 0.49152656458318233, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 239/516, D Loss: 0.4965508182067424, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 240/516, D Loss: 0.49983953215996735, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 241/516, D Loss: 0.4972055312246084, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 242/516, D Loss: 0.4971211883239448, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 243/516, D Loss: 0.49252148997038603, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 244/516, D Loss: 0.48081275075674057, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 245/516, D Loss: 0.4903885293751955, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 246/516, D Loss: 0.49852410494349897, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 247/516, D Loss: 0.49937333847628906, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 248/516, D Loss: 0.49184955190867186, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 249/516, D Loss: 0.49099552165716887, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 250/516, D Loss: 0.4986464031971991, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 251/516, D Loss: 0.4996560760191642, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 252/516, D Loss: 0.4949137242510915, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 253/516, D Loss: 0.48784021753817797, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 254/516, D Loss: 0.49370606057345867, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 255/516, D Loss: 0.49877569475211203, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 256/516, D Loss: 0.4990635896101594, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 257/516, D Loss: 0.4982230180175975, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 258/516, D Loss: 0.49080147966742516, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 259/516, D Loss: 0.4906414523720741, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 260/516, D Loss: 0.4984496782999486, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 261/516, D Loss: 0.495787285733968, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 262/516, D Loss: 0.49232175666838884, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 263/516, D Loss: 0.49533529952168465, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 264/516, D Loss: 0.48947884887456894, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 265/516, D Loss: 0.49852770392317325, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 266/516, D Loss: 0.49347285088151693, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 267/516, D Loss: 0.4885364370420575, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 268/516, D Loss: 0.49956823341199197, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 269/516, D Loss: 0.4920210475102067, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 270/516, D Loss: 0.4952602726407349, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 271/516, D Loss: 0.48784259147942066, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 272/516, D Loss: 0.49013693258166313, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 273/516, D Loss: 0.4998898562262184, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 274/516, D Loss: 0.4968176430556923, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 275/516, D Loss: 0.48354826122522354, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 276/516, D Loss: 0.4951263777911663, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 277/516, D Loss: 0.4951740703545511, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 278/516, D Loss: 0.4944831565953791, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 279/516, D Loss: 0.4946880182251334, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 280/516, D Loss: 0.4985859813168645, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 281/516, D Loss: 0.4988974455045536, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 282/516, D Loss: 0.49811476038303226, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 283/516, D Loss: 0.4967046494130045, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 284/516, D Loss: 0.4884582757949829, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 285/516, D Loss: 0.4911738969385624, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 286/516, D Loss: 0.49350350676104426, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 287/516, D Loss: 0.49719727667979896, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 288/516, D Loss: 0.495777927339077, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 289/516, D Loss: 0.4950062553398311, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 290/516, D Loss: 0.4902876000851393, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 291/516, D Loss: 0.49297144543379545, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 292/516, D Loss: 0.49961522861849517, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 293/516, D Loss: 0.48767433501780033, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 294/516, D Loss: 0.48861600551754236, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 295/516, D Loss: 0.4920369712635875, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 296/516, D Loss: 0.4937247233465314, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 297/516, D Loss: 0.4996656949806493, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 298/516, D Loss: 0.4911422682926059, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 299/516, D Loss: 0.4937275294214487, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 300/516, D Loss: 0.4959838236682117, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 301/516, D Loss: 0.4864424355328083, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 302/516, D Loss: 0.48663165234029293, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 303/516, D Loss: 0.49425963731482625, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 304/516, D Loss: 0.4997197315096855, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 305/516, D Loss: 0.48686738312244415, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 306/516, D Loss: 0.4954268028959632, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 307/516, D Loss: 0.492144376039505, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 308/516, D Loss: 0.4995607071905397, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 309/516, D Loss: 0.4989525737473741, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 310/516, D Loss: 0.48285892233252525, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 311/516, D Loss: 0.4959814874455333, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 312/516, D Loss: 0.4953248300589621, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 313/516, D Loss: 0.4851450817659497, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 314/516, D Loss: 0.49728506430983543, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 315/516, D Loss: 0.49270925391465425, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 316/516, D Loss: 0.49070083536207676, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 317/516, D Loss: 0.49754034355282784, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 318/516, D Loss: 0.49081078078597784, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 319/516, D Loss: 0.49490202218294144, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 320/516, D Loss: 0.4955111425369978, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 321/516, D Loss: 0.498201924841851, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 322/516, D Loss: 0.48820678424090147, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 9, Batch 323/516, D Loss: 0.49970529376878403, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 324/516, D Loss: 0.4901496386155486, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 325/516, D Loss: 0.495689757168293, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 326/516, D Loss: 0.49163280613720417, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 327/516, D Loss: 0.49226698093116283, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 328/516, D Loss: 0.49718023277819157, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 329/516, D Loss: 0.48657532036304474, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 330/516, D Loss: 0.48744570184499025, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 331/516, D Loss: 0.4955455819144845, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 332/516, D Loss: 0.4817296452820301, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 333/516, D Loss: 0.4934857366606593, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 334/516, D Loss: 0.49571727169677615, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 335/516, D Loss: 0.49463499849662185, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 336/516, D Loss: 0.4978635855950415, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 337/516, D Loss: 0.49569176277145743, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 338/516, D Loss: 0.4988289203029126, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 339/516, D Loss: 0.49669658904895186, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 340/516, D Loss: 0.48915756214410067, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 341/516, D Loss: 0.49167654290795326, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 342/516, D Loss: 0.49110346753150225, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 343/516, D Loss: 0.49545871699228883, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 344/516, D Loss: 0.49394135270267725, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 345/516, D Loss: 0.49199055321514606, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 346/516, D Loss: 0.490708383731544, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 347/516, D Loss: 0.4992346827639267, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 348/516, D Loss: 0.494014126714319, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 349/516, D Loss: 0.49938943004235625, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 350/516, D Loss: 0.49916614848189056, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 351/516, D Loss: 0.49960873849340715, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 352/516, D Loss: 0.49660342326387763, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 353/516, D Loss: 0.4983932067407295, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 354/516, D Loss: 0.498490329249762, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 355/516, D Loss: 0.4857183201238513, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 356/516, D Loss: 0.4840357266366482, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 357/516, D Loss: 0.49657733365893364, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 358/516, D Loss: 0.49274742137640715, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 359/516, D Loss: 0.4952730075456202, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 360/516, D Loss: 0.4956351020373404, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 361/516, D Loss: 0.49266938539221883, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 362/516, D Loss: 0.4926215992309153, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 363/516, D Loss: 0.4977232809178531, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 364/516, D Loss: 0.493520010728389, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 365/516, D Loss: 0.4990286949905567, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 366/516, D Loss: 0.49932323448592797, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 367/516, D Loss: 0.4994210200384259, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 368/516, D Loss: 0.49675329332239926, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 369/516, D Loss: 0.4997650399018312, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 370/516, D Loss: 0.4950198223814368, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 371/516, D Loss: 0.499906866447418, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 372/516, D Loss: 0.49656620156019926, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 373/516, D Loss: 0.4959359443746507, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 374/516, D Loss: 0.49141037091612816, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 375/516, D Loss: 0.49493590416386724, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 376/516, D Loss: 0.4966792094055563, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 377/516, D Loss: 0.4922152925282717, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 378/516, D Loss: 0.497270742431283, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 379/516, D Loss: 0.49843331752344966, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 380/516, D Loss: 0.49931818793993443, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 381/516, D Loss: 0.49034776259213686, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 382/516, D Loss: 0.49312938936054707, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 383/516, D Loss: 0.4948093290440738, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 384/516, D Loss: 0.4922964544966817, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 385/516, D Loss: 0.48233198560774326, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 386/516, D Loss: 0.49935714714229107, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 387/516, D Loss: 0.49496554071083665, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 388/516, D Loss: 0.49928786722011864, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 389/516, D Loss: 0.49949479184579104, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 390/516, D Loss: 0.49664157372899354, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 391/516, D Loss: 0.4972274377942085, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 392/516, D Loss: 0.4992482350789942, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 393/516, D Loss: 0.49646518658846617, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 394/516, D Loss: 0.49515123944729567, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 395/516, D Loss: 0.49055179581046104, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 9, Batch 396/516, D Loss: 0.49354449566453695, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 397/516, D Loss: 0.4922235473059118, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "Epoch 9, Batch 398/516, D Loss: 0.4990184787893668, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 399/516, D Loss: 0.4839278142899275, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 400/516, D Loss: 0.494364392478019, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 401/516, D Loss: 0.49958510624128394, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 402/516, D Loss: 0.49797077709808946, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 403/516, D Loss: 0.4881049348041415, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 404/516, D Loss: 0.4996496105741244, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 405/516, D Loss: 0.48639916256070137, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 406/516, D Loss: 0.4957748190499842, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 407/516, D Loss: 0.48795857932418585, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 408/516, D Loss: 0.49188051000237465, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 409/516, D Loss: 0.4951352239586413, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 410/516, D Loss: 0.49732280243188143, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 411/516, D Loss: 0.49920742656104267, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 412/516, D Loss: 0.4932126682251692, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 413/516, D Loss: 0.49752047983929515, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 414/516, D Loss: 0.489550962112844, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 415/516, D Loss: 0.4940546057187021, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 416/516, D Loss: 0.49958933031302877, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 417/516, D Loss: 0.49917456979164854, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 418/516, D Loss: 0.49511843966320157, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 419/516, D Loss: 0.4915937762707472, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 420/516, D Loss: 0.49916447832947597, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 421/516, D Loss: 0.4861065223813057, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 422/516, D Loss: 0.49427109165117145, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 423/516, D Loss: 0.4930714634247124, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 424/516, D Loss: 0.4957154328003526, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 425/516, D Loss: 0.4965888084843755, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 426/516, D Loss: 0.4947599391452968, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 427/516, D Loss: 0.4993041299167089, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 428/516, D Loss: 0.49179043527692556, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 429/516, D Loss: 0.48766680154949427, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 430/516, D Loss: 0.49924380192533135, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 431/516, D Loss: 0.4911138080060482, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 432/516, D Loss: 0.4937664014287293, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 433/516, D Loss: 0.49528782768175006, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 434/516, D Loss: 0.4964729028288275, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 435/516, D Loss: 0.49143807124346495, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 436/516, D Loss: 0.4896195027977228, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 437/516, D Loss: 0.4996817441424355, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 438/516, D Loss: 0.49879852368030697, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 439/516, D Loss: 0.49555392656475306, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 440/516, D Loss: 0.495336692314595, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 441/516, D Loss: 0.4890966201201081, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 442/516, D Loss: 0.4947750559076667, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 443/516, D Loss: 0.4996024545689579, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 444/516, D Loss: 0.4980649990029633, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 445/516, D Loss: 0.4966556974686682, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 446/516, D Loss: 0.49216742999851704, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 447/516, D Loss: 0.49259006744250655, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 448/516, D Loss: 0.4985065294895321, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 449/516, D Loss: 0.4918665159493685, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 450/516, D Loss: 0.4802943244576454, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 451/516, D Loss: 0.4909603791311383, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 452/516, D Loss: 0.4826745428144932, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 453/516, D Loss: 0.49067893251776695, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 454/516, D Loss: 0.49344581132754683, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 455/516, D Loss: 0.48656357917934656, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 456/516, D Loss: 0.49941487150499597, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 457/516, D Loss: 0.49212774634361267, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 458/516, D Loss: 0.4931161249987781, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 459/516, D Loss: 0.4985639330698177, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 460/516, D Loss: 0.4953585942275822, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 461/516, D Loss: 0.4876169189810753, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 462/516, D Loss: 0.49676889390684664, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 463/516, D Loss: 0.4981299575883895, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 9, Batch 464/516, D Loss: 0.49352880753576756, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 465/516, D Loss: 0.4940818948671222, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 466/516, D Loss: 0.4936957061290741, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 467/516, D Loss: 0.4980816268362105, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 468/516, D Loss: 0.49867213831748813, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 469/516, D Loss: 0.4966152748093009, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 470/516, D Loss: 0.49840050120837986, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 471/516, D Loss: 0.49960507606738247, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 472/516, D Loss: 0.49236481077969074, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 473/516, D Loss: 0.48790143337100744, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 474/516, D Loss: 0.495898119173944, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 475/516, D Loss: 0.49639198440127075, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 476/516, D Loss: 0.49550974601879716, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 477/516, D Loss: 0.4992462907684967, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 478/516, D Loss: 0.4952083523385227, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 479/516, D Loss: 0.49993931842254824, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 480/516, D Loss: 0.487229841761291, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 481/516, D Loss: 0.4813283886760473, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 482/516, D Loss: 0.48707790672779083, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 483/516, D Loss: 0.4993598568253219, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 484/516, D Loss: 0.4911551643162966, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 485/516, D Loss: 0.48720988258719444, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 486/516, D Loss: 0.49103805609047413, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 487/516, D Loss: 0.49776903726160526, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 488/516, D Loss: 0.4952152520418167, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 489/516, D Loss: 0.4988718281965703, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 490/516, D Loss: 0.4914535107091069, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 491/516, D Loss: 0.48670097161084414, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 492/516, D Loss: 0.4964679996483028, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 493/516, D Loss: 0.49005572218447924, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 494/516, D Loss: 0.4990018499083817, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 495/516, D Loss: 0.49353734496980906, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 496/516, D Loss: 0.48945934046059847, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 497/516, D Loss: 0.49906646204181015, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 498/516, D Loss: 0.4982773244846612, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 499/516, D Loss: 0.4985039443708956, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 500/516, D Loss: 0.49262321880087256, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 501/516, D Loss: 0.49580454593524337, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 502/516, D Loss: 0.49134570453315973, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 503/516, D Loss: 0.49535522842779756, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 504/516, D Loss: 0.4919286444783211, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 505/516, D Loss: 0.48995119519531727, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 506/516, D Loss: 0.49919419432990253, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 507/516, D Loss: 0.49456354789435863, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 9, Batch 508/516, D Loss: 0.499472729221452, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 509/516, D Loss: 0.4988248981535435, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 510/516, D Loss: 0.48838112875819206, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 511/516, D Loss: 0.49888977291993797, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 512/516, D Loss: 0.49618507642298937, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 513/516, D Loss: 0.48989661782979965, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 9, Batch 514/516, D Loss: 0.49541719583794475, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 515/516, D Loss: 0.4896370507776737, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 9, Batch 516/516, D Loss: 0.49551906157284975, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 1/516, D Loss: 0.49180302023887634, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 2/516, D Loss: 0.4912693500518799, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 3/516, D Loss: 0.4983003647066653, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 4/516, D Loss: 0.49527506018057466, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 5/516, D Loss: 0.4904616381973028, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 6/516, D Loss: 0.48551664128899574, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 7/516, D Loss: 0.49191372096538544, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 8/516, D Loss: 0.4867052976042032, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 9/516, D Loss: 0.4911924758926034, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 10/516, D Loss: 0.4987526562763378, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 11/516, D Loss: 0.48954854626208544, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 12/516, D Loss: 0.49670473858714104, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 13/516, D Loss: 0.49181295931339264, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 14/516, D Loss: 0.4972333940677345, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 15/516, D Loss: 0.49980842643708456, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 16/516, D Loss: 0.49660799792036414, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 17/516, D Loss: 0.4986678055720404, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 18/516, D Loss: 0.49866042332723737, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 19/516, D Loss: 0.4939384264871478, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 20/516, D Loss: 0.49961044985684566, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 21/516, D Loss: 0.4906605100259185, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 22/516, D Loss: 0.4866392333060503, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 23/516, D Loss: 0.4940966460853815, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 24/516, D Loss: 0.49180983379483223, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 25/516, D Loss: 0.48730586282908916, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 26/516, D Loss: 0.49475682293996215, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 27/516, D Loss: 0.4997014658583794, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 28/516, D Loss: 0.4992636673268862, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 29/516, D Loss: 0.49854654783848673, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 30/516, D Loss: 0.49038310814648867, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 31/516, D Loss: 0.49665900762192905, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 32/516, D Loss: 0.4940255582332611, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 33/516, D Loss: 0.49920815299265087, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 34/516, D Loss: 0.4993726304965094, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 35/516, D Loss: 0.4916439699009061, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 36/516, D Loss: 0.4871816784143448, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 37/516, D Loss: 0.49514052644371986, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 38/516, D Loss: 0.4982877002330497, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 39/516, D Loss: 0.49867254053242505, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 40/516, D Loss: 0.49933195509947836, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 41/516, D Loss: 0.495650980155915, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 42/516, D Loss: 0.4960377998650074, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 43/516, D Loss: 0.4929285906255245, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 44/516, D Loss: 0.4919489389285445, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 45/516, D Loss: 0.49025930371135473, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 46/516, D Loss: 0.4977573622018099, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 47/516, D Loss: 0.48825431894510984, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 48/516, D Loss: 0.4981002612039447, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 49/516, D Loss: 0.48949316143989563, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 50/516, D Loss: 0.4887574454769492, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 51/516, D Loss: 0.4977173116058111, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 52/516, D Loss: 0.4909856468439102, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 53/516, D Loss: 0.4918270409107208, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 54/516, D Loss: 0.4799012094736099, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 55/516, D Loss: 0.49968044261913747, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 56/516, D Loss: 0.495578542817384, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 57/516, D Loss: 0.4950295118615031, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 58/516, D Loss: 0.49778133211657405, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 59/516, D Loss: 0.49823560402728617, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 60/516, D Loss: 0.49335652915760875, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 61/516, D Loss: 0.4994839780847542, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 62/516, D Loss: 0.49958877204335295, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 63/516, D Loss: 0.49069991428405046, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 64/516, D Loss: 0.48974182084202766, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 65/516, D Loss: 0.49389558006078005, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 66/516, D Loss: 0.49474298022687435, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 67/516, D Loss: 0.4964286203030497, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 68/516, D Loss: 0.49957845773315057, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 69/516, D Loss: 0.4996285767410882, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 70/516, D Loss: 0.4958294937387109, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 71/516, D Loss: 0.49809814337641, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 72/516, D Loss: 0.49902299826499075, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 73/516, D Loss: 0.49261458683758974, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 74/516, D Loss: 0.4918791353702545, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 75/516, D Loss: 0.49577195616438985, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 76/516, D Loss: 0.4867753740400076, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 77/516, D Loss: 0.4975179792381823, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 78/516, D Loss: 0.4941782969981432, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 79/516, D Loss: 0.49506571236997843, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 80/516, D Loss: 0.49222367722541094, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 81/516, D Loss: 0.4977844129316509, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 82/516, D Loss: 0.4928759289905429, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 83/516, D Loss: 0.4993147114291787, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 84/516, D Loss: 0.495021129027009, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 85/516, D Loss: 0.49714467441663146, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 86/516, D Loss: 0.4871451975777745, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 87/516, D Loss: 0.4953656098805368, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 88/516, D Loss: 0.49323470424860716, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 89/516, D Loss: 0.4993428912712261, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 90/516, D Loss: 0.4991479674936272, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 91/516, D Loss: 0.4969296921044588, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 92/516, D Loss: 0.4914892064407468, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 93/516, D Loss: 0.49852464348077774, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 94/516, D Loss: 0.49805645109154284, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 95/516, D Loss: 0.4981134751578793, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 96/516, D Loss: 0.4993220561882481, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 97/516, D Loss: 0.496857134392485, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 98/516, D Loss: 0.4897075276821852, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 99/516, D Loss: 0.49153355974704027, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 100/516, D Loss: 0.4925119732506573, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 101/516, D Loss: 0.497468579094857, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 102/516, D Loss: 0.4997006710036658, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 103/516, D Loss: 0.49956438117078505, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 104/516, D Loss: 0.49523595767095685, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 105/516, D Loss: 0.4998032438743394, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 106/516, D Loss: 0.4933794364333153, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 107/516, D Loss: 0.4953847359865904, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 108/516, D Loss: 0.4946969449520111, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 109/516, D Loss: 0.4861556785181165, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 110/516, D Loss: 0.49531967379152775, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 111/516, D Loss: 0.4937213407829404, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 112/516, D Loss: 0.48964263405650854, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 113/516, D Loss: 0.47664408572018147, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 114/516, D Loss: 0.4996673307614401, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 115/516, D Loss: 0.497973364777863, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 116/516, D Loss: 0.4861965198069811, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 117/516, D Loss: 0.4984740798827261, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 118/516, D Loss: 0.4913138523697853, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 119/516, D Loss: 0.4875881243497133, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 120/516, D Loss: 0.4950797180645168, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 121/516, D Loss: 0.49872311274521053, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 122/516, D Loss: 0.49120521917939186, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 123/516, D Loss: 0.4877697369083762, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 124/516, D Loss: 0.49165486730635166, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 125/516, D Loss: 0.49899539875332266, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 126/516, D Loss: 0.49473377130925655, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 127/516, D Loss: 0.49203317891806364, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 128/516, D Loss: 0.4994373254594393, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 129/516, D Loss: 0.49698510905727744, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 130/516, D Loss: 0.4929976728744805, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 131/516, D Loss: 0.49577528052031994, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 132/516, D Loss: 0.49976401272579096, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 133/516, D Loss: 0.497683102497831, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 134/516, D Loss: 0.491727152839303, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 135/516, D Loss: 0.4863276453688741, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 136/516, D Loss: 0.4996195721323602, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 137/516, D Loss: 0.494059297721833, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 138/516, D Loss: 0.4995094198966399, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 139/516, D Loss: 0.49734252877533436, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 140/516, D Loss: 0.49369513243436813, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 141/516, D Loss: 0.49864324112422764, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 142/516, D Loss: 0.4954039235599339, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 143/516, D Loss: 0.4985132720321417, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 144/516, D Loss: 0.4806271940469742, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 145/516, D Loss: 0.49573110044002533, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 146/516, D Loss: 0.49474816443398595, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 147/516, D Loss: 0.48977987840771675, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 148/516, D Loss: 0.49690900021232665, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 149/516, D Loss: 0.49522207537665963, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 150/516, D Loss: 0.49187786784023046, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 151/516, D Loss: 0.49950185662601143, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 152/516, D Loss: 0.499285445548594, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 153/516, D Loss: 0.4893002100288868, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 154/516, D Loss: 0.4995889902347699, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 155/516, D Loss: 0.4972345936112106, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 156/516, D Loss: 0.48089636117219925, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 157/516, D Loss: 0.494963756762445, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 158/516, D Loss: 0.4950517131946981, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 159/516, D Loss: 0.4944771006703377, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 160/516, D Loss: 0.4720828104764223, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 161/516, D Loss: 0.49481939198449254, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 162/516, D Loss: 0.4941676277667284, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 163/516, D Loss: 0.4852826204150915, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 164/516, D Loss: 0.49078845977783203, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 165/516, D Loss: 0.49385826010257006, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 166/516, D Loss: 0.4955937215127051, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 167/516, D Loss: 0.4950529243797064, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 168/516, D Loss: 0.4995417899917811, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 169/516, D Loss: 0.49841068708337843, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 170/516, D Loss: 0.49440843844786286, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 171/516, D Loss: 0.4992214420926757, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 172/516, D Loss: 0.48875681683421135, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 173/516, D Loss: 0.4970305690076202, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 174/516, D Loss: 0.49904994620010257, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 175/516, D Loss: 0.49173738341778517, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 176/516, D Loss: 0.49140145629644394, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 177/516, D Loss: 0.4955266769975424, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 178/516, D Loss: 0.4919205950573087, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 179/516, D Loss: 0.49921855912543833, G Loss: -0.9999998807907104\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 180/516, D Loss: 0.48637837916612625, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 181/516, D Loss: 0.49822260974906385, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 182/516, D Loss: 0.4969589232932776, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 183/516, D Loss: 0.4980429874267429, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 184/516, D Loss: 0.4939067685045302, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 185/516, D Loss: 0.49307557567954063, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 186/516, D Loss: 0.4951452468521893, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 187/516, D Loss: 0.4958181534893811, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 188/516, D Loss: 0.4801894035190344, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 189/516, D Loss: 0.4905972499400377, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 190/516, D Loss: 0.493518709205091, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 191/516, D Loss: 0.4956431551836431, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 192/516, D Loss: 0.499543349375017, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 193/516, D Loss: 0.4977006916888058, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 194/516, D Loss: 0.4931310685351491, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 195/516, D Loss: 0.4969707771670073, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 196/516, D Loss: 0.4883429976180196, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 197/516, D Loss: 0.49117361661046743, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 198/516, D Loss: 0.49114514514803886, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 199/516, D Loss: 0.4989977078512311, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 200/516, D Loss: 0.49574881233274937, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 201/516, D Loss: 0.49941339646466076, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 202/516, D Loss: 0.4956156685948372, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 203/516, D Loss: 0.4876777231693268, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 204/516, D Loss: 0.48663198202848434, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 205/516, D Loss: 0.492386223282665, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 206/516, D Loss: 0.49467793572694063, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 207/516, D Loss: 0.49570414097979665, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 208/516, D Loss: 0.49367776280269027, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 209/516, D Loss: 0.4988358886912465, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 210/516, D Loss: 0.49728090898133814, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 211/516, D Loss: 0.49150946643203497, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 212/516, D Loss: 0.49878773698583245, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 213/516, D Loss: 0.4845475070178509, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 214/516, D Loss: 0.49140394665300846, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 215/516, D Loss: 0.491319864988327, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 216/516, D Loss: 0.49115330073982477, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 217/516, D Loss: 0.4990721577196382, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 218/516, D Loss: 0.48740825802087784, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 219/516, D Loss: 0.4936419343575835, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 220/516, D Loss: 0.49845120788086206, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 221/516, D Loss: 0.4961875039152801, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 222/516, D Loss: 0.4996724035590887, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 223/516, D Loss: 0.4997012934181839, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 224/516, D Loss: 0.488396423868835, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 225/516, D Loss: 0.4962736477609724, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 226/516, D Loss: 0.493982563726604, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 227/516, D Loss: 0.49849757296033204, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 228/516, D Loss: 0.49823054391890764, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 229/516, D Loss: 0.49315861985087395, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 230/516, D Loss: 0.4973648516461253, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 231/516, D Loss: 0.4902930026873946, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 232/516, D Loss: 0.49579236935824156, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 233/516, D Loss: 0.4962204813491553, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 234/516, D Loss: 0.48944460041821003, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 235/516, D Loss: 0.4989867778494954, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 236/516, D Loss: 0.4942706455476582, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 237/516, D Loss: 0.4870250876992941, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 238/516, D Loss: 0.49968387666740455, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 239/516, D Loss: 0.49779724469408393, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 240/516, D Loss: 0.49252871237695217, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 241/516, D Loss: 0.49454773124307394, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 242/516, D Loss: 0.49388492200523615, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 243/516, D Loss: 0.49930530379060656, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 244/516, D Loss: 0.4983036530902609, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 245/516, D Loss: 0.4916578819975257, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 246/516, D Loss: 0.49610894196666777, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 247/516, D Loss: 0.495602753944695, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 248/516, D Loss: 0.4953268482349813, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 249/516, D Loss: 0.48554955143481493, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 250/516, D Loss: 0.49514620238915086, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 251/516, D Loss: 0.4951350768096745, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 252/516, D Loss: 0.485981160774827, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 253/516, D Loss: 0.491718833334744, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 254/516, D Loss: 0.49458798533305526, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 255/516, D Loss: 0.49662483180873096, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 256/516, D Loss: 0.4976573991589248, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 257/516, D Loss: 0.49168986920267344, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 258/516, D Loss: 0.4868226833641529, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 259/516, D Loss: 0.49532421585172415, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 260/516, D Loss: 0.4952522022649646, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 261/516, D Loss: 0.49659399688243866, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 262/516, D Loss: 0.4980980029795319, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 263/516, D Loss: 0.4864479126408696, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 264/516, D Loss: 0.4980365647934377, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 265/516, D Loss: 0.4837205372750759, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 266/516, D Loss: 0.4984261884819716, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 267/516, D Loss: 0.4945470681414008, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 268/516, D Loss: 0.491052582859993, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 269/516, D Loss: 0.492848071269691, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 270/516, D Loss: 0.4931081384420395, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 271/516, D Loss: 0.4959693904966116, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 272/516, D Loss: 0.49465055018663406, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 273/516, D Loss: 0.49918822082690895, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 274/516, D Loss: 0.49881474394351244, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 275/516, D Loss: 0.49532057670876384, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 276/516, D Loss: 0.48937503434717655, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 277/516, D Loss: 0.495133135933429, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 278/516, D Loss: 0.4954932746477425, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 279/516, D Loss: 0.48725586757063866, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 280/516, D Loss: 0.4994212113087997, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 281/516, D Loss: 0.48939950205385685, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 282/516, D Loss: 0.4794715642929077, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 283/516, D Loss: 0.4994009028887376, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 284/516, D Loss: 0.49672138784080744, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 285/516, D Loss: 0.4979790332727134, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 286/516, D Loss: 0.49903224571608007, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 287/516, D Loss: 0.4906646404415369, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 288/516, D Loss: 0.49667717050760984, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 289/516, D Loss: 0.4939120477065444, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 290/516, D Loss: 0.4933903021737933, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 291/516, D Loss: 0.4911821186542511, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 292/516, D Loss: 0.4830216243863106, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 293/516, D Loss: 0.4905043700709939, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 294/516, D Loss: 0.49958169454475865, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 295/516, D Loss: 0.4938846807926893, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 296/516, D Loss: 0.4931567828170955, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 297/516, D Loss: 0.49062364362180233, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 298/516, D Loss: 0.4956072364002466, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 299/516, D Loss: 0.4997976865415694, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 300/516, D Loss: 0.4996554809913505, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 301/516, D Loss: 0.4996195222483948, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 302/516, D Loss: 0.48655201122164726, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 303/516, D Loss: 0.4985062766354531, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 304/516, D Loss: 0.4942855476401746, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 305/516, D Loss: 0.49182523787021637, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 306/516, D Loss: 0.4996062654827256, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 307/516, D Loss: 0.49961047334363684, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 308/516, D Loss: 0.49484329763799906, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 309/516, D Loss: 0.4953949642367661, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 310/516, D Loss: 0.48864895664155483, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 311/516, D Loss: 0.49387273751199245, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 312/516, D Loss: 0.4885850977152586, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 313/516, D Loss: 0.4965991689823568, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 314/516, D Loss: 0.48632653057575226, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 315/516, D Loss: 0.4864365067332983, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 316/516, D Loss: 0.4991612734156661, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 317/516, D Loss: 0.49403439555317163, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 318/516, D Loss: 0.4996468218159862, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 319/516, D Loss: 0.49516252148896456, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 320/516, D Loss: 0.4938520323485136, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 321/516, D Loss: 0.49612896726466715, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 322/516, D Loss: 0.4939389964565635, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 323/516, D Loss: 0.4886010466143489, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 324/516, D Loss: 0.4854905763641, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 325/516, D Loss: 0.4951577899046242, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 326/516, D Loss: 0.49926138727460057, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 327/516, D Loss: 0.47742085717618465, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 328/516, D Loss: 0.49089404847472906, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 329/516, D Loss: 0.4817141033709049, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 330/516, D Loss: 0.4956113239750266, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 331/516, D Loss: 0.49315744638442993, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 332/516, D Loss: 0.49338029930368066, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 333/516, D Loss: 0.49338974989950657, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 334/516, D Loss: 0.4891941752284765, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 335/516, D Loss: 0.4808472227305174, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 336/516, D Loss: 0.4898227555677295, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 337/516, D Loss: 0.49819104466587305, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 338/516, D Loss: 0.4891995834186673, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 339/516, D Loss: 0.498516543302685, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 340/516, D Loss: 0.49921913049183786, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 341/516, D Loss: 0.49251027312129736, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 342/516, D Loss: 0.4900980778038502, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 343/516, D Loss: 0.4889266872778535, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 344/516, D Loss: 0.4944688812829554, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 345/516, D Loss: 0.4866004232317209, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 346/516, D Loss: 0.4908019658178091, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 347/516, D Loss: 0.49501016177237034, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 348/516, D Loss: 0.48945632204413414, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 349/516, D Loss: 0.49925635632826015, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 350/516, D Loss: 0.49839794798754156, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 351/516, D Loss: 0.49277009861543775, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 352/516, D Loss: 0.4875272698700428, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 353/516, D Loss: 0.4942915989086032, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 354/516, D Loss: 0.4887837050482631, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 355/516, D Loss: 0.488775797188282, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 356/516, D Loss: 0.49946862168144435, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 357/516, D Loss: 0.491017060354352, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 358/516, D Loss: 0.4913715459406376, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 359/516, D Loss: 0.49314899323508143, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 360/516, D Loss: 0.4968816158361733, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 361/516, D Loss: 0.499036175548099, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 362/516, D Loss: 0.4987238309113309, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 363/516, D Loss: 0.4937808234244585, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 364/516, D Loss: 0.4948119120672345, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 365/516, D Loss: 0.4983096169307828, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 366/516, D Loss: 0.49664112320169806, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 367/516, D Loss: 0.49551936239004135, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 368/516, D Loss: 0.49803349701687694, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 369/516, D Loss: 0.4989398129982874, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 370/516, D Loss: 0.49453151458874345, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 371/516, D Loss: 0.4981124341720715, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 372/516, D Loss: 0.497010535094887, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 373/516, D Loss: 0.49450525967404246, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 374/516, D Loss: 0.4904142078012228, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 375/516, D Loss: 0.49535979237407446, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 376/516, D Loss: 0.499791862530401, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 377/516, D Loss: 0.4937496604397893, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 378/516, D Loss: 0.4891461916267872, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 379/516, D Loss: 0.4952646428719163, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 380/516, D Loss: 0.4907512776553631, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 381/516, D Loss: 0.48818893171846867, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 382/516, D Loss: 0.49762420053593814, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 383/516, D Loss: 0.4831848330795765, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 384/516, D Loss: 0.4905798267573118, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 385/516, D Loss: 0.4880743306130171, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 386/516, D Loss: 0.4953611777164042, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 387/516, D Loss: 0.49375715712085366, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 388/516, D Loss: 0.4977237756829709, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 389/516, D Loss: 0.4983626512112096, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 390/516, D Loss: 0.4994425397599116, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 391/516, D Loss: 0.49516073428094387, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 392/516, D Loss: 0.4996288311085664, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 393/516, D Loss: 0.49092015251517296, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 394/516, D Loss: 0.49176647886633873, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 395/516, D Loss: 0.49950599728617817, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 396/516, D Loss: 0.49891805252991617, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 397/516, D Loss: 0.4951804205775261, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 398/516, D Loss: 0.4988611317239702, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 399/516, D Loss: 0.4950658418238163, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 400/516, D Loss: 0.4987331531010568, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 401/516, D Loss: 0.4982221017125994, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 402/516, D Loss: 0.4873414868488908, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 403/516, D Loss: 0.49465966783463955, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 404/516, D Loss: 0.4958167923614383, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 405/516, D Loss: 0.4974755102302879, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 406/516, D Loss: 0.49111143313348293, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 407/516, D Loss: 0.49982449068920687, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 408/516, D Loss: 0.4988420024747029, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 409/516, D Loss: 0.4912644047290087, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 410/516, D Loss: 0.49710525129921734, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 411/516, D Loss: 0.4959496967494488, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 412/516, D Loss: 0.49011835642158985, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 413/516, D Loss: 0.49104222375899553, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 414/516, D Loss: 0.4956478402018547, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 415/516, D Loss: 0.4956493331119418, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 416/516, D Loss: 0.4990763694513589, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 417/516, D Loss: 0.49624872114509344, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 418/516, D Loss: 0.4995671375072561, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 419/516, D Loss: 0.49158854223787785, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 420/516, D Loss: 0.49039639346301556, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 421/516, D Loss: 0.49194348603487015, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 422/516, D Loss: 0.4855487234890461, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 423/516, D Loss: 0.49829993047751486, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 424/516, D Loss: 0.49519831500947475, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 425/516, D Loss: 0.4976474232971668, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 426/516, D Loss: 0.4942306587472558, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 427/516, D Loss: 0.49289188208058476, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 428/516, D Loss: 0.4882181379944086, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 429/516, D Loss: 0.4981396901421249, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 430/516, D Loss: 0.4944696011953056, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 431/516, D Loss: 0.49382732389494777, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 432/516, D Loss: 0.4988227396970615, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 433/516, D Loss: 0.49585353396832943, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 434/516, D Loss: 0.4893772602081299, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 435/516, D Loss: 0.4970772915985435, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 436/516, D Loss: 0.4941974296234548, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 437/516, D Loss: 0.4948587166145444, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 438/516, D Loss: 0.4893687069416046, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 439/516, D Loss: 0.49472259264439344, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 440/516, D Loss: 0.4933576458133757, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 441/516, D Loss: 0.49603468365967274, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 442/516, D Loss: 0.4946399158798158, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 443/516, D Loss: 0.48983669374138117, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 444/516, D Loss: 0.49826282798312604, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 445/516, D Loss: 0.4911160934716463, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 446/516, D Loss: 0.4950347561389208, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 447/516, D Loss: 0.4932294497266412, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 448/516, D Loss: 0.49923257157206535, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 449/516, D Loss: 0.49873604730237275, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 450/516, D Loss: 0.49781706742942333, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 451/516, D Loss: 0.49645307450555265, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 452/516, D Loss: 0.4874982042238116, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 453/516, D Loss: 0.4959131977520883, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 454/516, D Loss: 0.49987538636196405, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 455/516, D Loss: 0.49023537151515484, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 456/516, D Loss: 0.4993912937352434, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 457/516, D Loss: 0.4981452798238024, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 458/516, D Loss: 0.4955932730808854, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 459/516, D Loss: 0.4998402346391231, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 460/516, D Loss: 0.4951488832011819, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 461/516, D Loss: 0.49930095311719924, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 462/516, D Loss: 0.4978375365026295, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 463/516, D Loss: 0.489217147231102, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 464/516, D Loss: 0.4946814691647887, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 465/516, D Loss: 0.49643109925091267, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 466/516, D Loss: 0.4995711264782585, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 467/516, D Loss: 0.4843236058950424, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 468/516, D Loss: 0.49536143289878964, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 469/516, D Loss: 0.4937743078917265, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 470/516, D Loss: 0.4979974334128201, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 471/516, D Loss: 0.4949438045732677, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 472/516, D Loss: 0.495547154918313, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 473/516, D Loss: 0.49647148977965117, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 474/516, D Loss: 0.49490371346473694, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 475/516, D Loss: 0.49773575155995786, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 476/516, D Loss: 0.48854442313313484, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 477/516, D Loss: 0.49234567675739527, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 478/516, D Loss: 0.4908444248139858, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 479/516, D Loss: 0.491591265425086, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 480/516, D Loss: 0.48630725778639317, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 481/516, D Loss: 0.49245106521993876, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 482/516, D Loss: 0.4917318131774664, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 483/516, D Loss: 0.4898224277421832, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 484/516, D Loss: 0.4957653465680778, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Epoch 10, Batch 485/516, D Loss: 0.49863514385651797, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 486/516, D Loss: 0.49722669064067304, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 487/516, D Loss: 0.49383694399148226, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 488/516, D Loss: 0.49368889071047306, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 489/516, D Loss: 0.49922236730344594, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 490/516, D Loss: 0.49087011348456144, G Loss: -0.9999999403953552\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 491/516, D Loss: 0.49211248848587275, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 492/516, D Loss: 0.48979492392390966, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 493/516, D Loss: 0.47994064167141914, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 494/516, D Loss: 0.48865729849785566, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 495/516, D Loss: 0.4875243529677391, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 496/516, D Loss: 0.49531375290825963, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 497/516, D Loss: 0.49650567094795406, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 498/516, D Loss: 0.4927693731151521, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 499/516, D Loss: 0.4977947734296322, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 500/516, D Loss: 0.4913312140852213, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 501/516, D Loss: 0.4951037927530706, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 502/516, D Loss: 0.4952964768745005, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 503/516, D Loss: 0.49382156366482377, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 10, Batch 504/516, D Loss: 0.4951355424709618, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 505/516, D Loss: 0.49858882767148316, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 506/516, D Loss: 0.49866685294546187, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 507/516, D Loss: 0.48605024442076683, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 508/516, D Loss: 0.494882729370147, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 509/516, D Loss: 0.4947614283300936, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 510/516, D Loss: 0.4882285948842764, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 511/516, D Loss: 0.48923867754638195, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 512/516, D Loss: 0.4991855013067834, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 513/516, D Loss: 0.49360305815935135, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 514/516, D Loss: 0.4896109467372298, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Epoch 10, Batch 515/516, D Loss: 0.4969186536036432, G Loss: -1.0\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Epoch 10, Batch 516/516, D Loss: 0.49964795730193146, G Loss: -1.0\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the cswgan model\n",
    "generator = build_generator(latent_dim, real_data.shape[1])\n",
    "discriminator = build_discriminator(real_data.shape[1])\n",
    "cswgan = build_cswgan(generator, discriminator)\n",
    "discriminator.compile(optimizer='adam', loss=wasserstein_loss)\n",
    "cswgan.compile(optimizer='adam', loss=wasserstein_loss)\n",
    "\n",
    "# Train the Conditional Sig-Wasserstein GAN\n",
    "train_cswgan(generator, discriminator, cswgan, real_data, latent_dim, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "052b5c0f-b23d-48e2-96dc-e8bc176f00da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using the trained generator\n",
    "latent_samples = np.random.randn(100, latent_dim)  # Adjust as needed\n",
    "generated_data = generator.predict(latent_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03da2d7e-7d6c-4b20-8624-fb7f93a3fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-34.391167  -26.94673   -52.933018  ...  56.80726    12.696836\n",
      "   -8.417642 ]\n",
      " [-36.09364   -27.715927  -55.42204   ...  59.604023   13.376146\n",
      "   -9.168032 ]\n",
      " [-32.552982  -24.422543  -49.79395   ...  53.630466   12.180327\n",
      "   -8.507862 ]\n",
      " ...\n",
      " [-36.43808   -27.422878  -56.246296  ...  59.776436   13.415622\n",
      "   -9.743786 ]\n",
      " [-31.162361  -23.862991  -47.83083   ...  51.537567   11.606354\n",
      "   -7.4002347]\n",
      " [-34.705692  -25.037792  -51.693043  ...  55.939354   12.070661\n",
      "   -8.654562 ]]\n"
     ]
    }
   ],
   "source": [
    "# Display the generated data\n",
    "print(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d07f00-c4eb-40e8-a1c1-8e4d4c8190e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
